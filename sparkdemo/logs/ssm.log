[INFO]  [2018-05-21 15:29:24] [org.apache.spark.SparkContext]Running Spark version 2.0.0
[WARN]  [2018-05-21 15:29:25] [org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO]  [2018-05-21 15:29:26] [org.apache.spark.SecurityManager]Changing view acls to: Bruin,hadoop
[INFO]  [2018-05-21 15:29:26] [org.apache.spark.SecurityManager]Changing modify acls to: Bruin,hadoop
[INFO]  [2018-05-21 15:29:26] [org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO]  [2018-05-21 15:29:26] [org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO]  [2018-05-21 15:29:26] [org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Bruin, hadoop); groups with view permissions: Set(); users  with modify permissions: Set(Bruin, hadoop); groups with modify permissions: Set()
[INFO]  [2018-05-21 15:29:29] [org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 50074.
[INFO]  [2018-05-21 15:29:29] [org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO]  [2018-05-21 15:29:29] [org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO]  [2018-05-21 15:29:29] [org.apache.spark.storage.DiskBlockManager]Created local directory at C:\Users\Bruin\AppData\Local\Temp\blockmgr-f251cf04-8bcc-4003-a3f1-21c172d795ce
[INFO]  [2018-05-21 15:29:29] [org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 901.8 MB
[INFO]  [2018-05-21 15:29:30] [org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO]  [2018-05-21 15:29:30] [org.spark_project.jetty.util.log]Logging initialized @9325ms
[INFO]  [2018-05-21 15:29:31] [org.spark_project.jetty.server.Server]jetty-9.2.z-SNAPSHOT
[INFO]  [2018-05-21 15:29:31] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@fade1fc{/jobs,null,AVAILABLE}
[INFO]  [2018-05-21 15:29:31] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@67c2e933{/jobs/json,null,AVAILABLE}
[INFO]  [2018-05-21 15:29:31] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@41dd05a{/jobs/job,null,AVAILABLE}
[INFO]  [2018-05-21 15:29:31] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@613a8ee1{/jobs/job/json,null,AVAILABLE}
[INFO]  [2018-05-21 15:29:31] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@178213b{/stages,null,AVAILABLE}
[INFO]  [2018-05-21 15:29:31] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7103cb56{/stages/json,null,AVAILABLE}
[INFO]  [2018-05-21 15:29:31] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1b765a2c{/stages/stage,null,AVAILABLE}
[INFO]  [2018-05-21 15:29:31] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2e8e8225{/stages/stage/json,null,AVAILABLE}
[INFO]  [2018-05-21 15:29:31] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6ebf0f36{/stages/pool,null,AVAILABLE}
[INFO]  [2018-05-21 15:29:31] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@18920cc{/stages/pool/json,null,AVAILABLE}
[INFO]  [2018-05-21 15:29:31] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2807bdeb{/storage,null,AVAILABLE}
[INFO]  [2018-05-21 15:29:31] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@72c28d64{/storage/json,null,AVAILABLE}
[INFO]  [2018-05-21 15:29:31] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6492fab5{/storage/rdd,null,AVAILABLE}
[INFO]  [2018-05-21 15:29:31] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2c5529ab{/storage/rdd/json,null,AVAILABLE}
[INFO]  [2018-05-21 15:29:31] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@39a8312f{/environment,null,AVAILABLE}
[INFO]  [2018-05-21 15:29:31] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5f6722d3{/environment/json,null,AVAILABLE}
[INFO]  [2018-05-21 15:29:31] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2c532cd8{/executors,null,AVAILABLE}
[INFO]  [2018-05-21 15:29:31] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@294e5088{/executors/json,null,AVAILABLE}
[INFO]  [2018-05-21 15:29:31] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@51972dc7{/executors/threadDump,null,AVAILABLE}
[INFO]  [2018-05-21 15:29:31] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3700ec9c{/executors/threadDump/json,null,AVAILABLE}
[INFO]  [2018-05-21 15:29:31] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2002348{/static,null,AVAILABLE}
[INFO]  [2018-05-21 15:29:31] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5911e990{/,null,AVAILABLE}
[INFO]  [2018-05-21 15:29:31] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@31000e60{/api,null,AVAILABLE}
[INFO]  [2018-05-21 15:29:31] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1d470d0{/stages/stage/kill,null,AVAILABLE}
[INFO]  [2018-05-21 15:29:31] [org.spark_project.jetty.server.ServerConnector]Started ServerConnector@6b53bcc2{HTTP/1.1}{0.0.0.0:4040}
[INFO]  [2018-05-21 15:29:31] [org.spark_project.jetty.server.Server]Started @9729ms
[INFO]  [2018-05-21 15:29:31] [org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO]  [2018-05-21 15:29:31] [org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://172.18.42.247:4040
[INFO]  [2018-05-21 15:29:32] [org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO]  [2018-05-21 15:29:32] [org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50121.
[INFO]  [2018-05-21 15:29:32] [org.apache.spark.network.netty.NettyBlockTransferService]Server created on 172.18.42.247:50121
[INFO]  [2018-05-21 15:29:32] [org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 172.18.42.247, 50121)
[INFO]  [2018-05-21 15:29:32] [org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 172.18.42.247:50121 with 901.8 MB RAM, BlockManagerId(driver, 172.18.42.247, 50121)
[INFO]  [2018-05-21 15:29:32] [org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 172.18.42.247, 50121)
[INFO]  [2018-05-21 15:29:32] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6e9319f{/metrics/json,null,AVAILABLE}
[INFO]  [2018-05-21 15:29:33] [org.apache.spark.streaming.scheduler.ReceiverTracker]Starting 1 receivers
[INFO]  [2018-05-21 15:29:33] [org.apache.spark.streaming.scheduler.ReceiverTracker]ReceiverTracker started
[INFO]  [2018-05-21 15:29:33] [org.apache.spark.streaming.dstream.SocketInputDStream]Slide time = 1000 ms
[INFO]  [2018-05-21 15:29:33] [org.apache.spark.streaming.dstream.SocketInputDStream]Storage level = Serialized 1x Replicated
[INFO]  [2018-05-21 15:29:33] [org.apache.spark.streaming.dstream.SocketInputDStream]Checkpoint interval = null
[INFO]  [2018-05-21 15:29:33] [org.apache.spark.streaming.dstream.SocketInputDStream]Remember interval = 1000 ms
[INFO]  [2018-05-21 15:29:33] [org.apache.spark.streaming.dstream.SocketInputDStream]Initialized and validated org.apache.spark.streaming.dstream.SocketInputDStream@23f88dbc
[INFO]  [2018-05-21 15:29:33] [org.apache.spark.streaming.dstream.FlatMappedDStream]Slide time = 1000 ms
[INFO]  [2018-05-21 15:29:33] [org.apache.spark.streaming.dstream.FlatMappedDStream]Storage level = Serialized 1x Replicated
[INFO]  [2018-05-21 15:29:33] [org.apache.spark.streaming.dstream.FlatMappedDStream]Checkpoint interval = null
[INFO]  [2018-05-21 15:29:33] [org.apache.spark.streaming.dstream.FlatMappedDStream]Remember interval = 1000 ms
[INFO]  [2018-05-21 15:29:33] [org.apache.spark.streaming.dstream.FlatMappedDStream]Initialized and validated org.apache.spark.streaming.dstream.FlatMappedDStream@5f3b9eaf
[INFO]  [2018-05-21 15:29:33] [org.apache.spark.streaming.dstream.MappedDStream]Slide time = 1000 ms
[INFO]  [2018-05-21 15:29:33] [org.apache.spark.streaming.dstream.MappedDStream]Storage level = Serialized 1x Replicated
[INFO]  [2018-05-21 15:29:33] [org.apache.spark.streaming.dstream.MappedDStream]Checkpoint interval = null
[INFO]  [2018-05-21 15:29:33] [org.apache.spark.streaming.dstream.MappedDStream]Remember interval = 1000 ms
[INFO]  [2018-05-21 15:29:33] [org.apache.spark.streaming.dstream.MappedDStream]Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@7c2d8970
[INFO]  [2018-05-21 15:29:33] [org.apache.spark.streaming.dstream.ShuffledDStream]Slide time = 1000 ms
[INFO]  [2018-05-21 15:29:33] [org.apache.spark.streaming.dstream.ShuffledDStream]Storage level = Serialized 1x Replicated
[INFO]  [2018-05-21 15:29:33] [org.apache.spark.streaming.dstream.ShuffledDStream]Checkpoint interval = null
[INFO]  [2018-05-21 15:29:33] [org.apache.spark.streaming.dstream.ShuffledDStream]Remember interval = 1000 ms
[INFO]  [2018-05-21 15:29:33] [org.apache.spark.streaming.dstream.ShuffledDStream]Initialized and validated org.apache.spark.streaming.dstream.ShuffledDStream@36dc683c
[INFO]  [2018-05-21 15:29:33] [org.apache.spark.streaming.dstream.ForEachDStream]Slide time = 1000 ms
[INFO]  [2018-05-21 15:29:33] [org.apache.spark.streaming.dstream.ForEachDStream]Storage level = Serialized 1x Replicated
[INFO]  [2018-05-21 15:29:33] [org.apache.spark.streaming.dstream.ForEachDStream]Checkpoint interval = null
[INFO]  [2018-05-21 15:29:33] [org.apache.spark.streaming.dstream.ForEachDStream]Remember interval = 1000 ms
[INFO]  [2018-05-21 15:29:33] [org.apache.spark.streaming.dstream.ForEachDStream]Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@512e8636
[INFO]  [2018-05-21 15:29:34] [org.apache.spark.streaming.util.RecurringTimer]Started timer for JobGenerator at time 1526887774000
[INFO]  [2018-05-21 15:29:34] [org.apache.spark.streaming.scheduler.JobGenerator]Started JobGenerator at 1526887774000 ms
[INFO]  [2018-05-21 15:29:34] [org.apache.spark.streaming.scheduler.JobScheduler]Started JobScheduler
[INFO]  [2018-05-21 15:29:34] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:29:34] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6999cd39{/streaming,null,AVAILABLE}
[INFO]  [2018-05-21 15:29:34] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7ed9ae94{/streaming/json,null,AVAILABLE}
[INFO]  [2018-05-21 15:29:34] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@534ca02b{/streaming/batch,null,AVAILABLE}
[INFO]  [2018-05-21 15:29:34] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4b6ac111{/streaming/batch/json,null,AVAILABLE}
[INFO]  [2018-05-21 15:29:34] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7c9b78e3{/static/streaming,null,AVAILABLE}
[INFO]  [2018-05-21 15:29:34] [org.apache.spark.streaming.StreamingContext]StreamingContext started
[INFO]  [2018-05-21 15:29:34] [org.apache.spark.scheduler.DAGScheduler]Got job 0 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:29:34] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 0 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:34] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:29:34] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:34] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 0 (Receiver 0 ParallelCollectionRDD[0] at makeRDD at ReceiverTracker.scala:610), which has no missing parents
[INFO]  [2018-05-21 15:29:35] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526887774000 ms
[INFO]  [2018-05-21 15:29:35] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526887775000 ms
[INFO]  [2018-05-21 15:29:35] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526887774000 ms.0 from job set of time 1526887774000 ms
[INFO]  [2018-05-21 15:29:35] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:29:35] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 34.1 KB, free 901.8 MB)
[INFO]  [2018-05-21 15:29:35] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.8 MB)
[INFO]  [2018-05-21 15:29:35] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:29:35] [org.apache.spark.SparkContext]Created broadcast 0 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:35] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 0 (Receiver 0 ParallelCollectionRDD[0] at makeRDD at ReceiverTracker.scala:610)
[INFO]  [2018-05-21 15:29:35] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 0.0 with 1 tasks
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526887776000 ms
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 3 (mapToPair at SparkStreamingT.java:87)
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.scheduler.DAGScheduler]Got job 1 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 2 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 1)
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 2 (ShuffledRDD[4] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 3.8 KB, free 901.8 MB)
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.7 MB)
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.SparkContext]Created broadcast 1 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 2 (ShuffledRDD[4] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 2.0 with 1 tasks
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 2.0 (TID 1, localhost, partition 0, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.executor.Executor]Running task 0.0 in stage 0.0 (TID 0)
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.executor.Executor]Running task 0.0 in stage 2.0 (TID 1)
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 15 ms
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887776600
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 2.0 (TID 1). 1636 bytes result sent to driver
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 2.0 (TID 1) in 350 ms on localhost (1/1)
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 2.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.scheduler.DAGScheduler]ResultStage 2 (print at SparkStreamingT.java:101) finished in 0.417 s
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.scheduler.DAGScheduler]Job 1 finished: print at SparkStreamingT.java:101, took 1.400505 s
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 0 is 83 bytes
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.scheduler.DAGScheduler]Got job 2 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 4 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 3)
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 4 (ShuffledRDD[4] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_2 stored as values in memory (estimated size 3.8 KB, free 901.7 MB)
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.7 MB)
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_2_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.SparkContext]Created broadcast 2 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 4 (ShuffledRDD[4] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 4.0 with 1 tasks
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 4.0 (TID 2, localhost, partition 1, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.executor.Executor]Running task 0.0 in stage 4.0 (TID 2)
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 4.0 (TID 2). 1549 bytes result sent to driver
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 4.0 (TID 2) in 16 ms on localhost (1/1)
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 4.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.scheduler.DAGScheduler]ResultStage 4 (print at SparkStreamingT.java:101) finished in 0.022 s
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.scheduler.DAGScheduler]Job 2 finished: print at SparkStreamingT.java:101, took 0.049562 s
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526887774000 ms.0 from job set of time 1526887774000 ms
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 7 (mapToPair at SparkStreamingT.java:87)
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.scheduler.DAGScheduler]Got job 3 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 6 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 5)
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 2.756 s for time 1526887774000 ms (execution: 1.673 s)
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526887775000 ms.0 from job set of time 1526887775000 ms
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 6 (ShuffledRDD[8] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_3 stored as values in memory (estimated size 3.8 KB, free 901.7 MB)
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.7 MB)
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_3_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.SparkContext]Created broadcast 3 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 6 (ShuffledRDD[8] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 6.0 with 1 tasks
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 6.0 (TID 3, localhost, partition 0, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.executor.Executor]Running task 0.0 in stage 6.0 (TID 3)
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 6.0 (TID 3). 1549 bytes result sent to driver
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 6.0 (TID 3) in 12 ms on localhost (1/1)
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 6.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.scheduler.DAGScheduler]ResultStage 6 (print at SparkStreamingT.java:101) finished in 0.014 s
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.scheduler.DAGScheduler]Job 3 finished: print at SparkStreamingT.java:101, took 0.027471 s
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 1 is 83 bytes
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.scheduler.DAGScheduler]Got job 4 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 8 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 7)
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 8 (ShuffledRDD[8] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_4 stored as values in memory (estimated size 3.8 KB, free 901.7 MB)
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.7 MB)
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_4_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.SparkContext]Created broadcast 4 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 8 (ShuffledRDD[8] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 8.0 with 1 tasks
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 8.0 (TID 4, localhost, partition 1, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.executor.Executor]Running task 0.0 in stage 8.0 (TID 4)
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 8.0 (TID 4). 1549 bytes result sent to driver
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 8.0 (TID 4) in 9 ms on localhost (1/1)
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 8.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.scheduler.DAGScheduler]ResultStage 8 (print at SparkStreamingT.java:101) finished in 0.010 s
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.scheduler.DAGScheduler]Job 4 finished: print at SparkStreamingT.java:101, took 0.020611 s
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526887775000 ms.0 from job set of time 1526887775000 ms
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 1.824 s for time 1526887775000 ms (execution: 0.052 s)
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 4 from persistence list
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526887776000 ms.0 from job set of time 1526887776000 ms
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 11 (mapToPair at SparkStreamingT.java:87)
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.scheduler.DAGScheduler]Got job 5 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 10 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 9)
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 10 (ShuffledRDD[12] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_5 stored as values in memory (estimated size 3.8 KB, free 901.7 MB)
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.7 MB)
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_5_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.SparkContext]Created broadcast 5 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 3 from persistence list
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 10 (ShuffledRDD[12] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 10.0 with 1 tasks
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 2 from persistence list
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 10.0 (TID 5, localhost, partition 0, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.executor.Executor]Running task 0.0 in stage 10.0 (TID 5)
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.rdd.BlockRDD]Removing RDD 1 from persistence list
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.storage.BlockManager]Removing RDD 2
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.storage.BlockManager]Removing RDD 1
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.storage.BlockManager]Removing RDD 4
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.storage.BlockManager]Removing RDD 3
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.streaming.dstream.SocketInputDStream]Removing blocks of RDD BlockRDD[1] at socketTextStream at SparkStreamingT.java:72 of time 1526887775000 ms
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 10.0 (TID 5). 1549 bytes result sent to driver
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 10.0 (TID 5) in 23 ms on localhost (1/1)
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 10.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.scheduler.DAGScheduler]ResultStage 10 (print at SparkStreamingT.java:101) finished in 0.025 s
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.scheduler.DAGScheduler]Job 5 finished: print at SparkStreamingT.java:101, took 0.054893 s
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 2 is 83 bytes
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.scheduler.DAGScheduler]Got job 6 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 12 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 11)
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 12 (ShuffledRDD[12] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_6 stored as values in memory (estimated size 3.8 KB, free 901.7 MB)
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.7 MB)
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_6_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.SparkContext]Created broadcast 6 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 12 (ShuffledRDD[12] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 12.0 with 1 tasks
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 12.0 (TID 6, localhost, partition 1, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.executor.Executor]Running task 0.0 in stage 12.0 (TID 6)
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 12.0 (TID 6). 1549 bytes result sent to driver
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 12.0 (TID 6) in 7 ms on localhost (1/1)
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 12.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.scheduler.DAGScheduler]ResultStage 12 (print at SparkStreamingT.java:101) finished in 0.008 s
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.scheduler.DAGScheduler]Job 6 finished: print at SparkStreamingT.java:101, took 0.021617 s
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526887776000 ms.0 from job set of time 1526887776000 ms
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 8 from persistence list
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.934 s for time 1526887776000 ms (execution: 0.108 s)
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.storage.BlockManager]Removing RDD 8
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 7 from persistence list
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.storage.BlockManager]Removing RDD 7
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 6 from persistence list
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.storage.BlockManager]Removing RDD 6
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.rdd.BlockRDD]Removing RDD 5 from persistence list
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.storage.BlockManager]Removing RDD 5
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.streaming.dstream.SocketInputDStream]Removing blocks of RDD BlockRDD[5] at socketTextStream at SparkStreamingT.java:72 of time 1526887776000 ms
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 1526887774000 ms
[INFO]  [2018-05-21 15:29:36] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1526887774000 ms
[INFO]  [2018-05-21 15:29:37] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526887777000 ms
[INFO]  [2018-05-21 15:29:37] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526887777000 ms.0 from job set of time 1526887777000 ms
[INFO]  [2018-05-21 15:29:37] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:29:37] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 15 (mapToPair at SparkStreamingT.java:87)
[INFO]  [2018-05-21 15:29:37] [org.apache.spark.scheduler.DAGScheduler]Got job 7 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:29:37] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 14 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:29:37] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 13)
[INFO]  [2018-05-21 15:29:37] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:37] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 14 (ShuffledRDD[16] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:29:37] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_7 stored as values in memory (estimated size 3.8 KB, free 901.7 MB)
[INFO]  [2018-05-21 15:29:37] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.7 MB)
[INFO]  [2018-05-21 15:29:37] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_7_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:29:37] [org.apache.spark.SparkContext]Created broadcast 7 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:37] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 14 (ShuffledRDD[16] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:29:37] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 14.0 with 1 tasks
[INFO]  [2018-05-21 15:29:37] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 14.0 (TID 7, localhost, partition 0, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:29:37] [org.apache.spark.executor.Executor]Running task 0.0 in stage 14.0 (TID 7)
[INFO]  [2018-05-21 15:29:37] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:29:37] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO]  [2018-05-21 15:29:37] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 14.0 (TID 7). 1549 bytes result sent to driver
[INFO]  [2018-05-21 15:29:37] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 14.0 (TID 7) in 9 ms on localhost (1/1)
[INFO]  [2018-05-21 15:29:37] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 14.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:37] [org.apache.spark.scheduler.DAGScheduler]ResultStage 14 (print at SparkStreamingT.java:101) finished in 0.011 s
[INFO]  [2018-05-21 15:29:37] [org.apache.spark.scheduler.DAGScheduler]Job 7 finished: print at SparkStreamingT.java:101, took 0.022778 s
[INFO]  [2018-05-21 15:29:37] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:29:37] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 3 is 83 bytes
[INFO]  [2018-05-21 15:29:37] [org.apache.spark.scheduler.DAGScheduler]Got job 8 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:29:37] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 16 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:29:37] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 15)
[INFO]  [2018-05-21 15:29:37] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:37] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 16 (ShuffledRDD[16] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:29:37] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_8 stored as values in memory (estimated size 3.8 KB, free 901.7 MB)
[INFO]  [2018-05-21 15:29:37] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_8_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.7 MB)
[INFO]  [2018-05-21 15:29:37] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_8_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:29:37] [org.apache.spark.SparkContext]Created broadcast 8 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:37] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 16 (ShuffledRDD[16] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:29:37] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 16.0 with 1 tasks
[INFO]  [2018-05-21 15:29:37] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 16.0 (TID 8, localhost, partition 1, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:29:37] [org.apache.spark.executor.Executor]Running task 0.0 in stage 16.0 (TID 8)
[INFO]  [2018-05-21 15:29:37] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:29:37] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:29:37] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 16.0 (TID 8). 1549 bytes result sent to driver
[INFO]  [2018-05-21 15:29:37] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 16.0 (TID 8) in 10 ms on localhost (1/1)
[INFO]  [2018-05-21 15:29:37] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 16.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:37] [org.apache.spark.scheduler.DAGScheduler]ResultStage 16 (print at SparkStreamingT.java:101) finished in 0.011 s
[INFO]  [2018-05-21 15:29:37] [org.apache.spark.scheduler.DAGScheduler]Job 8 finished: print at SparkStreamingT.java:101, took 0.025926 s
[INFO]  [2018-05-21 15:29:37] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526887777000 ms.0 from job set of time 1526887777000 ms
[INFO]  [2018-05-21 15:29:37] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.071 s for time 1526887777000 ms (execution: 0.061 s)
[INFO]  [2018-05-21 15:29:37] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 12 from persistence list
[INFO]  [2018-05-21 15:29:37] [org.apache.spark.storage.BlockManager]Removing RDD 12
[INFO]  [2018-05-21 15:29:37] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 11 from persistence list
[INFO]  [2018-05-21 15:29:37] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 10 from persistence list
[INFO]  [2018-05-21 15:29:37] [org.apache.spark.storage.BlockManager]Removing RDD 11
[INFO]  [2018-05-21 15:29:37] [org.apache.spark.rdd.BlockRDD]Removing RDD 9 from persistence list
[INFO]  [2018-05-21 15:29:37] [org.apache.spark.storage.BlockManager]Removing RDD 10
[INFO]  [2018-05-21 15:29:37] [org.apache.spark.streaming.dstream.SocketInputDStream]Removing blocks of RDD BlockRDD[9] at socketTextStream at SparkStreamingT.java:72 of time 1526887777000 ms
[INFO]  [2018-05-21 15:29:37] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 1526887775000 ms
[INFO]  [2018-05-21 15:29:37] [org.apache.spark.storage.BlockManager]Removing RDD 9
[INFO]  [2018-05-21 15:29:37] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1526887775000 ms
[INFO]  [2018-05-21 15:29:38] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526887778000 ms.0 from job set of time 1526887778000 ms
[INFO]  [2018-05-21 15:29:38] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:29:38] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 19 (mapToPair at SparkStreamingT.java:87)
[INFO]  [2018-05-21 15:29:38] [org.apache.spark.scheduler.DAGScheduler]Got job 9 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:29:38] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 18 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:29:38] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 17)
[INFO]  [2018-05-21 15:29:38] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:38] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 18 (ShuffledRDD[20] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:29:38] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_9 stored as values in memory (estimated size 3.8 KB, free 901.7 MB)
[INFO]  [2018-05-21 15:29:38] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_9_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.7 MB)
[INFO]  [2018-05-21 15:29:38] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526887778000 ms
[INFO]  [2018-05-21 15:29:38] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_9_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:29:38] [org.apache.spark.SparkContext]Created broadcast 9 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:38] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 18 (ShuffledRDD[20] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:29:38] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 18.0 with 1 tasks
[INFO]  [2018-05-21 15:29:38] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 18.0 (TID 9, localhost, partition 0, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:29:38] [org.apache.spark.executor.Executor]Running task 0.0 in stage 18.0 (TID 9)
[INFO]  [2018-05-21 15:29:38] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:29:38] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:29:38] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 18.0 (TID 9). 1549 bytes result sent to driver
[INFO]  [2018-05-21 15:29:38] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 18.0 (TID 9) in 8 ms on localhost (1/1)
[INFO]  [2018-05-21 15:29:38] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 18.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:38] [org.apache.spark.scheduler.DAGScheduler]ResultStage 18 (print at SparkStreamingT.java:101) finished in 0.011 s
[INFO]  [2018-05-21 15:29:38] [org.apache.spark.scheduler.DAGScheduler]Job 9 finished: print at SparkStreamingT.java:101, took 0.034981 s
[INFO]  [2018-05-21 15:29:38] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:29:38] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 4 is 83 bytes
[INFO]  [2018-05-21 15:29:38] [org.apache.spark.scheduler.DAGScheduler]Got job 10 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:29:38] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 20 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:29:38] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 19)
[INFO]  [2018-05-21 15:29:38] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:38] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 20 (ShuffledRDD[20] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:29:38] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_10 stored as values in memory (estimated size 3.8 KB, free 901.7 MB)
[INFO]  [2018-05-21 15:29:38] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_10_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.7 MB)
[INFO]  [2018-05-21 15:29:38] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_10_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:29:38] [org.apache.spark.SparkContext]Created broadcast 10 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:38] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 20 (ShuffledRDD[20] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:29:38] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 20.0 with 1 tasks
[INFO]  [2018-05-21 15:29:38] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 20.0 (TID 10, localhost, partition 1, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:29:38] [org.apache.spark.executor.Executor]Running task 0.0 in stage 20.0 (TID 10)
[INFO]  [2018-05-21 15:29:38] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:29:38] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:29:38] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 20.0 (TID 10). 1549 bytes result sent to driver
[INFO]  [2018-05-21 15:29:38] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 20.0 (TID 10) in 6 ms on localhost (1/1)
[INFO]  [2018-05-21 15:29:38] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 20.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:38] [org.apache.spark.scheduler.DAGScheduler]ResultStage 20 (print at SparkStreamingT.java:101) finished in 0.007 s
[INFO]  [2018-05-21 15:29:38] [org.apache.spark.scheduler.DAGScheduler]Job 10 finished: print at SparkStreamingT.java:101, took 0.018911 s
[INFO]  [2018-05-21 15:29:38] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526887778000 ms.0 from job set of time 1526887778000 ms
[INFO]  [2018-05-21 15:29:38] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.085 s for time 1526887778000 ms (execution: 0.066 s)
[INFO]  [2018-05-21 15:29:38] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 16 from persistence list
[INFO]  [2018-05-21 15:29:38] [org.apache.spark.storage.BlockManager]Removing RDD 16
[INFO]  [2018-05-21 15:29:38] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 15 from persistence list
[INFO]  [2018-05-21 15:29:38] [org.apache.spark.storage.BlockManager]Removing RDD 15
[INFO]  [2018-05-21 15:29:38] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 14 from persistence list
[INFO]  [2018-05-21 15:29:38] [org.apache.spark.storage.BlockManager]Removing RDD 14
[INFO]  [2018-05-21 15:29:38] [org.apache.spark.rdd.BlockRDD]Removing RDD 13 from persistence list
[INFO]  [2018-05-21 15:29:38] [org.apache.spark.storage.BlockManager]Removing RDD 13
[INFO]  [2018-05-21 15:29:38] [org.apache.spark.streaming.dstream.SocketInputDStream]Removing blocks of RDD BlockRDD[13] at socketTextStream at SparkStreamingT.java:72 of time 1526887778000 ms
[INFO]  [2018-05-21 15:29:38] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 1526887776000 ms
[INFO]  [2018-05-21 15:29:38] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1526887776000 ms
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526887779000 ms
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526887779000 ms.0 from job set of time 1526887779000 ms
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 23 (mapToPair at SparkStreamingT.java:87)
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.scheduler.DAGScheduler]Got job 11 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 22 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 21)
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 22 (ShuffledRDD[24] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_11 stored as values in memory (estimated size 3.8 KB, free 901.7 MB)
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_11_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.7 MB)
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_11_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.SparkContext]Created broadcast 11 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 22 (ShuffledRDD[24] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 22.0 with 1 tasks
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 22.0 (TID 11, localhost, partition 0, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.executor.Executor]Running task 0.0 in stage 22.0 (TID 11)
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 22.0 (TID 11). 1549 bytes result sent to driver
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 22.0 (TID 11) in 8 ms on localhost (1/1)
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 22.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.scheduler.DAGScheduler]ResultStage 22 (print at SparkStreamingT.java:101) finished in 0.010 s
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.scheduler.DAGScheduler]Job 11 finished: print at SparkStreamingT.java:101, took 0.023576 s
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 5 is 83 bytes
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.scheduler.DAGScheduler]Got job 12 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 24 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 23)
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 24 (ShuffledRDD[24] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_12 stored as values in memory (estimated size 3.8 KB, free 901.7 MB)
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_12_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.7 MB)
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_12_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.SparkContext]Created broadcast 12 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 24 (ShuffledRDD[24] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 24.0 with 1 tasks
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 24.0 (TID 12, localhost, partition 1, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.executor.Executor]Running task 0.0 in stage 24.0 (TID 12)
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 24.0 (TID 12). 1549 bytes result sent to driver
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 24.0 (TID 12) in 6 ms on localhost (1/1)
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 24.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.scheduler.DAGScheduler]ResultStage 24 (print at SparkStreamingT.java:101) finished in 0.007 s
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.scheduler.DAGScheduler]Job 12 finished: print at SparkStreamingT.java:101, took 0.016742 s
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526887779000 ms.0 from job set of time 1526887779000 ms
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 20 from persistence list
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.076 s for time 1526887779000 ms (execution: 0.057 s)
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.storage.BlockManager]Removing RDD 20
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 19 from persistence list
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.storage.BlockManager]Removing RDD 19
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 18 from persistence list
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.storage.BlockManager]Removing RDD 18
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.rdd.BlockRDD]Removing RDD 17 from persistence list
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.storage.BlockManager]Removing RDD 17
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.streaming.dstream.SocketInputDStream]Removing blocks of RDD BlockRDD[17] at socketTextStream at SparkStreamingT.java:72 of time 1526887779000 ms
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 1526887777000 ms
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1526887777000 ms
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[ERROR]  [2018-05-21 15:29:39] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:29:39] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_6_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.ContextCleaner]Cleaned shuffle 3
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_7_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_8_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.ContextCleaner]Cleaned shuffle 4
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_9_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_10_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_11_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_12_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_1_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_2_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_3_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_4_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.ContextCleaner]Cleaned shuffle 2
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_5_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887779600
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:29:39] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:29:39] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 0.0 (TID 0)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN]  [2018-05-21 15:29:39] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 0.0 (TID 0, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:29:39] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 0.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 0.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 0
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.scheduler.DAGScheduler]ResultStage 0 (start at SparkStreamingT.java:107) failed in 3.718 s
[ERROR]  [2018-05-21 15:29:39] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.scheduler.DAGScheduler]Got job 13 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 25 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 25 (Receiver 0 ParallelCollectionRDD[25] at makeRDD at ReceiverTracker.scala:610), which has no missing parents
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_13 stored as values in memory (estimated size 34.1 KB, free 901.7 MB)
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_13_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.7 MB)
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_13_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.SparkContext]Created broadcast 13 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 25 (Receiver 0 ParallelCollectionRDD[25] at makeRDD at ReceiverTracker.scala:610)
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 25.0 with 1 tasks
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 25.0 (TID 13, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.executor.Executor]Running task 0.0 in stage 25.0 (TID 13)
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887780000
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[ERROR]  [2018-05-21 15:29:39] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:29:39] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:29:39] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526887780000 ms
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526887780000 ms.0 from job set of time 1526887780000 ms
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 28 (mapToPair at SparkStreamingT.java:87)
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.scheduler.DAGScheduler]Got job 14 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 27 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 26)
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 27 (ShuffledRDD[29] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_14 stored as values in memory (estimated size 3.8 KB, free 901.7 MB)
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_14_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.7 MB)
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_14_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.SparkContext]Created broadcast 14 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 27 (ShuffledRDD[29] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 27.0 with 1 tasks
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 27.0 (TID 14, localhost, partition 0, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.executor.Executor]Running task 0.0 in stage 27.0 (TID 14)
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 27.0 (TID 14). 1549 bytes result sent to driver
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 27.0 (TID 14) in 7 ms on localhost (1/1)
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 27.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.scheduler.DAGScheduler]ResultStage 27 (print at SparkStreamingT.java:101) finished in 0.008 s
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.scheduler.DAGScheduler]Job 14 finished: print at SparkStreamingT.java:101, took 0.017915 s
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 6 is 83 bytes
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.scheduler.DAGScheduler]Got job 15 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 29 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 28)
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 29 (ShuffledRDD[29] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_15 stored as values in memory (estimated size 3.8 KB, free 901.7 MB)
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_15_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.7 MB)
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_15_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.SparkContext]Created broadcast 15 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 29 (ShuffledRDD[29] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 29.0 with 1 tasks
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 29.0 (TID 15, localhost, partition 1, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.executor.Executor]Running task 0.0 in stage 29.0 (TID 15)
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 29.0 (TID 15). 1549 bytes result sent to driver
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 29.0 (TID 15) in 24 ms on localhost (1/1)
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 29.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.scheduler.DAGScheduler]ResultStage 29 (print at SparkStreamingT.java:101) finished in 0.028 s
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.scheduler.DAGScheduler]Job 15 finished: print at SparkStreamingT.java:101, took 0.051383 s
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526887780000 ms.0 from job set of time 1526887780000 ms
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.098 s for time 1526887780000 ms (execution: 0.090 s)
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 24 from persistence list
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.storage.BlockManager]Removing RDD 24
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 23 from persistence list
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 22 from persistence list
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.storage.BlockManager]Removing RDD 23
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.rdd.BlockRDD]Removing RDD 21 from persistence list
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.storage.BlockManager]Removing RDD 21
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.storage.BlockManager]Removing RDD 22
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.streaming.dstream.SocketInputDStream]Removing blocks of RDD BlockRDD[21] at socketTextStream at SparkStreamingT.java:72 of time 1526887780000 ms
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 1526887778000 ms
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1526887778000 ms
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887780200
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:29:40] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:29:40] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 25.0 (TID 13)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN]  [2018-05-21 15:29:40] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 25.0 (TID 13, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:29:40] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 25.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 25.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 25
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.scheduler.DAGScheduler]ResultStage 25 (start at SparkStreamingT.java:107) failed in 0.421 s
[ERROR]  [2018-05-21 15:29:40] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 25.0 failed 1 times, most recent failure: Lost task 0.0 in stage 25.0 (TID 13, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.scheduler.DAGScheduler]Got job 16 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 30 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 30 (Receiver 0 ParallelCollectionRDD[30] at makeRDD at ReceiverTracker.scala:610), which has no missing parents
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_16 stored as values in memory (estimated size 34.1 KB, free 901.7 MB)
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_16_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.7 MB)
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_16_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.SparkContext]Created broadcast 16 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 30 (Receiver 0 ParallelCollectionRDD[30] at makeRDD at ReceiverTracker.scala:610)
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 30.0 with 1 tasks
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 30.0 (TID 16, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.executor.Executor]Running task 0.0 in stage 30.0 (TID 16)
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887780400
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[ERROR]  [2018-05-21 15:29:40] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:29:40] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887780600
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:29:40] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:29:40] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 30.0 (TID 16)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN]  [2018-05-21 15:29:40] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 30.0 (TID 16, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:29:40] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 30.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 30.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 30
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.scheduler.DAGScheduler]ResultStage 30 (start at SparkStreamingT.java:107) failed in 0.370 s
[ERROR]  [2018-05-21 15:29:40] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 30.0 failed 1 times, most recent failure: Lost task 0.0 in stage 30.0 (TID 16, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.scheduler.DAGScheduler]Got job 17 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 31 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 31 (Receiver 0 ParallelCollectionRDD[31] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_17 stored as values in memory (estimated size 34.1 KB, free 901.6 MB)
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_17_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.6 MB)
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_17_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.SparkContext]Created broadcast 17 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 31 (Receiver 0 ParallelCollectionRDD[31] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 31.0 with 1 tasks
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 31.0 (TID 17, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.executor.Executor]Running task 0.0 in stage 31.0 (TID 17)
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887780800
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[ERROR]  [2018-05-21 15:29:40] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:29:40] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:29:40] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887781000
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:29:41] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:29:41] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 31.0 (TID 17)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526887781000 ms
[WARN]  [2018-05-21 15:29:41] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 31.0 (TID 17, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:29:41] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 31.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 31.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526887781000 ms.0 from job set of time 1526887781000 ms
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 31
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.scheduler.DAGScheduler]ResultStage 31 (start at SparkStreamingT.java:107) failed in 0.364 s
[ERROR]  [2018-05-21 15:29:41] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 31.0 failed 1 times, most recent failure: Lost task 0.0 in stage 31.0 (TID 17, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 34 (mapToPair at SparkStreamingT.java:87)
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.scheduler.DAGScheduler]Got job 18 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 33 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 32)
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 33 (ShuffledRDD[35] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_18 stored as values in memory (estimated size 3.8 KB, free 901.6 MB)
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_18_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.6 MB)
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_18_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.SparkContext]Created broadcast 18 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 33 (ShuffledRDD[35] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 33.0 with 1 tasks
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.scheduler.DAGScheduler]Got job 19 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 34 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 34 (Receiver 0 ParallelCollectionRDD[36] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 33.0 (TID 18, localhost, partition 0, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.executor.Executor]Running task 0.0 in stage 33.0 (TID 18)
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 33.0 (TID 18). 1549 bytes result sent to driver
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 33.0 (TID 18) in 13 ms on localhost (1/1)
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 33.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_19 stored as values in memory (estimated size 34.1 KB, free 901.6 MB)
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_19_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.6 MB)
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_19_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.SparkContext]Created broadcast 19 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 34 (Receiver 0 ParallelCollectionRDD[36] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 34.0 with 1 tasks
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 34.0 (TID 19, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.scheduler.DAGScheduler]ResultStage 33 (print at SparkStreamingT.java:101) finished in 0.023 s
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.scheduler.DAGScheduler]Job 18 finished: print at SparkStreamingT.java:101, took 0.043469 s
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.executor.Executor]Running task 0.0 in stage 34.0 (TID 19)
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 7 is 83 bytes
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.scheduler.DAGScheduler]Got job 20 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 36 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 35)
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 36 (ShuffledRDD[35] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_20 stored as values in memory (estimated size 3.8 KB, free 901.6 MB)
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_20_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.6 MB)
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_20_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.SparkContext]Created broadcast 20 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887781200
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 36 (ShuffledRDD[35] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 36.0 with 1 tasks
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[ERROR]  [2018-05-21 15:29:41] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:29:41] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 36.0 (TID 20, localhost, partition 1, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.executor.Executor]Running task 0.0 in stage 36.0 (TID 20)
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 36.0 (TID 20). 1549 bytes result sent to driver
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 36.0 (TID 20) in 7 ms on localhost (1/1)
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 36.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.scheduler.DAGScheduler]ResultStage 36 (print at SparkStreamingT.java:101) finished in 0.011 s
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.scheduler.DAGScheduler]Job 20 finished: print at SparkStreamingT.java:101, took 0.023539 s
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526887781000 ms.0 from job set of time 1526887781000 ms
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.094 s for time 1526887781000 ms (execution: 0.079 s)
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 29 from persistence list
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.storage.BlockManager]Removing RDD 29
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 28 from persistence list
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.storage.BlockManager]Removing RDD 28
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 27 from persistence list
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.storage.BlockManager]Removing RDD 27
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.rdd.BlockRDD]Removing RDD 26 from persistence list
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.storage.BlockManager]Removing RDD 26
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.streaming.dstream.SocketInputDStream]Removing blocks of RDD BlockRDD[26] at socketTextStream at SparkStreamingT.java:72 of time 1526887781000 ms
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 1526887779000 ms
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1526887779000 ms
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887781400
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:29:41] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:29:41] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 34.0 (TID 19)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN]  [2018-05-21 15:29:41] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 34.0 (TID 19, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:29:41] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 34.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 34.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 34
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.scheduler.DAGScheduler]ResultStage 34 (start at SparkStreamingT.java:107) failed in 0.360 s
[ERROR]  [2018-05-21 15:29:41] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 34.0 failed 1 times, most recent failure: Lost task 0.0 in stage 34.0 (TID 19, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.scheduler.DAGScheduler]Got job 21 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 37 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 37 (Receiver 0 ParallelCollectionRDD[37] at makeRDD at ReceiverTracker.scala:610), which has no missing parents
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_21 stored as values in memory (estimated size 34.1 KB, free 901.5 MB)
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_21_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.5 MB)
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_21_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.SparkContext]Created broadcast 21 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 37 (Receiver 0 ParallelCollectionRDD[37] at makeRDD at ReceiverTracker.scala:610)
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 37.0 with 1 tasks
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 37.0 (TID 21, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.executor.Executor]Running task 0.0 in stage 37.0 (TID 21)
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887781600
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[ERROR]  [2018-05-21 15:29:41] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:29:41] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887781800
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:29:41] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:29:41] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 37.0 (TID 21)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN]  [2018-05-21 15:29:41] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 37.0 (TID 21, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:29:41] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 37.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 37.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 37
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.scheduler.DAGScheduler]ResultStage 37 (start at SparkStreamingT.java:107) failed in 0.434 s
[ERROR]  [2018-05-21 15:29:41] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 37.0 failed 1 times, most recent failure: Lost task 0.0 in stage 37.0 (TID 21, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.scheduler.DAGScheduler]Got job 22 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 38 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 38 (Receiver 0 ParallelCollectionRDD[38] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_22 stored as values in memory (estimated size 34.1 KB, free 901.5 MB)
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_22_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.5 MB)
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_22_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.SparkContext]Created broadcast 22 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 38 (Receiver 0 ParallelCollectionRDD[38] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 38.0 with 1 tasks
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 38.0 (TID 22, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.executor.Executor]Running task 0.0 in stage 38.0 (TID 22)
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887782000
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[ERROR]  [2018-05-21 15:29:41] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:29:41] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:29:41] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526887782000 ms
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526887782000 ms.0 from job set of time 1526887782000 ms
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 41 (mapToPair at SparkStreamingT.java:87)
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.scheduler.DAGScheduler]Got job 23 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 40 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 39)
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 40 (ShuffledRDD[42] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_23 stored as values in memory (estimated size 3.8 KB, free 901.5 MB)
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_23_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.5 MB)
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_23_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.SparkContext]Created broadcast 23 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 40 (ShuffledRDD[42] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 40.0 with 1 tasks
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 40.0 (TID 23, localhost, partition 0, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.executor.Executor]Running task 0.0 in stage 40.0 (TID 23)
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 40.0 (TID 23). 1549 bytes result sent to driver
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 40.0 (TID 23) in 22 ms on localhost (1/1)
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 40.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.scheduler.DAGScheduler]ResultStage 40 (print at SparkStreamingT.java:101) finished in 0.026 s
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.scheduler.DAGScheduler]Job 23 finished: print at SparkStreamingT.java:101, took 0.051848 s
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 8 is 83 bytes
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.scheduler.DAGScheduler]Got job 24 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 42 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 41)
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 42 (ShuffledRDD[42] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_24 stored as values in memory (estimated size 3.8 KB, free 901.5 MB)
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_24_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.5 MB)
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_24_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.SparkContext]Created broadcast 24 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 42 (ShuffledRDD[42] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 42.0 with 1 tasks
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 42.0 (TID 24, localhost, partition 1, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.executor.Executor]Running task 0.0 in stage 42.0 (TID 24)
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 42.0 (TID 24). 1549 bytes result sent to driver
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 42.0 (TID 24) in 11 ms on localhost (1/1)
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 42.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.scheduler.DAGScheduler]ResultStage 42 (print at SparkStreamingT.java:101) finished in 0.012 s
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.scheduler.DAGScheduler]Job 24 finished: print at SparkStreamingT.java:101, took 0.031749 s
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526887782000 ms.0 from job set of time 1526887782000 ms
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.110 s for time 1526887782000 ms (execution: 0.103 s)
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 35 from persistence list
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 34 from persistence list
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.storage.BlockManager]Removing RDD 34
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.storage.BlockManager]Removing RDD 35
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 33 from persistence list
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.rdd.BlockRDD]Removing RDD 32 from persistence list
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.storage.BlockManager]Removing RDD 32
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.storage.BlockManager]Removing RDD 33
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.streaming.dstream.SocketInputDStream]Removing blocks of RDD BlockRDD[32] at socketTextStream at SparkStreamingT.java:72 of time 1526887782000 ms
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 1526887780000 ms
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1526887780000 ms
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887782200
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:29:42] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:29:42] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 38.0 (TID 22)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN]  [2018-05-21 15:29:42] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 38.0 (TID 22, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:29:42] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 38.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 38.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 38
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.scheduler.DAGScheduler]ResultStage 38 (start at SparkStreamingT.java:107) failed in 0.408 s
[ERROR]  [2018-05-21 15:29:42] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 38.0 failed 1 times, most recent failure: Lost task 0.0 in stage 38.0 (TID 22, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.scheduler.DAGScheduler]Got job 25 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 43 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 43 (Receiver 0 ParallelCollectionRDD[43] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_25 stored as values in memory (estimated size 34.1 KB, free 901.4 MB)
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_25_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.4 MB)
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_25_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.SparkContext]Created broadcast 25 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 43 (Receiver 0 ParallelCollectionRDD[43] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 43.0 with 1 tasks
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 43.0 (TID 25, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.executor.Executor]Running task 0.0 in stage 43.0 (TID 25)
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887782600
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[ERROR]  [2018-05-21 15:29:42] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:29:42] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887782800
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:29:42] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:29:42] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 43.0 (TID 25)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN]  [2018-05-21 15:29:42] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 43.0 (TID 25, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:29:42] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 43.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 43.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 43
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.scheduler.DAGScheduler]ResultStage 43 (start at SparkStreamingT.java:107) failed in 0.449 s
[ERROR]  [2018-05-21 15:29:42] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 43.0 failed 1 times, most recent failure: Lost task 0.0 in stage 43.0 (TID 25, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.scheduler.DAGScheduler]Got job 26 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 44 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 44 (Receiver 0 ParallelCollectionRDD[44] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_26 stored as values in memory (estimated size 34.1 KB, free 901.4 MB)
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_26_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.4 MB)
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_26_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.ContextCleaner]Cleaned shuffle 5
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.SparkContext]Created broadcast 26 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 44 (Receiver 0 ParallelCollectionRDD[44] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 44.0 with 1 tasks
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 44.0 (TID 26, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.executor.Executor]Running task 0.0 in stage 44.0 (TID 26)
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_13_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887783000
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.ContextCleaner]Cleaned shuffle 6
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_14_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.7 MB)
[ERROR]  [2018-05-21 15:29:42] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:29:42] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_15_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_16_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_17_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.ContextCleaner]Cleaned shuffle 7
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_18_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_19_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_20_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_21_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_22_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_23_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_24_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:29:42] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_25_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526887783000 ms
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526887783000 ms.0 from job set of time 1526887783000 ms
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 47 (mapToPair at SparkStreamingT.java:87)
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.scheduler.DAGScheduler]Got job 27 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 46 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 45)
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 46 (ShuffledRDD[48] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_27 stored as values in memory (estimated size 3.8 KB, free 901.7 MB)
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_27_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.7 MB)
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_27_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.SparkContext]Created broadcast 27 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 46 (ShuffledRDD[48] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 46.0 with 1 tasks
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 46.0 (TID 27, localhost, partition 0, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.executor.Executor]Running task 0.0 in stage 46.0 (TID 27)
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 46.0 (TID 27). 1549 bytes result sent to driver
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 46.0 (TID 27) in 25 ms on localhost (1/1)
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 46.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.scheduler.DAGScheduler]ResultStage 46 (print at SparkStreamingT.java:101) finished in 0.028 s
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.scheduler.DAGScheduler]Job 27 finished: print at SparkStreamingT.java:101, took 0.036897 s
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 9 is 83 bytes
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.scheduler.DAGScheduler]Got job 28 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 48 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 47)
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 48 (ShuffledRDD[48] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_28 stored as values in memory (estimated size 3.8 KB, free 901.7 MB)
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_28_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.7 MB)
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_28_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.SparkContext]Created broadcast 28 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 48 (ShuffledRDD[48] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 48.0 with 1 tasks
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 48.0 (TID 28, localhost, partition 1, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.executor.Executor]Running task 0.0 in stage 48.0 (TID 28)
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 48.0 (TID 28). 1549 bytes result sent to driver
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 48.0 (TID 28) in 7 ms on localhost (1/1)
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 48.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.scheduler.DAGScheduler]ResultStage 48 (print at SparkStreamingT.java:101) finished in 0.013 s
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.scheduler.DAGScheduler]Job 28 finished: print at SparkStreamingT.java:101, took 0.052279 s
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526887783000 ms.0 from job set of time 1526887783000 ms
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.109 s for time 1526887783000 ms (execution: 0.101 s)
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 42 from persistence list
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.storage.BlockManager]Removing RDD 42
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 41 from persistence list
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.storage.BlockManager]Removing RDD 41
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 40 from persistence list
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.rdd.BlockRDD]Removing RDD 39 from persistence list
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.storage.BlockManager]Removing RDD 40
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.streaming.dstream.SocketInputDStream]Removing blocks of RDD BlockRDD[39] at socketTextStream at SparkStreamingT.java:72 of time 1526887783000 ms
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 1526887781000 ms
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1526887781000 ms
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.storage.BlockManager]Removing RDD 39
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887783200
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:29:43] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:29:43] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 44.0 (TID 26)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN]  [2018-05-21 15:29:43] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 44.0 (TID 26, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:29:43] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 44.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 44.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 44
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.scheduler.DAGScheduler]ResultStage 44 (start at SparkStreamingT.java:107) failed in 0.326 s
[ERROR]  [2018-05-21 15:29:43] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 44.0 failed 1 times, most recent failure: Lost task 0.0 in stage 44.0 (TID 26, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.scheduler.DAGScheduler]Got job 29 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 49 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 49 (Receiver 0 ParallelCollectionRDD[49] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_29 stored as values in memory (estimated size 34.1 KB, free 901.7 MB)
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_29_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.7 MB)
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_29_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.SparkContext]Created broadcast 29 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 49 (Receiver 0 ParallelCollectionRDD[49] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 49.0 with 1 tasks
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 49.0 (TID 29, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.executor.Executor]Running task 0.0 in stage 49.0 (TID 29)
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887783400
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[ERROR]  [2018-05-21 15:29:43] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:29:43] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887783600
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:29:43] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:29:43] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 49.0 (TID 29)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN]  [2018-05-21 15:29:43] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 49.0 (TID 29, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:29:43] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 49.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 49.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 49
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.scheduler.DAGScheduler]ResultStage 49 (start at SparkStreamingT.java:107) failed in 0.344 s
[ERROR]  [2018-05-21 15:29:43] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 49.0 failed 1 times, most recent failure: Lost task 0.0 in stage 49.0 (TID 29, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.scheduler.DAGScheduler]Got job 30 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 50 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 50 (Receiver 0 ParallelCollectionRDD[50] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_30 stored as values in memory (estimated size 34.1 KB, free 901.6 MB)
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_30_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.6 MB)
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_30_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.SparkContext]Created broadcast 30 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 50 (Receiver 0 ParallelCollectionRDD[50] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 50.0 with 1 tasks
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 50.0 (TID 30, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.executor.Executor]Running task 0.0 in stage 50.0 (TID 30)
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887783800
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[ERROR]  [2018-05-21 15:29:43] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:29:43] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:29:43] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887784000
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:29:44] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:29:44] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 50.0 (TID 30)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN]  [2018-05-21 15:29:44] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 50.0 (TID 30, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:29:44] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 50.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 50.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 50
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.scheduler.DAGScheduler]ResultStage 50 (start at SparkStreamingT.java:107) failed in 0.356 s
[ERROR]  [2018-05-21 15:29:44] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 50.0 failed 1 times, most recent failure: Lost task 0.0 in stage 50.0 (TID 30, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526887784000 ms.0 from job set of time 1526887784000 ms
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 53 (mapToPair at SparkStreamingT.java:87)
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.scheduler.DAGScheduler]Got job 31 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 52 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 51)
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 52 (ShuffledRDD[54] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_31 stored as values in memory (estimated size 3.8 KB, free 901.6 MB)
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_31_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.6 MB)
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526887784000 ms
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_31_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.SparkContext]Created broadcast 31 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 52 (ShuffledRDD[54] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 52.0 with 1 tasks
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.scheduler.DAGScheduler]Got job 32 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 53 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 53 (Receiver 0 ParallelCollectionRDD[55] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_32 stored as values in memory (estimated size 34.1 KB, free 901.6 MB)
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_32_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.6 MB)
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_32_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 52.0 (TID 31, localhost, partition 0, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.SparkContext]Created broadcast 32 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 53 (Receiver 0 ParallelCollectionRDD[55] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 53.0 with 1 tasks
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.executor.Executor]Running task 0.0 in stage 52.0 (TID 31)
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 53.0 (TID 32, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 52.0 (TID 31). 1470 bytes result sent to driver
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.executor.Executor]Running task 0.0 in stage 53.0 (TID 32)
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 52.0 (TID 31) in 21 ms on localhost (1/1)
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 52.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887784200
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.scheduler.DAGScheduler]ResultStage 52 (print at SparkStreamingT.java:101) finished in 0.030 s
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.scheduler.DAGScheduler]Job 31 finished: print at SparkStreamingT.java:101, took 0.054538 s
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 10 is 83 bytes
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.scheduler.DAGScheduler]Got job 33 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 55 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 54)
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 55 (ShuffledRDD[54] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_33 stored as values in memory (estimated size 3.8 KB, free 901.6 MB)
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_33_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.6 MB)
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_33_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.SparkContext]Created broadcast 33 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 55 (ShuffledRDD[54] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 55.0 with 1 tasks
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 55.0 (TID 33, localhost, partition 1, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.executor.Executor]Running task 0.0 in stage 55.0 (TID 33)
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[ERROR]  [2018-05-21 15:29:44] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:29:44] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 55.0 (TID 33). 1549 bytes result sent to driver
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.scheduler.DAGScheduler]ResultStage 55 (print at SparkStreamingT.java:101) finished in 0.015 s
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 55.0 (TID 33) in 13 ms on localhost (1/1)
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 55.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.scheduler.DAGScheduler]Job 33 finished: print at SparkStreamingT.java:101, took 0.026353 s
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526887784000 ms.0 from job set of time 1526887784000 ms
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.113 s for time 1526887784000 ms (execution: 0.092 s)
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 48 from persistence list
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.storage.BlockManager]Removing RDD 48
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 47 from persistence list
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 46 from persistence list
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.storage.BlockManager]Removing RDD 47
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.storage.BlockManager]Removing RDD 46
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.rdd.BlockRDD]Removing RDD 45 from persistence list
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.streaming.dstream.SocketInputDStream]Removing blocks of RDD BlockRDD[45] at socketTextStream at SparkStreamingT.java:72 of time 1526887784000 ms
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 1526887782000 ms
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1526887782000 ms
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.storage.BlockManager]Removing RDD 45
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887784400
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:29:44] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:29:44] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 53.0 (TID 32)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN]  [2018-05-21 15:29:44] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 53.0 (TID 32, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:29:44] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 53.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 53.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 53
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.scheduler.DAGScheduler]ResultStage 53 (start at SparkStreamingT.java:107) failed in 0.346 s
[ERROR]  [2018-05-21 15:29:44] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 53.0 failed 1 times, most recent failure: Lost task 0.0 in stage 53.0 (TID 32, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.scheduler.DAGScheduler]Got job 34 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 56 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 56 (Receiver 0 ParallelCollectionRDD[56] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_34 stored as values in memory (estimated size 34.1 KB, free 901.5 MB)
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_34_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.5 MB)
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_34_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.SparkContext]Created broadcast 34 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 56 (Receiver 0 ParallelCollectionRDD[56] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 56.0 with 1 tasks
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 56.0 (TID 34, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.executor.Executor]Running task 0.0 in stage 56.0 (TID 34)
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887784600
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[ERROR]  [2018-05-21 15:29:44] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:29:44] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887784600
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:29:44] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:29:44] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 56.0 (TID 34)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN]  [2018-05-21 15:29:44] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 56.0 (TID 34, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:29:44] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 56.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 56.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 56
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.scheduler.DAGScheduler]ResultStage 56 (start at SparkStreamingT.java:107) failed in 0.198 s
[ERROR]  [2018-05-21 15:29:44] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 56.0 failed 1 times, most recent failure: Lost task 0.0 in stage 56.0 (TID 34, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.scheduler.DAGScheduler]Got job 35 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 57 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 57 (Receiver 0 ParallelCollectionRDD[57] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_35 stored as values in memory (estimated size 34.1 KB, free 901.5 MB)
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_35_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.5 MB)
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_35_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.SparkContext]Created broadcast 35 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 57 (Receiver 0 ParallelCollectionRDD[57] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 57.0 with 1 tasks
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 57.0 (TID 35, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.executor.Executor]Running task 0.0 in stage 57.0 (TID 35)
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887784800
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[ERROR]  [2018-05-21 15:29:44] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:29:44] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:29:44] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887785000
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:29:45] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:29:45] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 57.0 (TID 35)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN]  [2018-05-21 15:29:45] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 57.0 (TID 35, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:29:45] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 57.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 57.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 57
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.scheduler.DAGScheduler]ResultStage 57 (start at SparkStreamingT.java:107) failed in 0.315 s
[ERROR]  [2018-05-21 15:29:45] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 57.0 failed 1 times, most recent failure: Lost task 0.0 in stage 57.0 (TID 35, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526887785000 ms.0 from job set of time 1526887785000 ms
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526887785000 ms
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 60 (mapToPair at SparkStreamingT.java:87)
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.scheduler.DAGScheduler]Got job 36 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 59 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 58)
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 59 (ShuffledRDD[61] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_36 stored as values in memory (estimated size 3.8 KB, free 901.5 MB)
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_36_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.5 MB)
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_36_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.SparkContext]Created broadcast 36 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 59 (ShuffledRDD[61] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 59.0 with 1 tasks
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 59.0 (TID 36, localhost, partition 0, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.executor.Executor]Running task 0.0 in stage 59.0 (TID 36)
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.scheduler.DAGScheduler]Got job 37 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 60 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 60 (Receiver 0 ParallelCollectionRDD[62] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 59.0 (TID 36). 1549 bytes result sent to driver
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 59.0 (TID 36) in 6 ms on localhost (1/1)
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 59.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_37 stored as values in memory (estimated size 34.1 KB, free 901.4 MB)
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_37_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.4 MB)
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_37_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.SparkContext]Created broadcast 37 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 60 (Receiver 0 ParallelCollectionRDD[62] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 60.0 with 1 tasks
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.scheduler.DAGScheduler]ResultStage 59 (print at SparkStreamingT.java:101) finished in 0.013 s
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 60.0 (TID 37, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.scheduler.DAGScheduler]Job 36 finished: print at SparkStreamingT.java:101, took 0.023086 s
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.executor.Executor]Running task 0.0 in stage 60.0 (TID 37)
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 11 is 83 bytes
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.scheduler.DAGScheduler]Got job 38 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 62 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 61)
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 62 (ShuffledRDD[61] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887785200
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_38 stored as values in memory (estimated size 3.8 KB, free 901.4 MB)
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_38_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.4 MB)
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_38_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.SparkContext]Created broadcast 38 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[ERROR]  [2018-05-21 15:29:45] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:29:45] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 62 (ShuffledRDD[61] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 62.0 with 1 tasks
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 62.0 (TID 38, localhost, partition 1, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.executor.Executor]Running task 0.0 in stage 62.0 (TID 38)
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 62.0 (TID 38). 1549 bytes result sent to driver
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 62.0 (TID 38) in 8 ms on localhost (1/1)
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 62.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.scheduler.DAGScheduler]ResultStage 62 (print at SparkStreamingT.java:101) finished in 0.008 s
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.scheduler.DAGScheduler]Job 38 finished: print at SparkStreamingT.java:101, took 0.024185 s
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526887785000 ms.0 from job set of time 1526887785000 ms
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.072 s for time 1526887785000 ms (execution: 0.061 s)
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 54 from persistence list
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.storage.BlockManager]Removing RDD 54
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 53 from persistence list
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.storage.BlockManager]Removing RDD 53
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 52 from persistence list
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.storage.BlockManager]Removing RDD 52
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.rdd.BlockRDD]Removing RDD 51 from persistence list
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.storage.BlockManager]Removing RDD 51
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.streaming.dstream.SocketInputDStream]Removing blocks of RDD BlockRDD[51] at socketTextStream at SparkStreamingT.java:72 of time 1526887785000 ms
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 1526887783000 ms
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1526887783000 ms
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887785400
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:29:45] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:29:45] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 60.0 (TID 37)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN]  [2018-05-21 15:29:45] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 60.0 (TID 37, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:29:45] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 60.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 60.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 60
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.scheduler.DAGScheduler]ResultStage 60 (start at SparkStreamingT.java:107) failed in 0.365 s
[ERROR]  [2018-05-21 15:29:45] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 60.0 failed 1 times, most recent failure: Lost task 0.0 in stage 60.0 (TID 37, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.scheduler.DAGScheduler]Got job 39 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 63 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 63 (Receiver 0 ParallelCollectionRDD[63] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_39 stored as values in memory (estimated size 34.1 KB, free 901.4 MB)
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_39_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.4 MB)
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_39_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.SparkContext]Created broadcast 39 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 63 (Receiver 0 ParallelCollectionRDD[63] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 63.0 with 1 tasks
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 63.0 (TID 39, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.executor.Executor]Running task 0.0 in stage 63.0 (TID 39)
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887785600
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[ERROR]  [2018-05-21 15:29:45] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:29:45] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887785800
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:29:45] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:29:45] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 63.0 (TID 39)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN]  [2018-05-21 15:29:45] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 63.0 (TID 39, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:29:45] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 63.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 63.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 63
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.scheduler.DAGScheduler]ResultStage 63 (start at SparkStreamingT.java:107) failed in 0.382 s
[ERROR]  [2018-05-21 15:29:45] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 63.0 failed 1 times, most recent failure: Lost task 0.0 in stage 63.0 (TID 39, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.scheduler.DAGScheduler]Got job 40 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 64 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 64 (Receiver 0 ParallelCollectionRDD[64] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_40 stored as values in memory (estimated size 34.1 KB, free 901.3 MB)
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_40_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.3 MB)
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_40_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.SparkContext]Created broadcast 40 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 64 (Receiver 0 ParallelCollectionRDD[64] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 64.0 with 1 tasks
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 64.0 (TID 40, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:29:45] [org.apache.spark.executor.Executor]Running task 0.0 in stage 64.0 (TID 40)
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526887786000 ms
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887786000
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[ERROR]  [2018-05-21 15:29:46] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:29:46] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526887786000 ms.0 from job set of time 1526887786000 ms
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 67 (mapToPair at SparkStreamingT.java:87)
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.scheduler.DAGScheduler]Got job 41 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 66 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 65)
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 66 (ShuffledRDD[68] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_41 stored as values in memory (estimated size 3.8 KB, free 901.3 MB)
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_41_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.3 MB)
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_41_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.SparkContext]Created broadcast 41 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 66 (ShuffledRDD[68] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 66.0 with 1 tasks
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 66.0 (TID 41, localhost, partition 0, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.executor.Executor]Running task 0.0 in stage 66.0 (TID 41)
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 66.0 (TID 41). 1549 bytes result sent to driver
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 66.0 (TID 41) in 5 ms on localhost (1/1)
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 66.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.scheduler.DAGScheduler]ResultStage 66 (print at SparkStreamingT.java:101) finished in 0.009 s
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.scheduler.DAGScheduler]Job 41 finished: print at SparkStreamingT.java:101, took 0.028410 s
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 12 is 83 bytes
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.scheduler.DAGScheduler]Got job 42 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 68 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 67)
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 68 (ShuffledRDD[68] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_42 stored as values in memory (estimated size 3.8 KB, free 901.3 MB)
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_42_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.3 MB)
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_42_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.SparkContext]Created broadcast 42 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 68 (ShuffledRDD[68] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 68.0 with 1 tasks
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 68.0 (TID 42, localhost, partition 1, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.executor.Executor]Running task 0.0 in stage 68.0 (TID 42)
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 68.0 (TID 42). 1549 bytes result sent to driver
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 68.0 (TID 42) in 12 ms on localhost (1/1)
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 68.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.scheduler.DAGScheduler]ResultStage 68 (print at SparkStreamingT.java:101) finished in 0.020 s
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.scheduler.DAGScheduler]Job 42 finished: print at SparkStreamingT.java:101, took 0.027389 s
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526887786000 ms.0 from job set of time 1526887786000 ms
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.133 s for time 1526887786000 ms (execution: 0.055 s)
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 61 from persistence list
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.storage.BlockManager]Removing RDD 61
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 60 from persistence list
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 59 from persistence list
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.storage.BlockManager]Removing RDD 60
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.storage.BlockManager]Removing RDD 59
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.rdd.BlockRDD]Removing RDD 58 from persistence list
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.streaming.dstream.SocketInputDStream]Removing blocks of RDD BlockRDD[58] at socketTextStream at SparkStreamingT.java:72 of time 1526887786000 ms
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.storage.BlockManager]Removing RDD 58
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 1526887784000 ms
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1526887784000 ms
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887786200
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:29:46] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:29:46] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 64.0 (TID 40)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN]  [2018-05-21 15:29:46] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 64.0 (TID 40, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:29:46] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 64.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 64.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 64
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.scheduler.DAGScheduler]ResultStage 64 (start at SparkStreamingT.java:107) failed in 0.383 s
[ERROR]  [2018-05-21 15:29:46] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 64.0 failed 1 times, most recent failure: Lost task 0.0 in stage 64.0 (TID 40, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.scheduler.DAGScheduler]Got job 43 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 69 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 69 (Receiver 0 ParallelCollectionRDD[69] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_43 stored as values in memory (estimated size 34.1 KB, free 901.3 MB)
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_43_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.3 MB)
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_43_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.SparkContext]Created broadcast 43 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 69 (Receiver 0 ParallelCollectionRDD[69] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 69.0 with 1 tasks
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 69.0 (TID 43, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.executor.Executor]Running task 0.0 in stage 69.0 (TID 43)
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887786400
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[ERROR]  [2018-05-21 15:29:46] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:29:46] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887786600
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:29:46] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:29:46] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 69.0 (TID 43)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN]  [2018-05-21 15:29:46] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 69.0 (TID 43, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:29:46] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 69.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 69.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 69
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.scheduler.DAGScheduler]ResultStage 69 (start at SparkStreamingT.java:107) failed in 0.380 s
[ERROR]  [2018-05-21 15:29:46] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 69.0 failed 1 times, most recent failure: Lost task 0.0 in stage 69.0 (TID 43, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.scheduler.DAGScheduler]Got job 44 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 70 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 70 (Receiver 0 ParallelCollectionRDD[70] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_44 stored as values in memory (estimated size 34.1 KB, free 901.2 MB)
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_44_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.2 MB)
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_44_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.6 MB)
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.SparkContext]Created broadcast 44 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 70 (Receiver 0 ParallelCollectionRDD[70] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 70.0 with 1 tasks
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 70.0 (TID 44, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.executor.Executor]Running task 0.0 in stage 70.0 (TID 44)
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887786800
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[ERROR]  [2018-05-21 15:29:46] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:29:46] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:29:46] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887787000
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:29:47] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:29:47] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 70.0 (TID 44)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN]  [2018-05-21 15:29:47] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 70.0 (TID 44, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:29:47] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 70.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 70.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 70
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.scheduler.DAGScheduler]ResultStage 70 (start at SparkStreamingT.java:107) failed in 0.378 s
[ERROR]  [2018-05-21 15:29:47] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 70.0 failed 1 times, most recent failure: Lost task 0.0 in stage 70.0 (TID 44, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526887787000 ms
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526887787000 ms.0 from job set of time 1526887787000 ms
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 73 (mapToPair at SparkStreamingT.java:87)
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.scheduler.DAGScheduler]Got job 45 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 72 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 71)
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 72 (ShuffledRDD[74] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_45 stored as values in memory (estimated size 3.8 KB, free 901.2 MB)
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_45_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.2 MB)
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_45_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.6 MB)
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.SparkContext]Created broadcast 45 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 72 (ShuffledRDD[74] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 72.0 with 1 tasks
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.scheduler.DAGScheduler]Got job 46 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 73 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 72.0 (TID 45, localhost, partition 0, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 73 (Receiver 0 ParallelCollectionRDD[75] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.executor.Executor]Running task 0.0 in stage 72.0 (TID 45)
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 72.0 (TID 45). 1622 bytes result sent to driver
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_26_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 72.0 (TID 45) in 25 ms on localhost (1/1)
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_46 stored as values in memory (estimated size 34.1 KB, free 901.2 MB)
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 72.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.ContextCleaner]Cleaned shuffle 9
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_46_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.2 MB)
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_46_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.6 MB)
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.SparkContext]Created broadcast 46 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_27_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.6 MB)
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 73 (Receiver 0 ParallelCollectionRDD[75] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 73.0 with 1 tasks
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.scheduler.DAGScheduler]ResultStage 72 (print at SparkStreamingT.java:101) finished in 0.031 s
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.scheduler.DAGScheduler]Job 45 finished: print at SparkStreamingT.java:101, took 0.042671 s
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 73.0 (TID 46, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.executor.Executor]Running task 0.0 in stage 73.0 (TID 46)
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_28_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_29_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 13 is 83 bytes
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.scheduler.DAGScheduler]Got job 47 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 75 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 74)
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_30_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 75 (ShuffledRDD[74] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887787200
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_47 stored as values in memory (estimated size 3.8 KB, free 901.3 MB)
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.ContextCleaner]Cleaned shuffle 10
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[ERROR]  [2018-05-21 15:29:47] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:29:47] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_47_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.3 MB)
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_31_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_47_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.SparkContext]Created broadcast 47 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 75 (ShuffledRDD[74] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 75.0 with 1 tasks
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_32_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 75.0 (TID 47, localhost, partition 1, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.executor.Executor]Running task 0.0 in stage 75.0 (TID 47)
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_33_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 75.0 (TID 47). 1549 bytes result sent to driver
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_34_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 75.0 (TID 47) in 5 ms on localhost (1/1)
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 75.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.scheduler.DAGScheduler]ResultStage 75 (print at SparkStreamingT.java:101) finished in 0.006 s
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.scheduler.DAGScheduler]Job 47 finished: print at SparkStreamingT.java:101, took 0.013832 s
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526887787000 ms.0 from job set of time 1526887787000 ms
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.100 s for time 1526887787000 ms (execution: 0.080 s)
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 68 from persistence list
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_35_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.storage.BlockManager]Removing RDD 68
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 67 from persistence list
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.storage.BlockManager]Removing RDD 67
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 66 from persistence list
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.ContextCleaner]Cleaned shuffle 11
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.rdd.BlockRDD]Removing RDD 65 from persistence list
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.storage.BlockManager]Removing RDD 66
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.storage.BlockManager]Removing RDD 65
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.streaming.dstream.SocketInputDStream]Removing blocks of RDD BlockRDD[65] at socketTextStream at SparkStreamingT.java:72 of time 1526887787000 ms
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 1526887785000 ms
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1526887785000 ms
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_36_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_37_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_38_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_39_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_40_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_41_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_42_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_43_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_44_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.ContextCleaner]Cleaned shuffle 8
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887787400
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:29:47] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:29:47] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 73.0 (TID 46)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN]  [2018-05-21 15:29:47] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 73.0 (TID 46, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:29:47] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 73.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 73.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 73
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.scheduler.DAGScheduler]ResultStage 73 (start at SparkStreamingT.java:107) failed in 0.337 s
[ERROR]  [2018-05-21 15:29:47] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 73.0 failed 1 times, most recent failure: Lost task 0.0 in stage 73.0 (TID 46, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.scheduler.DAGScheduler]Got job 48 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 76 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 76 (Receiver 0 ParallelCollectionRDD[76] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_48 stored as values in memory (estimated size 34.1 KB, free 901.7 MB)
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_48_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.7 MB)
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_48_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.SparkContext]Created broadcast 48 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 76 (Receiver 0 ParallelCollectionRDD[76] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 76.0 with 1 tasks
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 76.0 (TID 48, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.executor.Executor]Running task 0.0 in stage 76.0 (TID 48)
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887787600
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[ERROR]  [2018-05-21 15:29:47] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:29:47] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887787800
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:29:47] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:29:47] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 76.0 (TID 48)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN]  [2018-05-21 15:29:47] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 76.0 (TID 48, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:29:47] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 76.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 76.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 76
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.scheduler.DAGScheduler]ResultStage 76 (start at SparkStreamingT.java:107) failed in 0.363 s
[ERROR]  [2018-05-21 15:29:47] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 76.0 failed 1 times, most recent failure: Lost task 0.0 in stage 76.0 (TID 48, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.scheduler.DAGScheduler]Got job 49 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 77 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 77 (Receiver 0 ParallelCollectionRDD[77] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_49 stored as values in memory (estimated size 34.1 KB, free 901.6 MB)
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_49_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.6 MB)
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_49_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.SparkContext]Created broadcast 49 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 77 (Receiver 0 ParallelCollectionRDD[77] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 77.0 with 1 tasks
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 77.0 (TID 49, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.executor.Executor]Running task 0.0 in stage 77.0 (TID 49)
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887788000
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[ERROR]  [2018-05-21 15:29:47] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:29:47] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:29:47] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526887788000 ms.0 from job set of time 1526887788000 ms
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526887788000 ms
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 80 (mapToPair at SparkStreamingT.java:87)
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.scheduler.DAGScheduler]Got job 50 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 79 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 78)
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 79 (ShuffledRDD[81] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_50 stored as values in memory (estimated size 3.8 KB, free 901.6 MB)
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_50_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.6 MB)
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_50_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.SparkContext]Created broadcast 50 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 79 (ShuffledRDD[81] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 79.0 with 1 tasks
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 79.0 (TID 50, localhost, partition 0, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.executor.Executor]Running task 0.0 in stage 79.0 (TID 50)
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 79.0 (TID 50). 1549 bytes result sent to driver
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 79.0 (TID 50) in 4 ms on localhost (1/1)
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 79.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.scheduler.DAGScheduler]ResultStage 79 (print at SparkStreamingT.java:101) finished in 0.006 s
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.scheduler.DAGScheduler]Job 50 finished: print at SparkStreamingT.java:101, took 0.021232 s
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 14 is 83 bytes
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.scheduler.DAGScheduler]Got job 51 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 81 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 80)
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 81 (ShuffledRDD[81] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_51 stored as values in memory (estimated size 3.8 KB, free 901.6 MB)
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_51_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.6 MB)
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_51_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.SparkContext]Created broadcast 51 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 81 (ShuffledRDD[81] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 81.0 with 1 tasks
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 81.0 (TID 51, localhost, partition 1, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.executor.Executor]Running task 0.0 in stage 81.0 (TID 51)
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 81.0 (TID 51). 1549 bytes result sent to driver
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 81.0 (TID 51) in 5 ms on localhost (1/1)
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 81.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.scheduler.DAGScheduler]ResultStage 81 (print at SparkStreamingT.java:101) finished in 0.006 s
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.scheduler.DAGScheduler]Job 51 finished: print at SparkStreamingT.java:101, took 0.012613 s
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526887788000 ms.0 from job set of time 1526887788000 ms
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.069 s for time 1526887788000 ms (execution: 0.053 s)
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 74 from persistence list
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 73 from persistence list
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.storage.BlockManager]Removing RDD 74
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 72 from persistence list
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.storage.BlockManager]Removing RDD 73
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.rdd.BlockRDD]Removing RDD 71 from persistence list
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.storage.BlockManager]Removing RDD 72
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.storage.BlockManager]Removing RDD 71
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.streaming.dstream.SocketInputDStream]Removing blocks of RDD BlockRDD[71] at socketTextStream at SparkStreamingT.java:72 of time 1526887788000 ms
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 1526887786000 ms
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1526887786000 ms
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887788200
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:29:48] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:29:48] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 77.0 (TID 49)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN]  [2018-05-21 15:29:48] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 77.0 (TID 49, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:29:48] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 77.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 77.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 77
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.scheduler.DAGScheduler]ResultStage 77 (start at SparkStreamingT.java:107) failed in 0.348 s
[ERROR]  [2018-05-21 15:29:48] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 77.0 failed 1 times, most recent failure: Lost task 0.0 in stage 77.0 (TID 49, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.scheduler.DAGScheduler]Got job 52 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 82 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 82 (Receiver 0 ParallelCollectionRDD[82] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_52 stored as values in memory (estimated size 34.1 KB, free 901.6 MB)
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_52_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.6 MB)
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_52_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.SparkContext]Created broadcast 52 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 82 (Receiver 0 ParallelCollectionRDD[82] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 82.0 with 1 tasks
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 82.0 (TID 52, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.executor.Executor]Running task 0.0 in stage 82.0 (TID 52)
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887788400
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[ERROR]  [2018-05-21 15:29:48] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:29:48] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887788600
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:29:48] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:29:48] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 82.0 (TID 52)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN]  [2018-05-21 15:29:48] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 82.0 (TID 52, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:29:48] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 82.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 82.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 82
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.scheduler.DAGScheduler]ResultStage 82 (start at SparkStreamingT.java:107) failed in 0.368 s
[ERROR]  [2018-05-21 15:29:48] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 82.0 failed 1 times, most recent failure: Lost task 0.0 in stage 82.0 (TID 52, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.scheduler.DAGScheduler]Got job 53 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 83 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 83 (Receiver 0 ParallelCollectionRDD[83] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_53 stored as values in memory (estimated size 34.1 KB, free 901.5 MB)
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_53_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.5 MB)
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_53_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.SparkContext]Created broadcast 53 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 83 (Receiver 0 ParallelCollectionRDD[83] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 83.0 with 1 tasks
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 83.0 (TID 53, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.executor.Executor]Running task 0.0 in stage 83.0 (TID 53)
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887788800
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[ERROR]  [2018-05-21 15:29:48] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:29:48] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:29:48] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887789000
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526887789000 ms
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526887789000 ms.0 from job set of time 1526887789000 ms
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:29:49] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:29:49] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 83.0 (TID 53)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 86 (mapToPair at SparkStreamingT.java:87)
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.scheduler.DAGScheduler]Got job 54 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 85 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 84)
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 85 (ShuffledRDD[87] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_54 stored as values in memory (estimated size 3.8 KB, free 901.5 MB)
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_54_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.5 MB)
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_54_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.SparkContext]Created broadcast 54 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 85 (ShuffledRDD[87] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 85.0 with 1 tasks
[WARN]  [2018-05-21 15:29:49] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 83.0 (TID 53, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:29:49] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 83.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 83.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 83
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.scheduler.DAGScheduler]ResultStage 83 (start at SparkStreamingT.java:107) failed in 0.373 s
[ERROR]  [2018-05-21 15:29:49] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 83.0 failed 1 times, most recent failure: Lost task 0.0 in stage 83.0 (TID 53, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 85.0 (TID 54, localhost, partition 0, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.executor.Executor]Running task 0.0 in stage 85.0 (TID 54)
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 85.0 (TID 54). 1470 bytes result sent to driver
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.scheduler.DAGScheduler]Got job 55 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 86 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 86 (Receiver 0 ParallelCollectionRDD[88] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 85.0 (TID 54) in 10 ms on localhost (1/1)
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 85.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_55 stored as values in memory (estimated size 34.1 KB, free 901.5 MB)
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_55_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.5 MB)
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_55_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.SparkContext]Created broadcast 55 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 86 (Receiver 0 ParallelCollectionRDD[88] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 86.0 with 1 tasks
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.scheduler.DAGScheduler]ResultStage 85 (print at SparkStreamingT.java:101) finished in 0.020 s
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 86.0 (TID 55, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.executor.Executor]Running task 0.0 in stage 86.0 (TID 55)
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.scheduler.DAGScheduler]Job 54 finished: print at SparkStreamingT.java:101, took 0.024883 s
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 15 is 83 bytes
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887789200
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.scheduler.DAGScheduler]Got job 56 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 88 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 87)
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 88 (ShuffledRDD[87] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[ERROR]  [2018-05-21 15:29:49] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:29:49] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_56 stored as values in memory (estimated size 3.8 KB, free 901.5 MB)
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_56_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.5 MB)
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_56_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.SparkContext]Created broadcast 56 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 88 (ShuffledRDD[87] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 88.0 with 1 tasks
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 88.0 (TID 56, localhost, partition 1, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.executor.Executor]Running task 0.0 in stage 88.0 (TID 56)
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 88.0 (TID 56). 1470 bytes result sent to driver
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 88.0 (TID 56) in 2 ms on localhost (1/1)
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 88.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.scheduler.DAGScheduler]ResultStage 88 (print at SparkStreamingT.java:101) finished in 0.002 s
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.scheduler.DAGScheduler]Job 56 finished: print at SparkStreamingT.java:101, took 0.009103 s
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526887789000 ms.0 from job set of time 1526887789000 ms
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.050 s for time 1526887789000 ms (execution: 0.045 s)
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 81 from persistence list
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.storage.BlockManager]Removing RDD 81
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 80 from persistence list
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 79 from persistence list
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.storage.BlockManager]Removing RDD 80
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.storage.BlockManager]Removing RDD 79
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.rdd.BlockRDD]Removing RDD 78 from persistence list
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.storage.BlockManager]Removing RDD 78
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.streaming.dstream.SocketInputDStream]Removing blocks of RDD BlockRDD[78] at socketTextStream at SparkStreamingT.java:72 of time 1526887789000 ms
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 1526887787000 ms
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1526887787000 ms
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887789400
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:29:49] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:29:49] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 86.0 (TID 55)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN]  [2018-05-21 15:29:49] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 86.0 (TID 55, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:29:49] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 86.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 86.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 86
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.scheduler.DAGScheduler]ResultStage 86 (start at SparkStreamingT.java:107) failed in 0.377 s
[ERROR]  [2018-05-21 15:29:49] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 86.0 failed 1 times, most recent failure: Lost task 0.0 in stage 86.0 (TID 55, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.scheduler.DAGScheduler]Got job 57 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 89 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 89 (Receiver 0 ParallelCollectionRDD[89] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_57 stored as values in memory (estimated size 34.1 KB, free 901.4 MB)
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_57_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.4 MB)
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_57_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.SparkContext]Created broadcast 57 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 89 (Receiver 0 ParallelCollectionRDD[89] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 89.0 with 1 tasks
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 89.0 (TID 57, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.executor.Executor]Running task 0.0 in stage 89.0 (TID 57)
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887789600
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:29:49] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:29:50] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526887790000 ms
[INFO]  [2018-05-21 15:29:50] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526887790000 ms.0 from job set of time 1526887790000 ms
[INFO]  [2018-05-21 15:29:50] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:29:50] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 92 (mapToPair at SparkStreamingT.java:87)
[INFO]  [2018-05-21 15:29:50] [org.apache.spark.scheduler.DAGScheduler]Got job 58 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:29:50] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 91 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:29:50] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 90)
[INFO]  [2018-05-21 15:29:50] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:50] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 91 (ShuffledRDD[93] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:29:50] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_58 stored as values in memory (estimated size 3.8 KB, free 901.4 MB)
[INFO]  [2018-05-21 15:29:50] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_58_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.4 MB)
[INFO]  [2018-05-21 15:29:50] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_58_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:50] [org.apache.spark.SparkContext]Created broadcast 58 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:50] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 91 (ShuffledRDD[93] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:29:50] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 91.0 with 1 tasks
[INFO]  [2018-05-21 15:29:50] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 91.0 (TID 58, localhost, partition 0, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:29:50] [org.apache.spark.executor.Executor]Running task 0.0 in stage 91.0 (TID 58)
[INFO]  [2018-05-21 15:29:50] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:29:50] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:29:50] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 91.0 (TID 58). 1549 bytes result sent to driver
[INFO]  [2018-05-21 15:29:50] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 91.0 (TID 58) in 4 ms on localhost (1/1)
[INFO]  [2018-05-21 15:29:50] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 91.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:50] [org.apache.spark.scheduler.DAGScheduler]ResultStage 91 (print at SparkStreamingT.java:101) finished in 0.005 s
[INFO]  [2018-05-21 15:29:50] [org.apache.spark.scheduler.DAGScheduler]Job 58 finished: print at SparkStreamingT.java:101, took 0.011489 s
[INFO]  [2018-05-21 15:29:50] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:29:50] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 16 is 83 bytes
[INFO]  [2018-05-21 15:29:50] [org.apache.spark.scheduler.DAGScheduler]Got job 59 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:29:50] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 93 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:29:50] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 92)
[INFO]  [2018-05-21 15:29:50] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:50] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 93 (ShuffledRDD[93] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:29:50] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_59 stored as values in memory (estimated size 3.8 KB, free 901.4 MB)
[INFO]  [2018-05-21 15:29:50] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_59_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.4 MB)
[INFO]  [2018-05-21 15:29:50] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_59_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:50] [org.apache.spark.SparkContext]Created broadcast 59 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:50] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 93 (ShuffledRDD[93] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:29:50] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 93.0 with 1 tasks
[INFO]  [2018-05-21 15:29:50] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 93.0 (TID 59, localhost, partition 1, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:29:50] [org.apache.spark.executor.Executor]Running task 0.0 in stage 93.0 (TID 59)
[INFO]  [2018-05-21 15:29:50] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:29:50] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:29:50] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 93.0 (TID 59). 1549 bytes result sent to driver
[INFO]  [2018-05-21 15:29:50] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 93.0 (TID 59) in 4 ms on localhost (1/1)
[INFO]  [2018-05-21 15:29:50] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 93.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:50] [org.apache.spark.scheduler.DAGScheduler]ResultStage 93 (print at SparkStreamingT.java:101) finished in 0.004 s
[INFO]  [2018-05-21 15:29:50] [org.apache.spark.scheduler.DAGScheduler]Job 59 finished: print at SparkStreamingT.java:101, took 0.014447 s
[INFO]  [2018-05-21 15:29:50] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526887790000 ms.0 from job set of time 1526887790000 ms
[INFO]  [2018-05-21 15:29:50] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.053 s for time 1526887790000 ms (execution: 0.036 s)
[INFO]  [2018-05-21 15:29:50] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 87 from persistence list
[INFO]  [2018-05-21 15:29:50] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 86 from persistence list
[INFO]  [2018-05-21 15:29:50] [org.apache.spark.storage.BlockManager]Removing RDD 87
[INFO]  [2018-05-21 15:29:50] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 85 from persistence list
[INFO]  [2018-05-21 15:29:50] [org.apache.spark.storage.BlockManager]Removing RDD 86
[INFO]  [2018-05-21 15:29:50] [org.apache.spark.storage.BlockManager]Removing RDD 85
[INFO]  [2018-05-21 15:29:50] [org.apache.spark.rdd.BlockRDD]Removing RDD 84 from persistence list
[INFO]  [2018-05-21 15:29:50] [org.apache.spark.storage.BlockManager]Removing RDD 84
[INFO]  [2018-05-21 15:29:50] [org.apache.spark.streaming.dstream.SocketInputDStream]Removing blocks of RDD BlockRDD[84] at socketTextStream at SparkStreamingT.java:72 of time 1526887790000 ms
[INFO]  [2018-05-21 15:29:50] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 1526887788000 ms
[INFO]  [2018-05-21 15:29:50] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1526887788000 ms
[INFO]  [2018-05-21 15:29:51] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526887791000 ms
[INFO]  [2018-05-21 15:29:51] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526887791000 ms.0 from job set of time 1526887791000 ms
[INFO]  [2018-05-21 15:29:51] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:29:51] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 96 (mapToPair at SparkStreamingT.java:87)
[INFO]  [2018-05-21 15:29:51] [org.apache.spark.scheduler.DAGScheduler]Got job 60 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:29:51] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 95 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:29:51] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 94)
[INFO]  [2018-05-21 15:29:51] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:51] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 95 (ShuffledRDD[97] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:29:51] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_60 stored as values in memory (estimated size 3.8 KB, free 901.4 MB)
[INFO]  [2018-05-21 15:29:51] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_60_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.4 MB)
[INFO]  [2018-05-21 15:29:51] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_60_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:51] [org.apache.spark.SparkContext]Created broadcast 60 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:51] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 95 (ShuffledRDD[97] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:29:51] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 95.0 with 1 tasks
[INFO]  [2018-05-21 15:29:51] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 95.0 (TID 60, localhost, partition 0, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:29:51] [org.apache.spark.executor.Executor]Running task 0.0 in stage 95.0 (TID 60)
[INFO]  [2018-05-21 15:29:51] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:29:51] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:29:51] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 95.0 (TID 60). 1549 bytes result sent to driver
[INFO]  [2018-05-21 15:29:51] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 95.0 (TID 60) in 4 ms on localhost (1/1)
[INFO]  [2018-05-21 15:29:51] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 95.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:51] [org.apache.spark.scheduler.DAGScheduler]ResultStage 95 (print at SparkStreamingT.java:101) finished in 0.005 s
[INFO]  [2018-05-21 15:29:51] [org.apache.spark.scheduler.DAGScheduler]Job 60 finished: print at SparkStreamingT.java:101, took 0.015243 s
[INFO]  [2018-05-21 15:29:51] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:29:51] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 17 is 83 bytes
[INFO]  [2018-05-21 15:29:51] [org.apache.spark.scheduler.DAGScheduler]Got job 61 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:29:51] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 97 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:29:51] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 96)
[INFO]  [2018-05-21 15:29:51] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:51] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 97 (ShuffledRDD[97] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:29:51] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_61 stored as values in memory (estimated size 3.8 KB, free 901.4 MB)
[INFO]  [2018-05-21 15:29:51] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_61_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.4 MB)
[INFO]  [2018-05-21 15:29:51] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_61_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:51] [org.apache.spark.SparkContext]Created broadcast 61 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:51] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 97 (ShuffledRDD[97] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:29:51] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 97.0 with 1 tasks
[INFO]  [2018-05-21 15:29:51] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 97.0 (TID 61, localhost, partition 1, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:29:51] [org.apache.spark.executor.Executor]Running task 0.0 in stage 97.0 (TID 61)
[INFO]  [2018-05-21 15:29:51] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:29:51] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:29:51] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 97.0 (TID 61). 1549 bytes result sent to driver
[INFO]  [2018-05-21 15:29:51] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 97.0 (TID 61) in 2 ms on localhost (1/1)
[INFO]  [2018-05-21 15:29:51] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 97.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:51] [org.apache.spark.scheduler.DAGScheduler]ResultStage 97 (print at SparkStreamingT.java:101) finished in 0.003 s
[INFO]  [2018-05-21 15:29:51] [org.apache.spark.scheduler.DAGScheduler]Job 61 finished: print at SparkStreamingT.java:101, took 0.009218 s
[INFO]  [2018-05-21 15:29:51] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526887791000 ms.0 from job set of time 1526887791000 ms
[INFO]  [2018-05-21 15:29:51] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.051 s for time 1526887791000 ms (execution: 0.036 s)
[INFO]  [2018-05-21 15:29:51] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 93 from persistence list
[INFO]  [2018-05-21 15:29:51] [org.apache.spark.storage.BlockManager]Removing RDD 93
[INFO]  [2018-05-21 15:29:51] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 92 from persistence list
[INFO]  [2018-05-21 15:29:51] [org.apache.spark.storage.BlockManager]Removing RDD 92
[INFO]  [2018-05-21 15:29:51] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 91 from persistence list
[INFO]  [2018-05-21 15:29:51] [org.apache.spark.rdd.BlockRDD]Removing RDD 90 from persistence list
[INFO]  [2018-05-21 15:29:51] [org.apache.spark.storage.BlockManager]Removing RDD 91
[INFO]  [2018-05-21 15:29:51] [org.apache.spark.storage.BlockManager]Removing RDD 90
[INFO]  [2018-05-21 15:29:51] [org.apache.spark.streaming.dstream.SocketInputDStream]Removing blocks of RDD BlockRDD[90] at socketTextStream at SparkStreamingT.java:72 of time 1526887791000 ms
[INFO]  [2018-05-21 15:29:51] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 1526887789000 ms
[INFO]  [2018-05-21 15:29:51] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1526887789000 ms
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[ERROR]  [2018-05-21 15:29:52] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:29:52] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526887792000 ms
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526887792000 ms.0 from job set of time 1526887792000 ms
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 100 (mapToPair at SparkStreamingT.java:87)
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.scheduler.DAGScheduler]Got job 62 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 99 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 98)
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 99 (ShuffledRDD[101] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_62 stored as values in memory (estimated size 3.8 KB, free 901.4 MB)
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_62_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.4 MB)
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_62_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.SparkContext]Created broadcast 62 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 99 (ShuffledRDD[101] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 99.0 with 1 tasks
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 99.0 (TID 62, localhost, partition 0, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.executor.Executor]Running task 0.0 in stage 99.0 (TID 62)
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 99.0 (TID 62). 1549 bytes result sent to driver
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 99.0 (TID 62) in 6 ms on localhost (1/1)
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 99.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.scheduler.DAGScheduler]ResultStage 99 (print at SparkStreamingT.java:101) finished in 0.007 s
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.scheduler.DAGScheduler]Job 62 finished: print at SparkStreamingT.java:101, took 0.011577 s
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 18 is 83 bytes
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.scheduler.DAGScheduler]Got job 63 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 101 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 100)
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 101 (ShuffledRDD[101] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_63 stored as values in memory (estimated size 3.8 KB, free 901.4 MB)
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_63_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.4 MB)
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_63_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.SparkContext]Created broadcast 63 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 101 (ShuffledRDD[101] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 101.0 with 1 tasks
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 101.0 (TID 63, localhost, partition 1, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.executor.Executor]Running task 0.0 in stage 101.0 (TID 63)
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 101.0 (TID 63). 1549 bytes result sent to driver
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 101.0 (TID 63) in 16 ms on localhost (1/1)
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 101.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.scheduler.DAGScheduler]ResultStage 101 (print at SparkStreamingT.java:101) finished in 0.017 s
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.scheduler.DAGScheduler]Job 63 finished: print at SparkStreamingT.java:101, took 0.023622 s
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526887792000 ms.0 from job set of time 1526887792000 ms
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.141 s for time 1526887792000 ms (execution: 0.055 s)
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 97 from persistence list
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 96 from persistence list
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.storage.BlockManager]Removing RDD 97
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 95 from persistence list
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.storage.BlockManager]Removing RDD 96
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.rdd.BlockRDD]Removing RDD 94 from persistence list
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.streaming.dstream.SocketInputDStream]Removing blocks of RDD BlockRDD[94] at socketTextStream at SparkStreamingT.java:72 of time 1526887792000 ms
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 1526887790000 ms
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1526887790000 ms
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.storage.BlockManager]Removing RDD 94
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.storage.BlockManager]Removing RDD 95
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887792400
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:29:52] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:29:52] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 89.0 (TID 57)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN]  [2018-05-21 15:29:52] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 89.0 (TID 57, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:29:52] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 89.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 89.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 89
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.scheduler.DAGScheduler]ResultStage 89 (start at SparkStreamingT.java:107) failed in 2.976 s
[ERROR]  [2018-05-21 15:29:52] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 89.0 failed 1 times, most recent failure: Lost task 0.0 in stage 89.0 (TID 57, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.scheduler.DAGScheduler]Got job 64 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 102 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 102 (Receiver 0 ParallelCollectionRDD[102] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_64 stored as values in memory (estimated size 34.1 KB, free 901.3 MB)
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_64_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.3 MB)
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_64_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.SparkContext]Created broadcast 64 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 102 (Receiver 0 ParallelCollectionRDD[102] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 102.0 with 1 tasks
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 102.0 (TID 64, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.executor.Executor]Running task 0.0 in stage 102.0 (TID 64)
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887792600
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[ERROR]  [2018-05-21 15:29:52] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:29:52] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887792800
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:29:52] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:29:52] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 102.0 (TID 64)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN]  [2018-05-21 15:29:52] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 102.0 (TID 64, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:29:52] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 102.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 102.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 102
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.scheduler.DAGScheduler]ResultStage 102 (start at SparkStreamingT.java:107) failed in 0.341 s
[ERROR]  [2018-05-21 15:29:52] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 102.0 failed 1 times, most recent failure: Lost task 0.0 in stage 102.0 (TID 64, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.scheduler.DAGScheduler]Got job 65 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 103 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 103 (Receiver 0 ParallelCollectionRDD[103] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_65 stored as values in memory (estimated size 34.1 KB, free 901.3 MB)
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_65_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.3 MB)
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_65_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.SparkContext]Created broadcast 65 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 103 (Receiver 0 ParallelCollectionRDD[103] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 103.0 with 1 tasks
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 103.0 (TID 65, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.executor.Executor]Running task 0.0 in stage 103.0 (TID 65)
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887793000
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[ERROR]  [2018-05-21 15:29:52] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:29:52] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:29:52] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526887793000 ms
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526887793000 ms.0 from job set of time 1526887793000 ms
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 106 (mapToPair at SparkStreamingT.java:87)
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.scheduler.DAGScheduler]Got job 66 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 105 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 104)
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 105 (ShuffledRDD[107] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_66 stored as values in memory (estimated size 3.8 KB, free 901.3 MB)
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_66_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.3 MB)
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_66_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.SparkContext]Created broadcast 66 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 105 (ShuffledRDD[107] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 105.0 with 1 tasks
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 105.0 (TID 66, localhost, partition 0, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.executor.Executor]Running task 0.0 in stage 105.0 (TID 66)
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 105.0 (TID 66). 1470 bytes result sent to driver
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 105.0 (TID 66) in 22 ms on localhost (1/1)
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 105.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.scheduler.DAGScheduler]ResultStage 105 (print at SparkStreamingT.java:101) finished in 0.023 s
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.scheduler.DAGScheduler]Job 66 finished: print at SparkStreamingT.java:101, took 0.033527 s
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 19 is 83 bytes
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.scheduler.DAGScheduler]Got job 67 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 107 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 106)
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 107 (ShuffledRDD[107] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_67 stored as values in memory (estimated size 3.8 KB, free 901.3 MB)
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_67_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.3 MB)
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_67_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.SparkContext]Created broadcast 67 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 107 (ShuffledRDD[107] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 107.0 with 1 tasks
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 107.0 (TID 67, localhost, partition 1, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.executor.Executor]Running task 0.0 in stage 107.0 (TID 67)
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 107.0 (TID 67). 1549 bytes result sent to driver
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 107.0 (TID 67) in 13 ms on localhost (1/1)
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 107.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.scheduler.DAGScheduler]ResultStage 107 (print at SparkStreamingT.java:101) finished in 0.016 s
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.scheduler.DAGScheduler]Job 67 finished: print at SparkStreamingT.java:101, took 0.045885 s
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526887793000 ms.0 from job set of time 1526887793000 ms
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.110 s for time 1526887793000 ms (execution: 0.096 s)
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 101 from persistence list
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.storage.BlockManager]Removing RDD 101
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 100 from persistence list
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.storage.BlockManager]Removing RDD 100
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 99 from persistence list
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.storage.BlockManager]Removing RDD 99
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.rdd.BlockRDD]Removing RDD 98 from persistence list
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.streaming.dstream.SocketInputDStream]Removing blocks of RDD BlockRDD[98] at socketTextStream at SparkStreamingT.java:72 of time 1526887793000 ms
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 1526887791000 ms
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1526887791000 ms
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.storage.BlockManager]Removing RDD 98
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.ContextCleaner]Cleaned shuffle 12
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887793200
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:29:53] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:29:53] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 103.0 (TID 65)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.ContextCleaner]Cleaned shuffle 13
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_45_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.7 MB)
[WARN]  [2018-05-21 15:29:53] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 103.0 (TID 65, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:29:53] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 103.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 103.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 103
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.scheduler.DAGScheduler]ResultStage 103 (start at SparkStreamingT.java:107) failed in 0.492 s
[ERROR]  [2018-05-21 15:29:53] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 103.0 failed 1 times, most recent failure: Lost task 0.0 in stage 103.0 (TID 65, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_46_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.scheduler.DAGScheduler]Got job 68 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 108 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 108 (Receiver 0 ParallelCollectionRDD[108] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_47_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_68 stored as values in memory (estimated size 34.1 KB, free 901.3 MB)
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_68_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.3 MB)
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_48_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_68_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.SparkContext]Created broadcast 68 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 108 (Receiver 0 ParallelCollectionRDD[108] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 108.0 with 1 tasks
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 108.0 (TID 68, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_49_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.executor.Executor]Running task 0.0 in stage 108.0 (TID 68)
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887793600
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[ERROR]  [2018-05-21 15:29:53] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:29:53] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.ContextCleaner]Cleaned shuffle 14
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_50_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_51_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_52_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_53_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.ContextCleaner]Cleaned shuffle 15
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_54_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_55_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_56_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_57_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.ContextCleaner]Cleaned shuffle 16
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_58_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_59_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.ContextCleaner]Cleaned shuffle 17
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_60_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_61_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.ContextCleaner]Cleaned shuffle 18
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_62_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_63_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_64_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_66_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_67_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887793800
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:29:53] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:29:53] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 108.0 (TID 68)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN]  [2018-05-21 15:29:53] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 108.0 (TID 68, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:29:53] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 108.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 108.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 108
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.scheduler.DAGScheduler]ResultStage 108 (start at SparkStreamingT.java:107) failed in 0.392 s
[ERROR]  [2018-05-21 15:29:53] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 108.0 failed 1 times, most recent failure: Lost task 0.0 in stage 108.0 (TID 68, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.scheduler.DAGScheduler]Got job 69 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 109 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 109 (Receiver 0 ParallelCollectionRDD[109] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_69 stored as values in memory (estimated size 34.1 KB, free 901.6 MB)
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_69_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.6 MB)
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_69_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.SparkContext]Created broadcast 69 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 109 (Receiver 0 ParallelCollectionRDD[109] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 109.0 with 1 tasks
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 109.0 (TID 69, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.executor.Executor]Running task 0.0 in stage 109.0 (TID 69)
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887794000
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[ERROR]  [2018-05-21 15:29:53] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:29:53] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:29:53] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526887794000 ms
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526887794000 ms.0 from job set of time 1526887794000 ms
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 112 (mapToPair at SparkStreamingT.java:87)
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.scheduler.DAGScheduler]Got job 70 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 111 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 110)
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 111 (ShuffledRDD[113] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_70 stored as values in memory (estimated size 3.8 KB, free 901.6 MB)
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_70_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.6 MB)
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_70_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.SparkContext]Created broadcast 70 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 111 (ShuffledRDD[113] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 111.0 with 1 tasks
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 111.0 (TID 70, localhost, partition 0, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.executor.Executor]Running task 0.0 in stage 111.0 (TID 70)
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 111.0 (TID 70). 1549 bytes result sent to driver
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 111.0 (TID 70) in 37 ms on localhost (1/1)
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 111.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.scheduler.DAGScheduler]ResultStage 111 (print at SparkStreamingT.java:101) finished in 0.042 s
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.scheduler.DAGScheduler]Job 70 finished: print at SparkStreamingT.java:101, took 0.101510 s
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 20 is 83 bytes
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.scheduler.DAGScheduler]Got job 71 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 113 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 112)
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 113 (ShuffledRDD[113] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_71 stored as values in memory (estimated size 3.8 KB, free 901.6 MB)
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_71_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.6 MB)
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_71_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.SparkContext]Created broadcast 71 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 113 (ShuffledRDD[113] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 113.0 with 1 tasks
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 113.0 (TID 71, localhost, partition 1, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.executor.Executor]Running task 0.0 in stage 113.0 (TID 71)
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 113.0 (TID 71). 1549 bytes result sent to driver
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 113.0 (TID 71) in 5 ms on localhost (1/1)
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 113.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.scheduler.DAGScheduler]ResultStage 113 (print at SparkStreamingT.java:101) finished in 0.005 s
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.scheduler.DAGScheduler]Job 71 finished: print at SparkStreamingT.java:101, took 0.014341 s
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526887794000 ms.0 from job set of time 1526887794000 ms
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.176 s for time 1526887794000 ms (execution: 0.141 s)
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 107 from persistence list
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.storage.BlockManager]Removing RDD 107
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 106 from persistence list
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 105 from persistence list
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.storage.BlockManager]Removing RDD 106
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.storage.BlockManager]Removing RDD 105
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.rdd.BlockRDD]Removing RDD 104 from persistence list
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.storage.BlockManager]Removing RDD 104
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.streaming.dstream.SocketInputDStream]Removing blocks of RDD BlockRDD[104] at socketTextStream at SparkStreamingT.java:72 of time 1526887794000 ms
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 1526887792000 ms
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1526887792000 ms
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887794200
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:29:54] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:29:54] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 109.0 (TID 69)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN]  [2018-05-21 15:29:54] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 109.0 (TID 69, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:29:54] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 109.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 109.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 109
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.scheduler.DAGScheduler]ResultStage 109 (start at SparkStreamingT.java:107) failed in 0.347 s
[ERROR]  [2018-05-21 15:29:54] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 109.0 failed 1 times, most recent failure: Lost task 0.0 in stage 109.0 (TID 69, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.scheduler.DAGScheduler]Got job 72 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 114 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 114 (Receiver 0 ParallelCollectionRDD[114] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_72 stored as values in memory (estimated size 34.1 KB, free 901.6 MB)
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_72_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.6 MB)
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_72_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.SparkContext]Created broadcast 72 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 114 (Receiver 0 ParallelCollectionRDD[114] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 114.0 with 1 tasks
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 114.0 (TID 72, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.executor.Executor]Running task 0.0 in stage 114.0 (TID 72)
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887794400
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[ERROR]  [2018-05-21 15:29:54] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:29:54] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887794600
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:29:54] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:29:54] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 114.0 (TID 72)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN]  [2018-05-21 15:29:54] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 114.0 (TID 72, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:29:54] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 114.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 114.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 114
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.scheduler.DAGScheduler]ResultStage 114 (start at SparkStreamingT.java:107) failed in 0.395 s
[ERROR]  [2018-05-21 15:29:54] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 114.0 failed 1 times, most recent failure: Lost task 0.0 in stage 114.0 (TID 72, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.scheduler.DAGScheduler]Got job 73 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 115 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 115 (Receiver 0 ParallelCollectionRDD[115] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_73 stored as values in memory (estimated size 34.1 KB, free 901.5 MB)
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_73_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.5 MB)
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_73_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.SparkContext]Created broadcast 73 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 115 (Receiver 0 ParallelCollectionRDD[115] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 115.0 with 1 tasks
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 115.0 (TID 73, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.executor.Executor]Running task 0.0 in stage 115.0 (TID 73)
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887794800
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[ERROR]  [2018-05-21 15:29:54] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:29:54] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:29:54] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887795000
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526887795000 ms
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526887795000 ms.0 from job set of time 1526887795000 ms
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:29:55] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:29:55] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 115.0 (TID 73)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN]  [2018-05-21 15:29:55] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 115.0 (TID 73, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:29:55] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 115.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 115.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 115
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.scheduler.DAGScheduler]ResultStage 115 (start at SparkStreamingT.java:107) failed in 0.370 s
[ERROR]  [2018-05-21 15:29:55] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 115.0 failed 1 times, most recent failure: Lost task 0.0 in stage 115.0 (TID 73, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 118 (mapToPair at SparkStreamingT.java:87)
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.scheduler.DAGScheduler]Got job 74 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 117 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 116)
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 117 (ShuffledRDD[119] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_74 stored as values in memory (estimated size 3.8 KB, free 901.5 MB)
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_74_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.5 MB)
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_74_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.SparkContext]Created broadcast 74 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 117 (ShuffledRDD[119] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 117.0 with 1 tasks
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.scheduler.DAGScheduler]Got job 75 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 118 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 118 (Receiver 0 ParallelCollectionRDD[120] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_75 stored as values in memory (estimated size 34.1 KB, free 901.5 MB)
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_75_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.5 MB)
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 117.0 (TID 74, localhost, partition 0, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_75_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.SparkContext]Created broadcast 75 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 118 (Receiver 0 ParallelCollectionRDD[120] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 118.0 with 1 tasks
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 118.0 (TID 75, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.executor.Executor]Running task 0.0 in stage 117.0 (TID 74)
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 117.0 (TID 74). 1383 bytes result sent to driver
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.executor.Executor]Running task 0.0 in stage 118.0 (TID 75)
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 117.0 (TID 74) in 33 ms on localhost (1/1)
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 117.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.scheduler.DAGScheduler]ResultStage 117 (print at SparkStreamingT.java:101) finished in 0.045 s
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.scheduler.DAGScheduler]Job 74 finished: print at SparkStreamingT.java:101, took 0.076726 s
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 21 is 83 bytes
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.scheduler.DAGScheduler]Got job 76 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 120 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 119)
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 120 (ShuffledRDD[119] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_76 stored as values in memory (estimated size 3.8 KB, free 901.5 MB)
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_76_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.5 MB)
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_76_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.SparkContext]Created broadcast 76 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 120 (ShuffledRDD[119] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 120.0 with 1 tasks
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887795200
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 120.0 (TID 76, localhost, partition 1, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.executor.Executor]Running task 0.0 in stage 120.0 (TID 76)
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 120.0 (TID 76). 1462 bytes result sent to driver
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[ERROR]  [2018-05-21 15:29:55] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:29:55] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 120.0 (TID 76) in 10 ms on localhost (1/1)
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 120.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.scheduler.DAGScheduler]ResultStage 120 (print at SparkStreamingT.java:101) finished in 0.029 s
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.scheduler.DAGScheduler]Job 76 finished: print at SparkStreamingT.java:101, took 0.035814 s
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526887795000 ms.0 from job set of time 1526887795000 ms
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.129 s for time 1526887795000 ms (execution: 0.122 s)
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 113 from persistence list
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 112 from persistence list
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.storage.BlockManager]Removing RDD 112
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.storage.BlockManager]Removing RDD 113
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 111 from persistence list
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.rdd.BlockRDD]Removing RDD 110 from persistence list
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.storage.BlockManager]Removing RDD 110
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.storage.BlockManager]Removing RDD 111
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.streaming.dstream.SocketInputDStream]Removing blocks of RDD BlockRDD[110] at socketTextStream at SparkStreamingT.java:72 of time 1526887795000 ms
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 1526887793000 ms
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1526887793000 ms
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887795400
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:29:55] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:29:55] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 118.0 (TID 75)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN]  [2018-05-21 15:29:55] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 118.0 (TID 75, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:29:55] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 118.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 118.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 118
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.scheduler.DAGScheduler]ResultStage 118 (start at SparkStreamingT.java:107) failed in 0.355 s
[ERROR]  [2018-05-21 15:29:55] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 118.0 failed 1 times, most recent failure: Lost task 0.0 in stage 118.0 (TID 75, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.scheduler.DAGScheduler]Got job 77 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 121 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 121 (Receiver 0 ParallelCollectionRDD[121] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_77 stored as values in memory (estimated size 34.1 KB, free 901.4 MB)
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_77_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.4 MB)
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_77_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.SparkContext]Created broadcast 77 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 121 (Receiver 0 ParallelCollectionRDD[121] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 121.0 with 1 tasks
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 121.0 (TID 77, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.executor.Executor]Running task 0.0 in stage 121.0 (TID 77)
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887795600
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[ERROR]  [2018-05-21 15:29:55] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:29:55] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887795800
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:29:55] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:29:55] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 121.0 (TID 77)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN]  [2018-05-21 15:29:55] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 121.0 (TID 77, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:29:55] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 121.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 121.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 121
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.scheduler.DAGScheduler]ResultStage 121 (start at SparkStreamingT.java:107) failed in 0.365 s
[ERROR]  [2018-05-21 15:29:55] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 121.0 failed 1 times, most recent failure: Lost task 0.0 in stage 121.0 (TID 77, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.scheduler.DAGScheduler]Got job 78 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 122 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 122 (Receiver 0 ParallelCollectionRDD[122] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_78 stored as values in memory (estimated size 34.1 KB, free 901.4 MB)
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_78_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.4 MB)
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_78_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.SparkContext]Created broadcast 78 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 122 (Receiver 0 ParallelCollectionRDD[122] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 122.0 with 1 tasks
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 122.0 (TID 78, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.executor.Executor]Running task 0.0 in stage 122.0 (TID 78)
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887796000
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[ERROR]  [2018-05-21 15:29:55] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:29:55] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:29:55] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526887796000 ms
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526887796000 ms.0 from job set of time 1526887796000 ms
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 125 (mapToPair at SparkStreamingT.java:87)
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.scheduler.DAGScheduler]Got job 79 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 124 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 123)
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 124 (ShuffledRDD[126] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_79 stored as values in memory (estimated size 3.8 KB, free 901.4 MB)
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_79_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.4 MB)
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_79_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.SparkContext]Created broadcast 79 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 124 (ShuffledRDD[126] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 124.0 with 1 tasks
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 124.0 (TID 79, localhost, partition 0, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.executor.Executor]Running task 0.0 in stage 124.0 (TID 79)
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 124.0 (TID 79). 1470 bytes result sent to driver
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 124.0 (TID 79) in 4 ms on localhost (1/1)
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 124.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.scheduler.DAGScheduler]ResultStage 124 (print at SparkStreamingT.java:101) finished in 0.004 s
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.scheduler.DAGScheduler]Job 79 finished: print at SparkStreamingT.java:101, took 0.007810 s
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 22 is 83 bytes
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.scheduler.DAGScheduler]Got job 80 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 126 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 125)
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 126 (ShuffledRDD[126] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_80 stored as values in memory (estimated size 3.8 KB, free 901.4 MB)
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_80_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.4 MB)
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_80_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.SparkContext]Created broadcast 80 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 126 (ShuffledRDD[126] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 126.0 with 1 tasks
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 126.0 (TID 80, localhost, partition 1, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.executor.Executor]Running task 0.0 in stage 126.0 (TID 80)
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 126.0 (TID 80). 1549 bytes result sent to driver
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 126.0 (TID 80) in 4 ms on localhost (1/1)
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 126.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.scheduler.DAGScheduler]ResultStage 126 (print at SparkStreamingT.java:101) finished in 0.006 s
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.scheduler.DAGScheduler]Job 80 finished: print at SparkStreamingT.java:101, took 0.022950 s
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526887796000 ms.0 from job set of time 1526887796000 ms
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.045 s for time 1526887796000 ms (execution: 0.039 s)
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 119 from persistence list
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 118 from persistence list
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.storage.BlockManager]Removing RDD 118
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.storage.BlockManager]Removing RDD 119
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 117 from persistence list
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.rdd.BlockRDD]Removing RDD 116 from persistence list
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.storage.BlockManager]Removing RDD 117
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.streaming.dstream.SocketInputDStream]Removing blocks of RDD BlockRDD[116] at socketTextStream at SparkStreamingT.java:72 of time 1526887796000 ms
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 1526887794000 ms
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1526887794000 ms
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.storage.BlockManager]Removing RDD 116
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887796200
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:29:56] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:29:56] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 122.0 (TID 78)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN]  [2018-05-21 15:29:56] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 122.0 (TID 78, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:29:56] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 122.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 122.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 122
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.scheduler.DAGScheduler]ResultStage 122 (start at SparkStreamingT.java:107) failed in 0.368 s
[ERROR]  [2018-05-21 15:29:56] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 122.0 failed 1 times, most recent failure: Lost task 0.0 in stage 122.0 (TID 78, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.scheduler.DAGScheduler]Got job 81 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 127 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 127 (Receiver 0 ParallelCollectionRDD[127] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_81 stored as values in memory (estimated size 34.1 KB, free 901.3 MB)
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_81_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.3 MB)
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_81_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.SparkContext]Created broadcast 81 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 127 (Receiver 0 ParallelCollectionRDD[127] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 127.0 with 1 tasks
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 127.0 (TID 81, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.executor.Executor]Running task 0.0 in stage 127.0 (TID 81)
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887796400
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[ERROR]  [2018-05-21 15:29:56] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:29:56] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887796400
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:29:56] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:29:56] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 127.0 (TID 81)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN]  [2018-05-21 15:29:56] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 127.0 (TID 81, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:29:56] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 127.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 127.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 127
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.scheduler.DAGScheduler]ResultStage 127 (start at SparkStreamingT.java:107) failed in 0.175 s
[ERROR]  [2018-05-21 15:29:56] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 127.0 failed 1 times, most recent failure: Lost task 0.0 in stage 127.0 (TID 81, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.scheduler.DAGScheduler]Got job 82 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 128 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 128 (Receiver 0 ParallelCollectionRDD[128] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_82 stored as values in memory (estimated size 34.1 KB, free 901.3 MB)
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_82_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.3 MB)
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_82_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.SparkContext]Created broadcast 82 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 128 (Receiver 0 ParallelCollectionRDD[128] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 128.0 with 1 tasks
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 128.0 (TID 82, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.executor.Executor]Running task 0.0 in stage 128.0 (TID 82)
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887796600
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[ERROR]  [2018-05-21 15:29:56] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:29:56] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887796800
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:29:56] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:29:56] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 128.0 (TID 82)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN]  [2018-05-21 15:29:56] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 128.0 (TID 82, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:29:56] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 128.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 128.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 128
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.scheduler.DAGScheduler]ResultStage 128 (start at SparkStreamingT.java:107) failed in 0.385 s
[ERROR]  [2018-05-21 15:29:56] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 128.0 failed 1 times, most recent failure: Lost task 0.0 in stage 128.0 (TID 82, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.scheduler.DAGScheduler]Got job 83 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 129 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 129 (Receiver 0 ParallelCollectionRDD[129] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_83 stored as values in memory (estimated size 34.1 KB, free 901.2 MB)
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_83_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.2 MB)
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_83_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.SparkContext]Created broadcast 83 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 129 (Receiver 0 ParallelCollectionRDD[129] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 129.0 with 1 tasks
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 129.0 (TID 83, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.executor.Executor]Running task 0.0 in stage 129.0 (TID 83)
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887797000
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[ERROR]  [2018-05-21 15:29:56] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:29:56] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:29:56] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526887797000 ms
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526887797000 ms.0 from job set of time 1526887797000 ms
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 132 (mapToPair at SparkStreamingT.java:87)
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.scheduler.DAGScheduler]Got job 84 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 131 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 130)
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 131 (ShuffledRDD[133] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_84 stored as values in memory (estimated size 3.8 KB, free 901.2 MB)
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_84_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.2 MB)
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_84_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.SparkContext]Created broadcast 84 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 131 (ShuffledRDD[133] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 131.0 with 1 tasks
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 131.0 (TID 84, localhost, partition 0, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.executor.Executor]Running task 0.0 in stage 131.0 (TID 84)
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 131.0 (TID 84). 1549 bytes result sent to driver
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 131.0 (TID 84) in 3 ms on localhost (1/1)
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 131.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.scheduler.DAGScheduler]ResultStage 131 (print at SparkStreamingT.java:101) finished in 0.009 s
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.scheduler.DAGScheduler]Job 84 finished: print at SparkStreamingT.java:101, took 0.023868 s
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 23 is 83 bytes
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.scheduler.DAGScheduler]Got job 85 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 133 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 132)
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 133 (ShuffledRDD[133] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_85 stored as values in memory (estimated size 3.8 KB, free 901.2 MB)
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_85_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.2 MB)
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_85_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.6 MB)
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.SparkContext]Created broadcast 85 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 133 (ShuffledRDD[133] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 133.0 with 1 tasks
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 133.0 (TID 85, localhost, partition 1, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.executor.Executor]Running task 0.0 in stage 133.0 (TID 85)
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 133.0 (TID 85). 1470 bytes result sent to driver
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 133.0 (TID 85) in 10 ms on localhost (1/1)
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 133.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.scheduler.DAGScheduler]ResultStage 133 (print at SparkStreamingT.java:101) finished in 0.011 s
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.scheduler.DAGScheduler]Job 85 finished: print at SparkStreamingT.java:101, took 0.019141 s
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526887797000 ms.0 from job set of time 1526887797000 ms
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.085 s for time 1526887797000 ms (execution: 0.068 s)
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 126 from persistence list
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.storage.BlockManager]Removing RDD 126
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 125 from persistence list
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.storage.BlockManager]Removing RDD 125
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 124 from persistence list
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.storage.BlockManager]Removing RDD 124
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.rdd.BlockRDD]Removing RDD 123 from persistence list
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.streaming.dstream.SocketInputDStream]Removing blocks of RDD BlockRDD[123] at socketTextStream at SparkStreamingT.java:72 of time 1526887797000 ms
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 1526887795000 ms
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1526887795000 ms
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.storage.BlockManager]Removing RDD 123
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887797200
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:29:57] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:29:57] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 129.0 (TID 83)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN]  [2018-05-21 15:29:57] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 129.0 (TID 83, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:29:57] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 129.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 129.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 129
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.scheduler.DAGScheduler]ResultStage 129 (start at SparkStreamingT.java:107) failed in 0.376 s
[ERROR]  [2018-05-21 15:29:57] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 129.0 failed 1 times, most recent failure: Lost task 0.0 in stage 129.0 (TID 83, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.scheduler.DAGScheduler]Got job 86 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 134 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 134 (Receiver 0 ParallelCollectionRDD[134] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_86 stored as values in memory (estimated size 34.1 KB, free 901.2 MB)
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_86_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.2 MB)
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_86_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.6 MB)
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.SparkContext]Created broadcast 86 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 134 (Receiver 0 ParallelCollectionRDD[134] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 134.0 with 1 tasks
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 134.0 (TID 86, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.executor.Executor]Running task 0.0 in stage 134.0 (TID 86)
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887797400
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[ERROR]  [2018-05-21 15:29:57] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:29:57] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887797600
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:29:57] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:29:57] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 134.0 (TID 86)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN]  [2018-05-21 15:29:57] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 134.0 (TID 86, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:29:57] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 134.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 134.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 134
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.scheduler.DAGScheduler]ResultStage 134 (start at SparkStreamingT.java:107) failed in 0.385 s
[ERROR]  [2018-05-21 15:29:57] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 134.0 failed 1 times, most recent failure: Lost task 0.0 in stage 134.0 (TID 86, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.scheduler.DAGScheduler]Got job 87 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 135 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 135 (Receiver 0 ParallelCollectionRDD[135] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_87 stored as values in memory (estimated size 34.1 KB, free 901.1 MB)
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_87_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.1 MB)
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_87_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.6 MB)
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.SparkContext]Created broadcast 87 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 135 (Receiver 0 ParallelCollectionRDD[135] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 135.0 with 1 tasks
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 135.0 (TID 87, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.executor.Executor]Running task 0.0 in stage 135.0 (TID 87)
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887797800
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[ERROR]  [2018-05-21 15:29:57] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:29:57] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:29:57] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887798000
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:29:58] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:29:58] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 135.0 (TID 87)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526887798000 ms
[WARN]  [2018-05-21 15:29:58] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 135.0 (TID 87, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:29:58] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 135.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 135.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526887798000 ms.0 from job set of time 1526887798000 ms
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 135
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.scheduler.DAGScheduler]ResultStage 135 (start at SparkStreamingT.java:107) failed in 0.384 s
[ERROR]  [2018-05-21 15:29:58] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 135.0 failed 1 times, most recent failure: Lost task 0.0 in stage 135.0 (TID 87, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 138 (mapToPair at SparkStreamingT.java:87)
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.scheduler.DAGScheduler]Got job 88 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 137 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 136)
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 137 (ShuffledRDD[139] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_88 stored as values in memory (estimated size 3.8 KB, free 901.1 MB)
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_88_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.1 MB)
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_88_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.6 MB)
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.SparkContext]Created broadcast 88 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 137 (ShuffledRDD[139] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 137.0 with 1 tasks
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 137.0 (TID 88, localhost, partition 0, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.executor.Executor]Running task 0.0 in stage 137.0 (TID 88)
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 137.0 (TID 88). 1470 bytes result sent to driver
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.scheduler.DAGScheduler]ResultStage 137 (print at SparkStreamingT.java:101) finished in 0.005 s
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 137.0 (TID 88) in 5 ms on localhost (1/1)
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 137.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.scheduler.DAGScheduler]Job 88 finished: print at SparkStreamingT.java:101, took 0.013330 s
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 24 is 83 bytes
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.scheduler.DAGScheduler]Got job 89 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 139 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 138)
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 139 (ShuffledRDD[139] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_89 stored as values in memory (estimated size 3.8 KB, free 901.1 MB)
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_89_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.1 MB)
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_89_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.6 MB)
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.SparkContext]Created broadcast 89 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 139 (ShuffledRDD[139] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 139.0 with 1 tasks
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 139.0 (TID 89, localhost, partition 1, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.scheduler.DAGScheduler]Got job 90 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 140 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 140 (Receiver 0 ParallelCollectionRDD[140] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.executor.Executor]Running task 0.0 in stage 139.0 (TID 89)
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 139.0 (TID 89). 1470 bytes result sent to driver
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_90 stored as values in memory (estimated size 34.1 KB, free 901.1 MB)
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_90_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.1 MB)
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 139.0 (TID 89) in 8 ms on localhost (1/1)
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 139.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_90_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.6 MB)
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.SparkContext]Created broadcast 90 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 140 (Receiver 0 ParallelCollectionRDD[140] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 140.0 with 1 tasks
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.scheduler.DAGScheduler]ResultStage 139 (print at SparkStreamingT.java:101) finished in 0.010 s
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 140.0 (TID 90, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.scheduler.DAGScheduler]Job 89 finished: print at SparkStreamingT.java:101, took 0.016871 s
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526887798000 ms.0 from job set of time 1526887798000 ms
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.047 s for time 1526887798000 ms (execution: 0.038 s)
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.executor.Executor]Running task 0.0 in stage 140.0 (TID 90)
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 133 from persistence list
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 132 from persistence list
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.storage.BlockManager]Removing RDD 133
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.storage.BlockManager]Removing RDD 132
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 131 from persistence list
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.rdd.BlockRDD]Removing RDD 130 from persistence list
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887798200
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.storage.BlockManager]Removing RDD 131
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.streaming.dstream.SocketInputDStream]Removing blocks of RDD BlockRDD[130] at socketTextStream at SparkStreamingT.java:72 of time 1526887798000 ms
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 1526887796000 ms
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1526887796000 ms
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.storage.BlockManager]Removing RDD 130
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[ERROR]  [2018-05-21 15:29:58] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:29:58] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887798400
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:29:58] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:29:58] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 140.0 (TID 90)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN]  [2018-05-21 15:29:58] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 140.0 (TID 90, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:29:58] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 140.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 140.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 140
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.scheduler.DAGScheduler]ResultStage 140 (start at SparkStreamingT.java:107) failed in 0.370 s
[ERROR]  [2018-05-21 15:29:58] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 140.0 failed 1 times, most recent failure: Lost task 0.0 in stage 140.0 (TID 90, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.scheduler.DAGScheduler]Got job 91 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 141 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 141 (Receiver 0 ParallelCollectionRDD[141] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_91 stored as values in memory (estimated size 34.1 KB, free 901.0 MB)
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_91_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.0 MB)
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_91_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.6 MB)
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.SparkContext]Created broadcast 91 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 141 (Receiver 0 ParallelCollectionRDD[141] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 141.0 with 1 tasks
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 141.0 (TID 91, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.executor.Executor]Running task 0.0 in stage 141.0 (TID 91)
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887798600
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[ERROR]  [2018-05-21 15:29:58] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:29:58] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887798800
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:29:58] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:29:58] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 141.0 (TID 91)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN]  [2018-05-21 15:29:58] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 141.0 (TID 91, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:29:58] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 141.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 141.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 141
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.scheduler.DAGScheduler]ResultStage 141 (start at SparkStreamingT.java:107) failed in 0.353 s
[ERROR]  [2018-05-21 15:29:58] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 141.0 failed 1 times, most recent failure: Lost task 0.0 in stage 141.0 (TID 91, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.scheduler.DAGScheduler]Got job 92 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 142 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 142 (Receiver 0 ParallelCollectionRDD[142] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_92 stored as values in memory (estimated size 34.1 KB, free 901.0 MB)
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_92_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.0 MB)
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_92_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.6 MB)
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.SparkContext]Created broadcast 92 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 142 (Receiver 0 ParallelCollectionRDD[142] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 142.0 with 1 tasks
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 142.0 (TID 92, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.executor.Executor]Running task 0.0 in stage 142.0 (TID 92)
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_65_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.6 MB)
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887799000
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.ContextCleaner]Cleaned shuffle 19
[ERROR]  [2018-05-21 15:29:58] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:29:58] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_68_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.6 MB)
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_69_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.6 MB)
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.ContextCleaner]Cleaned shuffle 20
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_70_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.6 MB)
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_71_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.6 MB)
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_72_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.6 MB)
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_73_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.6 MB)
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.ContextCleaner]Cleaned shuffle 21
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_74_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_75_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_76_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_77_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_78_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.ContextCleaner]Cleaned shuffle 22
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_79_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_80_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_81_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_82_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_83_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.ContextCleaner]Cleaned shuffle 23
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_84_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_85_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_86_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_87_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_88_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_89_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_90_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:29:58] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_91_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526887799000 ms
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526887799000 ms.0 from job set of time 1526887799000 ms
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 145 (mapToPair at SparkStreamingT.java:87)
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.scheduler.DAGScheduler]Got job 93 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 144 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 143)
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 144 (ShuffledRDD[146] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_93 stored as values in memory (estimated size 3.8 KB, free 901.7 MB)
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_93_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.7 MB)
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_93_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.SparkContext]Created broadcast 93 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 144 (ShuffledRDD[146] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 144.0 with 1 tasks
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 144.0 (TID 93, localhost, partition 0, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.executor.Executor]Running task 0.0 in stage 144.0 (TID 93)
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 144.0 (TID 93). 1470 bytes result sent to driver
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 144.0 (TID 93) in 10 ms on localhost (1/1)
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 144.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.scheduler.DAGScheduler]ResultStage 144 (print at SparkStreamingT.java:101) finished in 0.010 s
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.scheduler.DAGScheduler]Job 93 finished: print at SparkStreamingT.java:101, took 0.016771 s
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 25 is 83 bytes
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.scheduler.DAGScheduler]Got job 94 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 146 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 145)
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 146 (ShuffledRDD[146] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_94 stored as values in memory (estimated size 3.8 KB, free 901.7 MB)
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_94_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.7 MB)
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_94_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.SparkContext]Created broadcast 94 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 146 (ShuffledRDD[146] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 146.0 with 1 tasks
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 146.0 (TID 94, localhost, partition 1, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.executor.Executor]Running task 0.0 in stage 146.0 (TID 94)
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 146.0 (TID 94). 1549 bytes result sent to driver
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 146.0 (TID 94) in 4 ms on localhost (1/1)
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 146.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.scheduler.DAGScheduler]ResultStage 146 (print at SparkStreamingT.java:101) finished in 0.004 s
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.scheduler.DAGScheduler]Job 94 finished: print at SparkStreamingT.java:101, took 0.012356 s
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526887799000 ms.0 from job set of time 1526887799000 ms
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.049 s for time 1526887799000 ms (execution: 0.038 s)
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 139 from persistence list
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.storage.BlockManager]Removing RDD 139
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 138 from persistence list
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.storage.BlockManager]Removing RDD 138
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 137 from persistence list
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.storage.BlockManager]Removing RDD 137
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.rdd.BlockRDD]Removing RDD 136 from persistence list
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.storage.BlockManager]Removing RDD 136
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.streaming.dstream.SocketInputDStream]Removing blocks of RDD BlockRDD[136] at socketTextStream at SparkStreamingT.java:72 of time 1526887799000 ms
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 1526887797000 ms
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1526887797000 ms
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887799200
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:29:59] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:29:59] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 142.0 (TID 92)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN]  [2018-05-21 15:29:59] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 142.0 (TID 92, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:29:59] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 142.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 142.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 142
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.scheduler.DAGScheduler]ResultStage 142 (start at SparkStreamingT.java:107) failed in 0.357 s
[ERROR]  [2018-05-21 15:29:59] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 142.0 failed 1 times, most recent failure: Lost task 0.0 in stage 142.0 (TID 92, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.scheduler.DAGScheduler]Got job 95 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 147 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 147 (Receiver 0 ParallelCollectionRDD[147] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_95 stored as values in memory (estimated size 34.1 KB, free 901.7 MB)
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_95_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.7 MB)
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_95_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.SparkContext]Created broadcast 95 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 147 (Receiver 0 ParallelCollectionRDD[147] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 147.0 with 1 tasks
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 147.0 (TID 95, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.executor.Executor]Running task 0.0 in stage 147.0 (TID 95)
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887799400
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[ERROR]  [2018-05-21 15:29:59] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:29:59] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887799600
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:29:59] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:29:59] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 147.0 (TID 95)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN]  [2018-05-21 15:29:59] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 147.0 (TID 95, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:29:59] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 147.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 147.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 147
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.scheduler.DAGScheduler]ResultStage 147 (start at SparkStreamingT.java:107) failed in 0.373 s
[ERROR]  [2018-05-21 15:29:59] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 147.0 failed 1 times, most recent failure: Lost task 0.0 in stage 147.0 (TID 95, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.scheduler.DAGScheduler]Got job 96 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 148 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 148 (Receiver 0 ParallelCollectionRDD[148] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_96 stored as values in memory (estimated size 34.1 KB, free 901.6 MB)
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_96_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.6 MB)
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_96_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.SparkContext]Created broadcast 96 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 148 (Receiver 0 ParallelCollectionRDD[148] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 148.0 with 1 tasks
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 148.0 (TID 96, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.executor.Executor]Running task 0.0 in stage 148.0 (TID 96)
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887799800
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[ERROR]  [2018-05-21 15:29:59] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:29:59] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:29:59] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887800000
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526887800000 ms
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:30:00] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:30:00] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 148.0 (TID 96)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526887800000 ms.0 from job set of time 1526887800000 ms
[WARN]  [2018-05-21 15:30:00] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 148.0 (TID 96, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:30:00] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 148.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 148.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 148
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.scheduler.DAGScheduler]ResultStage 148 (start at SparkStreamingT.java:107) failed in 0.382 s
[ERROR]  [2018-05-21 15:30:00] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 148.0 failed 1 times, most recent failure: Lost task 0.0 in stage 148.0 (TID 96, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 151 (mapToPair at SparkStreamingT.java:87)
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.scheduler.DAGScheduler]Got job 97 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 150 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 149)
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 150 (ShuffledRDD[152] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_97 stored as values in memory (estimated size 3.8 KB, free 901.6 MB)
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_97_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.6 MB)
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_97_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.SparkContext]Created broadcast 97 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 150 (ShuffledRDD[152] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 150.0 with 1 tasks
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.scheduler.DAGScheduler]Got job 98 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 151 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 151 (Receiver 0 ParallelCollectionRDD[153] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 150.0 (TID 97, localhost, partition 0, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.executor.Executor]Running task 0.0 in stage 150.0 (TID 97)
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 150.0 (TID 97). 1470 bytes result sent to driver
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 150.0 (TID 97) in 4 ms on localhost (1/1)
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 150.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_98 stored as values in memory (estimated size 34.1 KB, free 901.6 MB)
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_98_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.6 MB)
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_98_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.SparkContext]Created broadcast 98 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 151 (Receiver 0 ParallelCollectionRDD[153] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 151.0 with 1 tasks
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.scheduler.DAGScheduler]ResultStage 150 (print at SparkStreamingT.java:101) finished in 0.010 s
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 151.0 (TID 98, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.scheduler.DAGScheduler]Job 97 finished: print at SparkStreamingT.java:101, took 0.019530 s
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 26 is 83 bytes
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.scheduler.DAGScheduler]Got job 99 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 153 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 152)
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 153 (ShuffledRDD[152] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_99 stored as values in memory (estimated size 3.8 KB, free 901.6 MB)
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_99_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.6 MB)
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.executor.Executor]Running task 0.0 in stage 151.0 (TID 98)
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_99_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.SparkContext]Created broadcast 99 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 153 (ShuffledRDD[152] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 153.0 with 1 tasks
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 153.0 (TID 99, localhost, partition 1, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.executor.Executor]Running task 0.0 in stage 153.0 (TID 99)
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887800200
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 153.0 (TID 99). 1549 bytes result sent to driver
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 153.0 (TID 99) in 4 ms on localhost (1/1)
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 153.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.scheduler.DAGScheduler]ResultStage 153 (print at SparkStreamingT.java:101) finished in 0.004 s
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.scheduler.DAGScheduler]Job 99 finished: print at SparkStreamingT.java:101, took 0.010379 s
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526887800000 ms.0 from job set of time 1526887800000 ms
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.071 s for time 1526887800000 ms (execution: 0.056 s)
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 146 from persistence list
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 145 from persistence list
[ERROR]  [2018-05-21 15:30:00] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:30:00] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 144 from persistence list
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.storage.BlockManager]Removing RDD 145
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.storage.BlockManager]Removing RDD 146
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.rdd.BlockRDD]Removing RDD 143 from persistence list
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.storage.BlockManager]Removing RDD 144
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.streaming.dstream.SocketInputDStream]Removing blocks of RDD BlockRDD[143] at socketTextStream at SparkStreamingT.java:72 of time 1526887800000 ms
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 1526887798000 ms
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1526887798000 ms
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.storage.BlockManager]Removing RDD 143
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887800400
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:30:00] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:30:00] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 151.0 (TID 98)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN]  [2018-05-21 15:30:00] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 151.0 (TID 98, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:30:00] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 151.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 151.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 151
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.scheduler.DAGScheduler]ResultStage 151 (start at SparkStreamingT.java:107) failed in 0.368 s
[ERROR]  [2018-05-21 15:30:00] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 151.0 failed 1 times, most recent failure: Lost task 0.0 in stage 151.0 (TID 98, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.scheduler.DAGScheduler]Got job 100 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 154 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 154 (Receiver 0 ParallelCollectionRDD[154] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_100 stored as values in memory (estimated size 34.1 KB, free 901.5 MB)
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_100_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.5 MB)
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_100_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.SparkContext]Created broadcast 100 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 154 (Receiver 0 ParallelCollectionRDD[154] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 154.0 with 1 tasks
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 154.0 (TID 100, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.executor.Executor]Running task 0.0 in stage 154.0 (TID 100)
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887800600
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[ERROR]  [2018-05-21 15:30:00] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:30:00] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887800600
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:30:00] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:30:00] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 154.0 (TID 100)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN]  [2018-05-21 15:30:00] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 154.0 (TID 100, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:30:00] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 154.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 154.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 154
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.scheduler.DAGScheduler]ResultStage 154 (start at SparkStreamingT.java:107) failed in 0.163 s
[ERROR]  [2018-05-21 15:30:00] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 154.0 failed 1 times, most recent failure: Lost task 0.0 in stage 154.0 (TID 100, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.scheduler.DAGScheduler]Got job 101 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 155 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 155 (Receiver 0 ParallelCollectionRDD[155] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_101 stored as values in memory (estimated size 34.1 KB, free 901.5 MB)
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_101_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.5 MB)
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_101_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.SparkContext]Created broadcast 101 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 155 (Receiver 0 ParallelCollectionRDD[155] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 155.0 with 1 tasks
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 155.0 (TID 101, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.executor.Executor]Running task 0.0 in stage 155.0 (TID 101)
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887800800
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[ERROR]  [2018-05-21 15:30:00] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:30:00] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:30:00] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887801000
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:30:01] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:30:01] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 155.0 (TID 101)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526887801000 ms
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526887801000 ms.0 from job set of time 1526887801000 ms
[WARN]  [2018-05-21 15:30:01] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 155.0 (TID 101, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:30:01] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 155.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 155.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 155
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.scheduler.DAGScheduler]ResultStage 155 (start at SparkStreamingT.java:107) failed in 0.368 s
[ERROR]  [2018-05-21 15:30:01] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 155.0 failed 1 times, most recent failure: Lost task 0.0 in stage 155.0 (TID 101, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 158 (mapToPair at SparkStreamingT.java:87)
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.scheduler.DAGScheduler]Got job 102 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 157 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 156)
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 157 (ShuffledRDD[159] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_102 stored as values in memory (estimated size 3.8 KB, free 901.5 MB)
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_102_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.5 MB)
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_102_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.SparkContext]Created broadcast 102 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 157 (ShuffledRDD[159] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 157.0 with 1 tasks
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 157.0 (TID 102, localhost, partition 0, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.executor.Executor]Running task 0.0 in stage 157.0 (TID 102)
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 157.0 (TID 102). 1462 bytes result sent to driver
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 157.0 (TID 102) in 5 ms on localhost (1/1)
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 157.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.scheduler.DAGScheduler]ResultStage 157 (print at SparkStreamingT.java:101) finished in 0.005 s
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.scheduler.DAGScheduler]Job 102 finished: print at SparkStreamingT.java:101, took 0.011563 s
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.scheduler.DAGScheduler]Got job 103 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 158 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 158 (Receiver 0 ParallelCollectionRDD[160] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_103 stored as values in memory (estimated size 34.1 KB, free 901.4 MB)
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_103_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.4 MB)
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_103_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.SparkContext]Created broadcast 103 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 158 (Receiver 0 ParallelCollectionRDD[160] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 158.0 with 1 tasks
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 27 is 83 bytes
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 158.0 (TID 103, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.executor.Executor]Running task 0.0 in stage 158.0 (TID 103)
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.scheduler.DAGScheduler]Got job 104 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 160 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 159)
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 160 (ShuffledRDD[159] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_104 stored as values in memory (estimated size 3.8 KB, free 901.4 MB)
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_104_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.4 MB)
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_104_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.SparkContext]Created broadcast 104 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 160 (ShuffledRDD[159] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 160.0 with 1 tasks
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887801200
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 160.0 (TID 104, localhost, partition 1, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.executor.Executor]Running task 0.0 in stage 160.0 (TID 104)
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[ERROR]  [2018-05-21 15:30:01] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:30:01] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 160.0 (TID 104). 1462 bytes result sent to driver
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 160.0 (TID 104) in 2 ms on localhost (1/1)
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 160.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.scheduler.DAGScheduler]ResultStage 160 (print at SparkStreamingT.java:101) finished in 0.005 s
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.scheduler.DAGScheduler]Job 104 finished: print at SparkStreamingT.java:101, took 0.020016 s
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526887801000 ms.0 from job set of time 1526887801000 ms
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.049 s for time 1526887801000 ms (execution: 0.042 s)
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 152 from persistence list
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.storage.BlockManager]Removing RDD 152
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 151 from persistence list
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.storage.BlockManager]Removing RDD 151
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 150 from persistence list
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.storage.BlockManager]Removing RDD 150
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.rdd.BlockRDD]Removing RDD 149 from persistence list
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.storage.BlockManager]Removing RDD 149
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.streaming.dstream.SocketInputDStream]Removing blocks of RDD BlockRDD[149] at socketTextStream at SparkStreamingT.java:72 of time 1526887801000 ms
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 1526887799000 ms
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1526887799000 ms
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887801400
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:30:01] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:30:01] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 158.0 (TID 103)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN]  [2018-05-21 15:30:01] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 158.0 (TID 103, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:30:01] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 158.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 158.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 158
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.scheduler.DAGScheduler]ResultStage 158 (start at SparkStreamingT.java:107) failed in 0.368 s
[ERROR]  [2018-05-21 15:30:01] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 158.0 failed 1 times, most recent failure: Lost task 0.0 in stage 158.0 (TID 103, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.scheduler.DAGScheduler]Got job 105 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 161 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 161 (Receiver 0 ParallelCollectionRDD[161] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_105 stored as values in memory (estimated size 34.1 KB, free 901.4 MB)
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_105_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.4 MB)
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_105_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.SparkContext]Created broadcast 105 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 161 (Receiver 0 ParallelCollectionRDD[161] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 161.0 with 1 tasks
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 161.0 (TID 105, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.executor.Executor]Running task 0.0 in stage 161.0 (TID 105)
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887801600
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[ERROR]  [2018-05-21 15:30:01] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:30:01] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887801800
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:30:01] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:30:01] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 161.0 (TID 105)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN]  [2018-05-21 15:30:01] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 161.0 (TID 105, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:30:01] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 161.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 161.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 161
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.scheduler.DAGScheduler]ResultStage 161 (start at SparkStreamingT.java:107) failed in 0.365 s
[ERROR]  [2018-05-21 15:30:01] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 161.0 failed 1 times, most recent failure: Lost task 0.0 in stage 161.0 (TID 105, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.scheduler.DAGScheduler]Got job 106 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 162 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 162 (Receiver 0 ParallelCollectionRDD[162] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_106 stored as values in memory (estimated size 34.1 KB, free 901.3 MB)
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_106_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.3 MB)
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_106_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.SparkContext]Created broadcast 106 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 162 (Receiver 0 ParallelCollectionRDD[162] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 162.0 with 1 tasks
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 162.0 (TID 106, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.executor.Executor]Running task 0.0 in stage 162.0 (TID 106)
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887802000
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[ERROR]  [2018-05-21 15:30:01] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:30:01] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:30:01] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526887802000 ms
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526887802000 ms.0 from job set of time 1526887802000 ms
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 165 (mapToPair at SparkStreamingT.java:87)
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.scheduler.DAGScheduler]Got job 107 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 164 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 163)
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 164 (ShuffledRDD[166] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_107 stored as values in memory (estimated size 3.8 KB, free 901.3 MB)
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_107_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.3 MB)
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_107_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.SparkContext]Created broadcast 107 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 164 (ShuffledRDD[166] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 164.0 with 1 tasks
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 164.0 (TID 107, localhost, partition 0, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.executor.Executor]Running task 0.0 in stage 164.0 (TID 107)
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 164.0 (TID 107). 1549 bytes result sent to driver
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 164.0 (TID 107) in 4 ms on localhost (1/1)
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 164.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.scheduler.DAGScheduler]ResultStage 164 (print at SparkStreamingT.java:101) finished in 0.008 s
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.scheduler.DAGScheduler]Job 107 finished: print at SparkStreamingT.java:101, took 0.014549 s
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 28 is 83 bytes
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.scheduler.DAGScheduler]Got job 108 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 166 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 165)
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 166 (ShuffledRDD[166] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_108 stored as values in memory (estimated size 3.8 KB, free 901.3 MB)
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_108_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.3 MB)
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_108_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.SparkContext]Created broadcast 108 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 166 (ShuffledRDD[166] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 166.0 with 1 tasks
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 166.0 (TID 108, localhost, partition 1, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.executor.Executor]Running task 0.0 in stage 166.0 (TID 108)
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 166.0 (TID 108). 1549 bytes result sent to driver
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 166.0 (TID 108) in 55 ms on localhost (1/1)
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 166.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.scheduler.DAGScheduler]ResultStage 166 (print at SparkStreamingT.java:101) finished in 0.055 s
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.scheduler.DAGScheduler]Job 108 finished: print at SparkStreamingT.java:101, took 0.063105 s
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526887802000 ms.0 from job set of time 1526887802000 ms
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.237 s for time 1526887802000 ms (execution: 0.222 s)
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 159 from persistence list
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 158 from persistence list
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.storage.BlockManager]Removing RDD 159
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.storage.BlockManager]Removing RDD 158
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 157 from persistence list
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887802200
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.storage.BlockManager]Removing RDD 157
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.rdd.BlockRDD]Removing RDD 156 from persistence list
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.storage.BlockManager]Removing RDD 156
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.streaming.dstream.SocketInputDStream]Removing blocks of RDD BlockRDD[156] at socketTextStream at SparkStreamingT.java:72 of time 1526887802000 ms
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 1526887800000 ms
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1526887800000 ms
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:30:02] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:30:02] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 162.0 (TID 106)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN]  [2018-05-21 15:30:02] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 162.0 (TID 106, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:30:02] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 162.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 162.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 162
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.scheduler.DAGScheduler]ResultStage 162 (start at SparkStreamingT.java:107) failed in 0.432 s
[ERROR]  [2018-05-21 15:30:02] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 162.0 failed 1 times, most recent failure: Lost task 0.0 in stage 162.0 (TID 106, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.scheduler.DAGScheduler]Got job 109 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 167 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 167 (Receiver 0 ParallelCollectionRDD[167] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_109 stored as values in memory (estimated size 34.1 KB, free 901.3 MB)
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_109_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.3 MB)
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_109_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.SparkContext]Created broadcast 109 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 167 (Receiver 0 ParallelCollectionRDD[167] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 167.0 with 1 tasks
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 167.0 (TID 109, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.executor.Executor]Running task 0.0 in stage 167.0 (TID 109)
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887802400
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:30:02] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:30:03] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526887803000 ms
[INFO]  [2018-05-21 15:30:03] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526887803000 ms.0 from job set of time 1526887803000 ms
[INFO]  [2018-05-21 15:30:03] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:30:03] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 170 (mapToPair at SparkStreamingT.java:87)
[INFO]  [2018-05-21 15:30:03] [org.apache.spark.scheduler.DAGScheduler]Got job 110 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:30:03] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 169 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:30:03] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 168)
[INFO]  [2018-05-21 15:30:03] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:03] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 169 (ShuffledRDD[171] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:30:03] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_110 stored as values in memory (estimated size 3.8 KB, free 901.3 MB)
[INFO]  [2018-05-21 15:30:03] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_110_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.3 MB)
[INFO]  [2018-05-21 15:30:03] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_110_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:03] [org.apache.spark.SparkContext]Created broadcast 110 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:03] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 169 (ShuffledRDD[171] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:30:03] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 169.0 with 1 tasks
[INFO]  [2018-05-21 15:30:03] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 169.0 (TID 110, localhost, partition 0, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:30:03] [org.apache.spark.executor.Executor]Running task 0.0 in stage 169.0 (TID 110)
[INFO]  [2018-05-21 15:30:03] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:30:03] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:30:03] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 169.0 (TID 110). 1549 bytes result sent to driver
[INFO]  [2018-05-21 15:30:03] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 169.0 (TID 110) in 2 ms on localhost (1/1)
[INFO]  [2018-05-21 15:30:03] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 169.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:03] [org.apache.spark.scheduler.DAGScheduler]ResultStage 169 (print at SparkStreamingT.java:101) finished in 0.003 s
[INFO]  [2018-05-21 15:30:03] [org.apache.spark.scheduler.DAGScheduler]Job 110 finished: print at SparkStreamingT.java:101, took 0.029246 s
[INFO]  [2018-05-21 15:30:03] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:30:03] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 29 is 83 bytes
[INFO]  [2018-05-21 15:30:03] [org.apache.spark.scheduler.DAGScheduler]Got job 111 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:30:03] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 171 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:30:03] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 170)
[INFO]  [2018-05-21 15:30:03] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:03] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 171 (ShuffledRDD[171] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:30:03] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_111 stored as values in memory (estimated size 3.8 KB, free 901.3 MB)
[INFO]  [2018-05-21 15:30:03] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_111_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.3 MB)
[INFO]  [2018-05-21 15:30:03] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_111_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:03] [org.apache.spark.SparkContext]Created broadcast 111 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:03] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 171 (ShuffledRDD[171] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:30:03] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 171.0 with 1 tasks
[INFO]  [2018-05-21 15:30:03] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 171.0 (TID 111, localhost, partition 1, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:30:03] [org.apache.spark.executor.Executor]Running task 0.0 in stage 171.0 (TID 111)
[INFO]  [2018-05-21 15:30:03] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:30:03] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:30:03] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 171.0 (TID 111). 1470 bytes result sent to driver
[INFO]  [2018-05-21 15:30:03] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 171.0 (TID 111) in 3 ms on localhost (1/1)
[INFO]  [2018-05-21 15:30:03] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 171.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:03] [org.apache.spark.scheduler.DAGScheduler]ResultStage 171 (print at SparkStreamingT.java:101) finished in 0.003 s
[INFO]  [2018-05-21 15:30:03] [org.apache.spark.scheduler.DAGScheduler]Job 111 finished: print at SparkStreamingT.java:101, took 0.011730 s
[INFO]  [2018-05-21 15:30:03] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526887803000 ms.0 from job set of time 1526887803000 ms
[INFO]  [2018-05-21 15:30:03] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.053 s for time 1526887803000 ms (execution: 0.047 s)
[INFO]  [2018-05-21 15:30:03] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 166 from persistence list
[INFO]  [2018-05-21 15:30:03] [org.apache.spark.storage.BlockManager]Removing RDD 166
[INFO]  [2018-05-21 15:30:03] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 165 from persistence list
[INFO]  [2018-05-21 15:30:03] [org.apache.spark.storage.BlockManager]Removing RDD 165
[INFO]  [2018-05-21 15:30:03] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 164 from persistence list
[INFO]  [2018-05-21 15:30:03] [org.apache.spark.storage.BlockManager]Removing RDD 164
[INFO]  [2018-05-21 15:30:03] [org.apache.spark.rdd.BlockRDD]Removing RDD 163 from persistence list
[INFO]  [2018-05-21 15:30:03] [org.apache.spark.storage.BlockManager]Removing RDD 163
[INFO]  [2018-05-21 15:30:03] [org.apache.spark.streaming.dstream.SocketInputDStream]Removing blocks of RDD BlockRDD[163] at socketTextStream at SparkStreamingT.java:72 of time 1526887803000 ms
[INFO]  [2018-05-21 15:30:03] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 1526887801000 ms
[INFO]  [2018-05-21 15:30:03] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1526887801000 ms
[INFO]  [2018-05-21 15:30:04] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526887804000 ms
[INFO]  [2018-05-21 15:30:04] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526887804000 ms.0 from job set of time 1526887804000 ms
[INFO]  [2018-05-21 15:30:04] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:30:04] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 174 (mapToPair at SparkStreamingT.java:87)
[INFO]  [2018-05-21 15:30:04] [org.apache.spark.scheduler.DAGScheduler]Got job 112 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:30:04] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 173 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:30:04] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 172)
[INFO]  [2018-05-21 15:30:04] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:04] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 173 (ShuffledRDD[175] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:30:04] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_112 stored as values in memory (estimated size 3.8 KB, free 901.2 MB)
[INFO]  [2018-05-21 15:30:04] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_112_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.2 MB)
[INFO]  [2018-05-21 15:30:04] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_112_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:04] [org.apache.spark.SparkContext]Created broadcast 112 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:04] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 173 (ShuffledRDD[175] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:30:04] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 173.0 with 1 tasks
[INFO]  [2018-05-21 15:30:04] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 173.0 (TID 112, localhost, partition 0, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:30:04] [org.apache.spark.executor.Executor]Running task 0.0 in stage 173.0 (TID 112)
[INFO]  [2018-05-21 15:30:04] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:30:04] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:30:04] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 173.0 (TID 112). 1549 bytes result sent to driver
[INFO]  [2018-05-21 15:30:04] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 173.0 (TID 112) in 2 ms on localhost (1/1)
[INFO]  [2018-05-21 15:30:04] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 173.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:04] [org.apache.spark.scheduler.DAGScheduler]ResultStage 173 (print at SparkStreamingT.java:101) finished in 0.003 s
[INFO]  [2018-05-21 15:30:04] [org.apache.spark.scheduler.DAGScheduler]Job 112 finished: print at SparkStreamingT.java:101, took 0.010604 s
[INFO]  [2018-05-21 15:30:04] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:30:04] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 30 is 83 bytes
[INFO]  [2018-05-21 15:30:04] [org.apache.spark.scheduler.DAGScheduler]Got job 113 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:30:04] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 175 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:30:04] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 174)
[INFO]  [2018-05-21 15:30:04] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:04] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 175 (ShuffledRDD[175] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:30:04] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_113 stored as values in memory (estimated size 3.8 KB, free 901.2 MB)
[INFO]  [2018-05-21 15:30:04] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_113_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.2 MB)
[INFO]  [2018-05-21 15:30:04] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_113_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:04] [org.apache.spark.SparkContext]Created broadcast 113 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:04] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 175 (ShuffledRDD[175] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:30:04] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 175.0 with 1 tasks
[INFO]  [2018-05-21 15:30:04] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 175.0 (TID 113, localhost, partition 1, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:30:04] [org.apache.spark.executor.Executor]Running task 0.0 in stage 175.0 (TID 113)
[INFO]  [2018-05-21 15:30:04] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:30:04] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:30:04] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 175.0 (TID 113). 1549 bytes result sent to driver
[INFO]  [2018-05-21 15:30:04] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 175.0 (TID 113) in 2 ms on localhost (1/1)
[INFO]  [2018-05-21 15:30:04] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 175.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:04] [org.apache.spark.scheduler.DAGScheduler]ResultStage 175 (print at SparkStreamingT.java:101) finished in 0.003 s
[INFO]  [2018-05-21 15:30:04] [org.apache.spark.scheduler.DAGScheduler]Job 113 finished: print at SparkStreamingT.java:101, took 0.007610 s
[INFO]  [2018-05-21 15:30:04] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526887804000 ms.0 from job set of time 1526887804000 ms
[INFO]  [2018-05-21 15:30:04] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.041 s for time 1526887804000 ms (execution: 0.029 s)
[INFO]  [2018-05-21 15:30:04] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 171 from persistence list
[INFO]  [2018-05-21 15:30:04] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 170 from persistence list
[INFO]  [2018-05-21 15:30:04] [org.apache.spark.storage.BlockManager]Removing RDD 171
[INFO]  [2018-05-21 15:30:04] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 169 from persistence list
[INFO]  [2018-05-21 15:30:04] [org.apache.spark.storage.BlockManager]Removing RDD 170
[INFO]  [2018-05-21 15:30:04] [org.apache.spark.rdd.BlockRDD]Removing RDD 168 from persistence list
[INFO]  [2018-05-21 15:30:04] [org.apache.spark.storage.BlockManager]Removing RDD 169
[INFO]  [2018-05-21 15:30:04] [org.apache.spark.storage.BlockManager]Removing RDD 168
[INFO]  [2018-05-21 15:30:04] [org.apache.spark.streaming.dstream.SocketInputDStream]Removing blocks of RDD BlockRDD[168] at socketTextStream at SparkStreamingT.java:72 of time 1526887804000 ms
[INFO]  [2018-05-21 15:30:04] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 1526887802000 ms
[INFO]  [2018-05-21 15:30:04] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1526887802000 ms
[INFO]  [2018-05-21 15:30:04] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:30:04] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:30:04] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[ERROR]  [2018-05-21 15:30:04] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:30:04] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:30:04] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526887805000 ms
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526887805000 ms.0 from job set of time 1526887805000 ms
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 178 (mapToPair at SparkStreamingT.java:87)
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.scheduler.DAGScheduler]Got job 114 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 177 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 176)
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 177 (ShuffledRDD[179] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_114 stored as values in memory (estimated size 3.8 KB, free 901.2 MB)
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_114_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.2 MB)
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_114_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.6 MB)
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.SparkContext]Created broadcast 114 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 177 (ShuffledRDD[179] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 177.0 with 1 tasks
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 177.0 (TID 114, localhost, partition 0, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.executor.Executor]Running task 0.0 in stage 177.0 (TID 114)
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 177.0 (TID 114). 1470 bytes result sent to driver
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 177.0 (TID 114) in 4 ms on localhost (1/1)
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 177.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.scheduler.DAGScheduler]ResultStage 177 (print at SparkStreamingT.java:101) finished in 0.004 s
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.scheduler.DAGScheduler]Job 114 finished: print at SparkStreamingT.java:101, took 0.021650 s
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 31 is 83 bytes
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.scheduler.DAGScheduler]Got job 115 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 179 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 178)
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 179 (ShuffledRDD[179] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_115 stored as values in memory (estimated size 3.8 KB, free 901.2 MB)
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_115_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.2 MB)
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_115_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.6 MB)
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.SparkContext]Created broadcast 115 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 179 (ShuffledRDD[179] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 179.0 with 1 tasks
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 179.0 (TID 115, localhost, partition 1, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.executor.Executor]Running task 0.0 in stage 179.0 (TID 115)
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 179.0 (TID 115). 1470 bytes result sent to driver
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 179.0 (TID 115) in 5 ms on localhost (1/1)
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 179.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.scheduler.DAGScheduler]ResultStage 179 (print at SparkStreamingT.java:101) finished in 0.008 s
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.scheduler.DAGScheduler]Job 115 finished: print at SparkStreamingT.java:101, took 0.012500 s
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526887805000 ms.0 from job set of time 1526887805000 ms
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.046 s for time 1526887805000 ms (execution: 0.040 s)
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 175 from persistence list
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.storage.BlockManager]Removing RDD 175
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 174 from persistence list
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 173 from persistence list
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.storage.BlockManager]Removing RDD 174
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.rdd.BlockRDD]Removing RDD 172 from persistence list
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.storage.BlockManager]Removing RDD 173
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.storage.BlockManager]Removing RDD 172
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.streaming.dstream.SocketInputDStream]Removing blocks of RDD BlockRDD[172] at socketTextStream at SparkStreamingT.java:72 of time 1526887805000 ms
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 1526887803000 ms
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1526887803000 ms
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887805200
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:30:05] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:30:05] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 167.0 (TID 109)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN]  [2018-05-21 15:30:05] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 167.0 (TID 109, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:30:05] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 167.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 167.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 167
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.scheduler.DAGScheduler]ResultStage 167 (start at SparkStreamingT.java:107) failed in 2.929 s
[ERROR]  [2018-05-21 15:30:05] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 167.0 failed 1 times, most recent failure: Lost task 0.0 in stage 167.0 (TID 109, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.scheduler.DAGScheduler]Got job 116 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 180 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 180 (Receiver 0 ParallelCollectionRDD[180] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_116 stored as values in memory (estimated size 34.1 KB, free 901.2 MB)
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_116_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.2 MB)
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_116_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.6 MB)
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.SparkContext]Created broadcast 116 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 180 (Receiver 0 ParallelCollectionRDD[180] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 180.0 with 1 tasks
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 180.0 (TID 116, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.executor.Executor]Running task 0.0 in stage 180.0 (TID 116)
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887805400
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[ERROR]  [2018-05-21 15:30:05] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:30:05] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887805600
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:30:05] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:30:05] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 180.0 (TID 116)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN]  [2018-05-21 15:30:05] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 180.0 (TID 116, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:30:05] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 180.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 180.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 180
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.scheduler.DAGScheduler]ResultStage 180 (start at SparkStreamingT.java:107) failed in 0.370 s
[ERROR]  [2018-05-21 15:30:05] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 180.0 failed 1 times, most recent failure: Lost task 0.0 in stage 180.0 (TID 116, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.scheduler.DAGScheduler]Got job 117 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 181 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 181 (Receiver 0 ParallelCollectionRDD[181] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_117 stored as values in memory (estimated size 34.1 KB, free 901.2 MB)
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_117_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.1 MB)
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_117_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.6 MB)
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.SparkContext]Created broadcast 117 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 181 (Receiver 0 ParallelCollectionRDD[181] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 181.0 with 1 tasks
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 181.0 (TID 117, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.executor.Executor]Running task 0.0 in stage 181.0 (TID 117)
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887805800
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[ERROR]  [2018-05-21 15:30:05] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:30:05] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:30:05] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887806000
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:30:06] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:30:06] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 181.0 (TID 117)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526887806000 ms
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526887806000 ms.0 from job set of time 1526887806000 ms
[WARN]  [2018-05-21 15:30:06] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 181.0 (TID 117, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:30:06] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 181.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 181.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 181
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.scheduler.DAGScheduler]ResultStage 181 (start at SparkStreamingT.java:107) failed in 0.282 s
[ERROR]  [2018-05-21 15:30:06] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 181.0 failed 1 times, most recent failure: Lost task 0.0 in stage 181.0 (TID 117, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 184 (mapToPair at SparkStreamingT.java:87)
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.scheduler.DAGScheduler]Got job 118 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 183 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 182)
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 183 (ShuffledRDD[185] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_118 stored as values in memory (estimated size 3.8 KB, free 901.1 MB)
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_118_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.1 MB)
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_118_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.6 MB)
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.SparkContext]Created broadcast 118 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 183 (ShuffledRDD[185] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 183.0 with 1 tasks
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 183.0 (TID 118, localhost, partition 0, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.ContextCleaner]Cleaned shuffle 24
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.executor.Executor]Running task 0.0 in stage 183.0 (TID 118)
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 183.0 (TID 118). 1470 bytes result sent to driver
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 183.0 (TID 118) in 5 ms on localhost (1/1)
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 183.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_92_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.6 MB)
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.scheduler.DAGScheduler]ResultStage 183 (print at SparkStreamingT.java:101) finished in 0.008 s
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.scheduler.DAGScheduler]Job 118 finished: print at SparkStreamingT.java:101, took 0.033959 s
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.scheduler.DAGScheduler]Got job 119 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 184 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 184 (Receiver 0 ParallelCollectionRDD[186] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.ContextCleaner]Cleaned shuffle 25
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_93_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.6 MB)
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_94_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.6 MB)
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_95_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.6 MB)
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_96_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_119 stored as values in memory (estimated size 34.1 KB, free 901.2 MB)
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.ContextCleaner]Cleaned shuffle 26
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_119_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.2 MB)
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_97_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_119_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.SparkContext]Created broadcast 119 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 184 (Receiver 0 ParallelCollectionRDD[186] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 184.0 with 1 tasks
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 32 is 83 bytes
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.scheduler.DAGScheduler]Got job 120 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 186 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 185)
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 186 (ShuffledRDD[185] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_120 stored as values in memory (estimated size 3.8 KB, free 901.2 MB)
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_120_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.2 MB)
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_120_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.6 MB)
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.SparkContext]Created broadcast 120 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 186 (ShuffledRDD[185] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 186.0 with 1 tasks
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 184.0 (TID 119, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.executor.Executor]Running task 0.0 in stage 184.0 (TID 119)
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 186.0 (TID 120, localhost, partition 1, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_98_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.executor.Executor]Running task 0.0 in stage 186.0 (TID 120)
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 186.0 (TID 120). 1470 bytes result sent to driver
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887806200
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 186.0 (TID 120) in 8 ms on localhost (1/1)
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 186.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.scheduler.DAGScheduler]ResultStage 186 (print at SparkStreamingT.java:101) finished in 0.008 s
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.scheduler.DAGScheduler]Job 120 finished: print at SparkStreamingT.java:101, took 0.020250 s
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526887806000 ms.0 from job set of time 1526887806000 ms
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.077 s for time 1526887806000 ms (execution: 0.065 s)
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 179 from persistence list
[ERROR]  [2018-05-21 15:30:06] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:30:06] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_99_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.storage.BlockManager]Removing RDD 179
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 178 from persistence list
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 177 from persistence list
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.storage.BlockManager]Removing RDD 178
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.rdd.BlockRDD]Removing RDD 176 from persistence list
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.storage.BlockManager]Removing RDD 177
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.streaming.dstream.SocketInputDStream]Removing blocks of RDD BlockRDD[176] at socketTextStream at SparkStreamingT.java:72 of time 1526887806000 ms
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 1526887804000 ms
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1526887804000 ms
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.storage.BlockManager]Removing RDD 176
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_100_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_101_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.ContextCleaner]Cleaned shuffle 27
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_102_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_103_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_104_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_105_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_106_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.ContextCleaner]Cleaned shuffle 28
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_107_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_108_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_109_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.ContextCleaner]Cleaned shuffle 29
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_110_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_111_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.ContextCleaner]Cleaned shuffle 30
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_112_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_113_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_114_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_115_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_116_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_117_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887806400
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:30:06] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:30:06] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 184.0 (TID 119)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN]  [2018-05-21 15:30:06] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 184.0 (TID 119, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:30:06] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 184.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 184.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 184
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.scheduler.DAGScheduler]ResultStage 184 (start at SparkStreamingT.java:107) failed in 0.367 s
[ERROR]  [2018-05-21 15:30:06] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 184.0 failed 1 times, most recent failure: Lost task 0.0 in stage 184.0 (TID 119, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.scheduler.DAGScheduler]Got job 121 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 187 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 187 (Receiver 0 ParallelCollectionRDD[187] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_121 stored as values in memory (estimated size 34.1 KB, free 901.7 MB)
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_121_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.7 MB)
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_121_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.SparkContext]Created broadcast 121 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 187 (Receiver 0 ParallelCollectionRDD[187] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 187.0 with 1 tasks
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 187.0 (TID 121, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.executor.Executor]Running task 0.0 in stage 187.0 (TID 121)
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887806600
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[ERROR]  [2018-05-21 15:30:06] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:30:06] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887806600
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:30:06] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:30:06] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 187.0 (TID 121)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN]  [2018-05-21 15:30:06] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 187.0 (TID 121, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:30:06] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 187.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 187.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 187
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.scheduler.DAGScheduler]ResultStage 187 (start at SparkStreamingT.java:107) failed in 0.165 s
[ERROR]  [2018-05-21 15:30:06] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 187.0 failed 1 times, most recent failure: Lost task 0.0 in stage 187.0 (TID 121, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.scheduler.DAGScheduler]Got job 122 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 188 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 188 (Receiver 0 ParallelCollectionRDD[188] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_122 stored as values in memory (estimated size 34.1 KB, free 901.6 MB)
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_122_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.6 MB)
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_122_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.SparkContext]Created broadcast 122 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 188 (Receiver 0 ParallelCollectionRDD[188] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 188.0 with 1 tasks
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 188.0 (TID 122, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.executor.Executor]Running task 0.0 in stage 188.0 (TID 122)
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887806800
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[ERROR]  [2018-05-21 15:30:06] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:30:06] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:30:06] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887807000
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526887807000 ms
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526887807000 ms.0 from job set of time 1526887807000 ms
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:30:07] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:30:07] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 188.0 (TID 122)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 191 (mapToPair at SparkStreamingT.java:87)
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.scheduler.DAGScheduler]Got job 123 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 190 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 189)
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[WARN]  [2018-05-21 15:30:07] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 188.0 (TID 122, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:30:07] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 190 (ShuffledRDD[192] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[ERROR]  [2018-05-21 15:30:07] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 188.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 188.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_123 stored as values in memory (estimated size 3.8 KB, free 901.6 MB)
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_123_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.6 MB)
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_123_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.SparkContext]Created broadcast 123 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 190 (ShuffledRDD[192] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 190.0 with 1 tasks
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 190.0 (TID 123, localhost, partition 0, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 188
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.executor.Executor]Running task 0.0 in stage 190.0 (TID 123)
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.scheduler.DAGScheduler]ResultStage 188 (start at SparkStreamingT.java:107) failed in 0.345 s
[ERROR]  [2018-05-21 15:30:07] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 188.0 failed 1 times, most recent failure: Lost task 0.0 in stage 188.0 (TID 122, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 2 ms
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 190.0 (TID 123). 1462 bytes result sent to driver
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 190.0 (TID 123) in 2 ms on localhost (1/1)
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 190.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.scheduler.DAGScheduler]ResultStage 190 (print at SparkStreamingT.java:101) finished in 0.002 s
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.scheduler.DAGScheduler]Job 123 finished: print at SparkStreamingT.java:101, took 0.010589 s
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 33 is 83 bytes
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.scheduler.DAGScheduler]Got job 124 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 192 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 191)
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 192 (ShuffledRDD[192] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_124 stored as values in memory (estimated size 3.8 KB, free 901.6 MB)
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_124_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.6 MB)
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_124_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.SparkContext]Created broadcast 124 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 192 (ShuffledRDD[192] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 192.0 with 1 tasks
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.scheduler.DAGScheduler]Got job 125 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 193 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 192.0 (TID 124, localhost, partition 1, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 193 (Receiver 0 ParallelCollectionRDD[193] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.executor.Executor]Running task 0.0 in stage 192.0 (TID 124)
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 192.0 (TID 124). 1462 bytes result sent to driver
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_125 stored as values in memory (estimated size 34.1 KB, free 901.6 MB)
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_125_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.6 MB)
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_125_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.SparkContext]Created broadcast 125 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 193 (Receiver 0 ParallelCollectionRDD[193] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 193.0 with 1 tasks
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 193.0 (TID 125, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 192.0 (TID 124) in 15 ms on localhost (1/1)
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 192.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.scheduler.DAGScheduler]ResultStage 192 (print at SparkStreamingT.java:101) finished in 0.015 s
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.executor.Executor]Running task 0.0 in stage 193.0 (TID 125)
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.scheduler.DAGScheduler]Job 124 finished: print at SparkStreamingT.java:101, took 0.021065 s
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526887807000 ms.0 from job set of time 1526887807000 ms
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.047 s for time 1526887807000 ms (execution: 0.043 s)
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 185 from persistence list
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.storage.BlockManager]Removing RDD 185
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 184 from persistence list
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 183 from persistence list
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.storage.BlockManager]Removing RDD 184
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.rdd.BlockRDD]Removing RDD 182 from persistence list
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.storage.BlockManager]Removing RDD 183
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.storage.BlockManager]Removing RDD 182
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.streaming.dstream.SocketInputDStream]Removing blocks of RDD BlockRDD[182] at socketTextStream at SparkStreamingT.java:72 of time 1526887807000 ms
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 1526887805000 ms
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1526887805000 ms
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887807200
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[ERROR]  [2018-05-21 15:30:07] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:30:07] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887807400
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:30:07] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:30:07] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 193.0 (TID 125)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN]  [2018-05-21 15:30:07] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 193.0 (TID 125, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:30:07] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 193.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 193.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 193
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.scheduler.DAGScheduler]ResultStage 193 (start at SparkStreamingT.java:107) failed in 0.388 s
[ERROR]  [2018-05-21 15:30:07] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 193.0 failed 1 times, most recent failure: Lost task 0.0 in stage 193.0 (TID 125, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.scheduler.DAGScheduler]Got job 126 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 194 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 194 (Receiver 0 ParallelCollectionRDD[194] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_126 stored as values in memory (estimated size 34.1 KB, free 901.5 MB)
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_126_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.5 MB)
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_126_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.SparkContext]Created broadcast 126 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 194 (Receiver 0 ParallelCollectionRDD[194] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 194.0 with 1 tasks
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 194.0 (TID 126, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.executor.Executor]Running task 0.0 in stage 194.0 (TID 126)
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887807600
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[ERROR]  [2018-05-21 15:30:07] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:30:07] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887807800
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:30:07] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:30:07] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 194.0 (TID 126)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN]  [2018-05-21 15:30:07] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 194.0 (TID 126, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:30:07] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 194.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 194.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 194
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.scheduler.DAGScheduler]ResultStage 194 (start at SparkStreamingT.java:107) failed in 0.489 s
[ERROR]  [2018-05-21 15:30:07] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 194.0 failed 1 times, most recent failure: Lost task 0.0 in stage 194.0 (TID 126, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.scheduler.DAGScheduler]Got job 127 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 195 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 195 (Receiver 0 ParallelCollectionRDD[195] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_127 stored as values in memory (estimated size 34.1 KB, free 901.5 MB)
[INFO]  [2018-05-21 15:30:07] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_127_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.5 MB)
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_127_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.SparkContext]Created broadcast 127 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 195 (Receiver 0 ParallelCollectionRDD[195] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 195.0 with 1 tasks
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 195.0 (TID 127, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.executor.Executor]Running task 0.0 in stage 195.0 (TID 127)
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887808200
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[ERROR]  [2018-05-21 15:30:08] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:30:08] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526887808000 ms
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 198 (mapToPair at SparkStreamingT.java:87)
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.scheduler.DAGScheduler]Got job 128 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 197 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 196)
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 197 (ShuffledRDD[199] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_128 stored as values in memory (estimated size 3.8 KB, free 901.5 MB)
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_128_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.5 MB)
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526887808000 ms.0 from job set of time 1526887808000 ms
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_128_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.SparkContext]Created broadcast 128 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 197 (ShuffledRDD[199] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 197.0 with 1 tasks
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 197.0 (TID 128, localhost, partition 0, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.executor.Executor]Running task 0.0 in stage 197.0 (TID 128)
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 197.0 (TID 128). 1549 bytes result sent to driver
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 197.0 (TID 128) in 4 ms on localhost (1/1)
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 197.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.scheduler.DAGScheduler]ResultStage 197 (print at SparkStreamingT.java:101) finished in 0.021 s
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.scheduler.DAGScheduler]Job 128 finished: print at SparkStreamingT.java:101, took 0.028227 s
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 34 is 83 bytes
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.scheduler.DAGScheduler]Got job 129 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 199 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 198)
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 199 (ShuffledRDD[199] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_129 stored as values in memory (estimated size 3.8 KB, free 901.5 MB)
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_129_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.5 MB)
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_129_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.SparkContext]Created broadcast 129 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 199 (ShuffledRDD[199] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 199.0 with 1 tasks
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 199.0 (TID 129, localhost, partition 1, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.executor.Executor]Running task 0.0 in stage 199.0 (TID 129)
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 199.0 (TID 129). 1549 bytes result sent to driver
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 199.0 (TID 129) in 4 ms on localhost (1/1)
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 199.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.scheduler.DAGScheduler]ResultStage 199 (print at SparkStreamingT.java:101) finished in 0.006 s
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.scheduler.DAGScheduler]Job 129 finished: print at SparkStreamingT.java:101, took 0.011805 s
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526887808000 ms.0 from job set of time 1526887808000 ms
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.107 s for time 1526887808000 ms (execution: 0.057 s)
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 192 from persistence list
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.storage.BlockManager]Removing RDD 192
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 191 from persistence list
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.storage.BlockManager]Removing RDD 191
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 190 from persistence list
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.storage.BlockManager]Removing RDD 190
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.rdd.BlockRDD]Removing RDD 189 from persistence list
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.streaming.dstream.SocketInputDStream]Removing blocks of RDD BlockRDD[189] at socketTextStream at SparkStreamingT.java:72 of time 1526887808000 ms
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 1526887806000 ms
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1526887806000 ms
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.storage.BlockManager]Removing RDD 189
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887808400
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:30:08] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:30:08] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 195.0 (TID 127)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN]  [2018-05-21 15:30:08] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 195.0 (TID 127, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:30:08] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 195.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 195.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 195
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.scheduler.DAGScheduler]ResultStage 195 (start at SparkStreamingT.java:107) failed in 0.403 s
[ERROR]  [2018-05-21 15:30:08] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 195.0 failed 1 times, most recent failure: Lost task 0.0 in stage 195.0 (TID 127, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.scheduler.DAGScheduler]Got job 130 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 200 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 200 (Receiver 0 ParallelCollectionRDD[200] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_130 stored as values in memory (estimated size 34.1 KB, free 901.4 MB)
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_130_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.4 MB)
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_130_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.SparkContext]Created broadcast 130 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 200 (Receiver 0 ParallelCollectionRDD[200] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 200.0 with 1 tasks
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 200.0 (TID 130, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.executor.Executor]Running task 0.0 in stage 200.0 (TID 130)
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887808600
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[ERROR]  [2018-05-21 15:30:08] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:30:08] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887808800
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:30:08] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:30:08] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 200.0 (TID 130)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN]  [2018-05-21 15:30:08] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 200.0 (TID 130, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:30:08] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 200.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 200.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 200
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.scheduler.DAGScheduler]ResultStage 200 (start at SparkStreamingT.java:107) failed in 0.368 s
[ERROR]  [2018-05-21 15:30:08] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 200.0 failed 1 times, most recent failure: Lost task 0.0 in stage 200.0 (TID 130, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.scheduler.DAGScheduler]Got job 131 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 201 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 201 (Receiver 0 ParallelCollectionRDD[201] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_131 stored as values in memory (estimated size 34.1 KB, free 901.4 MB)
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_131_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.4 MB)
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_131_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.SparkContext]Created broadcast 131 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 201 (Receiver 0 ParallelCollectionRDD[201] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 201.0 with 1 tasks
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 201.0 (TID 131, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.executor.Executor]Running task 0.0 in stage 201.0 (TID 131)
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887809000
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[ERROR]  [2018-05-21 15:30:08] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:30:08] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:30:08] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526887809000 ms
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526887809000 ms.0 from job set of time 1526887809000 ms
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 204 (mapToPair at SparkStreamingT.java:87)
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.scheduler.DAGScheduler]Got job 132 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 203 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 202)
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 203 (ShuffledRDD[205] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_132 stored as values in memory (estimated size 3.8 KB, free 901.4 MB)
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_132_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.4 MB)
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_132_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.SparkContext]Created broadcast 132 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 203 (ShuffledRDD[205] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 203.0 with 1 tasks
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 203.0 (TID 132, localhost, partition 0, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.executor.Executor]Running task 0.0 in stage 203.0 (TID 132)
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 203.0 (TID 132). 1549 bytes result sent to driver
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 203.0 (TID 132) in 4 ms on localhost (1/1)
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 203.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.scheduler.DAGScheduler]ResultStage 203 (print at SparkStreamingT.java:101) finished in 0.005 s
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.scheduler.DAGScheduler]Job 132 finished: print at SparkStreamingT.java:101, took 0.008575 s
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 35 is 83 bytes
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.scheduler.DAGScheduler]Got job 133 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 205 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 204)
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 205 (ShuffledRDD[205] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_133 stored as values in memory (estimated size 3.8 KB, free 901.4 MB)
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_133_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.4 MB)
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_133_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.SparkContext]Created broadcast 133 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 205 (ShuffledRDD[205] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 205.0 with 1 tasks
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 205.0 (TID 133, localhost, partition 1, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.executor.Executor]Running task 0.0 in stage 205.0 (TID 133)
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 205.0 (TID 133). 1470 bytes result sent to driver
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 205.0 (TID 133) in 5 ms on localhost (1/1)
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 205.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.scheduler.DAGScheduler]ResultStage 205 (print at SparkStreamingT.java:101) finished in 0.007 s
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.scheduler.DAGScheduler]Job 133 finished: print at SparkStreamingT.java:101, took 0.015345 s
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526887809000 ms.0 from job set of time 1526887809000 ms
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.033 s for time 1526887809000 ms (execution: 0.029 s)
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 199 from persistence list
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 198 from persistence list
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.storage.BlockManager]Removing RDD 199
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 197 from persistence list
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.storage.BlockManager]Removing RDD 198
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.rdd.BlockRDD]Removing RDD 196 from persistence list
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.storage.BlockManager]Removing RDD 197
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.streaming.dstream.SocketInputDStream]Removing blocks of RDD BlockRDD[196] at socketTextStream at SparkStreamingT.java:72 of time 1526887809000 ms
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 1526887807000 ms
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1526887807000 ms
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.storage.BlockManager]Removing RDD 196
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887809200
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:30:09] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:30:09] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 201.0 (TID 131)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN]  [2018-05-21 15:30:09] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 201.0 (TID 131, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:30:09] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 201.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 201.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 201
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.scheduler.DAGScheduler]ResultStage 201 (start at SparkStreamingT.java:107) failed in 0.342 s
[ERROR]  [2018-05-21 15:30:09] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 201.0 failed 1 times, most recent failure: Lost task 0.0 in stage 201.0 (TID 131, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.scheduler.DAGScheduler]Got job 134 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 206 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 206 (Receiver 0 ParallelCollectionRDD[206] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_134 stored as values in memory (estimated size 34.1 KB, free 901.3 MB)
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_134_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.3 MB)
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_134_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.SparkContext]Created broadcast 134 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 206 (Receiver 0 ParallelCollectionRDD[206] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 206.0 with 1 tasks
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 206.0 (TID 134, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.executor.Executor]Running task 0.0 in stage 206.0 (TID 134)
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887809400
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[ERROR]  [2018-05-21 15:30:09] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:30:09] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887809600
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:30:09] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:30:09] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 206.0 (TID 134)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN]  [2018-05-21 15:30:09] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 206.0 (TID 134, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:30:09] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 206.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 206.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 206
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.scheduler.DAGScheduler]ResultStage 206 (start at SparkStreamingT.java:107) failed in 0.357 s
[ERROR]  [2018-05-21 15:30:09] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 206.0 failed 1 times, most recent failure: Lost task 0.0 in stage 206.0 (TID 134, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.scheduler.DAGScheduler]Got job 135 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 207 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 207 (Receiver 0 ParallelCollectionRDD[207] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_135 stored as values in memory (estimated size 34.1 KB, free 901.3 MB)
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_135_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.3 MB)
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_135_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.SparkContext]Created broadcast 135 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 207 (Receiver 0 ParallelCollectionRDD[207] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 207.0 with 1 tasks
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 207.0 (TID 135, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.executor.Executor]Running task 0.0 in stage 207.0 (TID 135)
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887809800
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[ERROR]  [2018-05-21 15:30:09] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:30:09] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:30:09] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887810000
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:30:10] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:30:10] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 207.0 (TID 135)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN]  [2018-05-21 15:30:10] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 207.0 (TID 135, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:30:10] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 207.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 207.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 207
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.scheduler.DAGScheduler]ResultStage 207 (start at SparkStreamingT.java:107) failed in 0.367 s
[ERROR]  [2018-05-21 15:30:10] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 207.0 failed 1 times, most recent failure: Lost task 0.0 in stage 207.0 (TID 135, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526887810000 ms.0 from job set of time 1526887810000 ms
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526887810000 ms
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 210 (mapToPair at SparkStreamingT.java:87)
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.scheduler.DAGScheduler]Got job 136 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 209 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 208)
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 209 (ShuffledRDD[211] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_136 stored as values in memory (estimated size 3.8 KB, free 901.3 MB)
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_136_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.3 MB)
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_136_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.SparkContext]Created broadcast 136 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 209 (ShuffledRDD[211] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 209.0 with 1 tasks
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 209.0 (TID 136, localhost, partition 0, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.executor.Executor]Running task 0.0 in stage 209.0 (TID 136)
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 209.0 (TID 136). 1470 bytes result sent to driver
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.scheduler.DAGScheduler]Got job 137 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 209.0 (TID 136) in 3 ms on localhost (1/1)
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 210 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 209.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 210 (Receiver 0 ParallelCollectionRDD[212] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_137 stored as values in memory (estimated size 34.1 KB, free 901.2 MB)
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_137_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.2 MB)
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_137_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.6 MB)
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.SparkContext]Created broadcast 137 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 210 (Receiver 0 ParallelCollectionRDD[212] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 210.0 with 1 tasks
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.scheduler.DAGScheduler]ResultStage 209 (print at SparkStreamingT.java:101) finished in 0.010 s
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 210.0 (TID 137, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.scheduler.DAGScheduler]Job 136 finished: print at SparkStreamingT.java:101, took 0.020701 s
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.executor.Executor]Running task 0.0 in stage 210.0 (TID 137)
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 36 is 83 bytes
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.scheduler.DAGScheduler]Got job 138 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 212 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 211)
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 212 (ShuffledRDD[211] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887810200
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_138 stored as values in memory (estimated size 3.8 KB, free 901.2 MB)
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_138_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.2 MB)
[ERROR]  [2018-05-21 15:30:10] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:30:10] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_138_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.6 MB)
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.SparkContext]Created broadcast 138 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 212 (ShuffledRDD[211] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 212.0 with 1 tasks
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 212.0 (TID 138, localhost, partition 1, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.executor.Executor]Running task 0.0 in stage 212.0 (TID 138)
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 212.0 (TID 138). 1383 bytes result sent to driver
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 212.0 (TID 138) in 2 ms on localhost (1/1)
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 212.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.scheduler.DAGScheduler]ResultStage 212 (print at SparkStreamingT.java:101) finished in 0.002 s
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.scheduler.DAGScheduler]Job 138 finished: print at SparkStreamingT.java:101, took 0.007129 s
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526887810000 ms.0 from job set of time 1526887810000 ms
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.052 s for time 1526887810000 ms (execution: 0.040 s)
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 205 from persistence list
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 204 from persistence list
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 203 from persistence list
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.storage.BlockManager]Removing RDD 204
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.storage.BlockManager]Removing RDD 205
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.storage.BlockManager]Removing RDD 203
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.rdd.BlockRDD]Removing RDD 202 from persistence list
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.storage.BlockManager]Removing RDD 202
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.streaming.dstream.SocketInputDStream]Removing blocks of RDD BlockRDD[202] at socketTextStream at SparkStreamingT.java:72 of time 1526887810000 ms
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 1526887808000 ms
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1526887808000 ms
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887810400
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:30:10] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:30:10] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 210.0 (TID 137)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN]  [2018-05-21 15:30:10] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 210.0 (TID 137, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:30:10] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 210.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 210.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 210
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.scheduler.DAGScheduler]ResultStage 210 (start at SparkStreamingT.java:107) failed in 0.368 s
[ERROR]  [2018-05-21 15:30:10] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 210.0 failed 1 times, most recent failure: Lost task 0.0 in stage 210.0 (TID 137, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.scheduler.DAGScheduler]Got job 139 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 213 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 213 (Receiver 0 ParallelCollectionRDD[213] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_139 stored as values in memory (estimated size 34.1 KB, free 901.2 MB)
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_139_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.2 MB)
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_139_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.6 MB)
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.SparkContext]Created broadcast 139 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 213 (Receiver 0 ParallelCollectionRDD[213] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 213.0 with 1 tasks
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 213.0 (TID 139, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.executor.Executor]Running task 0.0 in stage 213.0 (TID 139)
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887810600
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[ERROR]  [2018-05-21 15:30:10] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:30:10] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887810800
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:30:10] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:30:10] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 213.0 (TID 139)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN]  [2018-05-21 15:30:10] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 213.0 (TID 139, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:30:10] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 213.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 213.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 213
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.scheduler.DAGScheduler]ResultStage 213 (start at SparkStreamingT.java:107) failed in 0.363 s
[ERROR]  [2018-05-21 15:30:10] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 213.0 failed 1 times, most recent failure: Lost task 0.0 in stage 213.0 (TID 139, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.scheduler.DAGScheduler]Got job 140 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 214 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 214 (Receiver 0 ParallelCollectionRDD[214] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_140 stored as values in memory (estimated size 34.1 KB, free 901.1 MB)
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_140_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.1 MB)
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_140_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.6 MB)
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.SparkContext]Created broadcast 140 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 214 (Receiver 0 ParallelCollectionRDD[214] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 214.0 with 1 tasks
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 214.0 (TID 140, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.executor.Executor]Running task 0.0 in stage 214.0 (TID 140)
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887811000
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[ERROR]  [2018-05-21 15:30:10] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:30:10] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:30:10] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526887811000 ms
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526887811000 ms.0 from job set of time 1526887811000 ms
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 217 (mapToPair at SparkStreamingT.java:87)
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.scheduler.DAGScheduler]Got job 141 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 216 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 215)
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 216 (ShuffledRDD[218] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_141 stored as values in memory (estimated size 3.8 KB, free 901.1 MB)
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_141_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.1 MB)
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_141_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.6 MB)
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.SparkContext]Created broadcast 141 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 216 (ShuffledRDD[218] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 216.0 with 1 tasks
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 216.0 (TID 141, localhost, partition 0, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.executor.Executor]Running task 0.0 in stage 216.0 (TID 141)
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 216.0 (TID 141). 1462 bytes result sent to driver
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 216.0 (TID 141) in 7 ms on localhost (1/1)
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 216.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.scheduler.DAGScheduler]ResultStage 216 (print at SparkStreamingT.java:101) finished in 0.008 s
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.scheduler.DAGScheduler]Job 141 finished: print at SparkStreamingT.java:101, took 0.012196 s
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 37 is 83 bytes
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.scheduler.DAGScheduler]Got job 142 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 218 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 217)
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 218 (ShuffledRDD[218] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_142 stored as values in memory (estimated size 3.8 KB, free 901.1 MB)
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_142_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.1 MB)
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_142_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.6 MB)
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.SparkContext]Created broadcast 142 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 218 (ShuffledRDD[218] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 218.0 with 1 tasks
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 218.0 (TID 142, localhost, partition 1, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.executor.Executor]Running task 0.0 in stage 218.0 (TID 142)
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 218.0 (TID 142). 1549 bytes result sent to driver
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 218.0 (TID 142) in 6 ms on localhost (1/1)
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 218.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.scheduler.DAGScheduler]ResultStage 218 (print at SparkStreamingT.java:101) finished in 0.006 s
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.scheduler.DAGScheduler]Job 142 finished: print at SparkStreamingT.java:101, took 0.014915 s
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526887811000 ms.0 from job set of time 1526887811000 ms
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.057 s for time 1526887811000 ms (execution: 0.046 s)
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 211 from persistence list
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 210 from persistence list
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.storage.BlockManager]Removing RDD 211
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 209 from persistence list
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.rdd.BlockRDD]Removing RDD 208 from persistence list
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.storage.BlockManager]Removing RDD 210
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.storage.BlockManager]Removing RDD 209
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.storage.BlockManager]Removing RDD 208
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.streaming.dstream.SocketInputDStream]Removing blocks of RDD BlockRDD[208] at socketTextStream at SparkStreamingT.java:72 of time 1526887811000 ms
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 1526887809000 ms
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1526887809000 ms
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887811200
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:30:11] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:30:11] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 214.0 (TID 140)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN]  [2018-05-21 15:30:11] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 214.0 (TID 140, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:30:11] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 214.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 214.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 214
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.scheduler.DAGScheduler]ResultStage 214 (start at SparkStreamingT.java:107) failed in 0.357 s
[ERROR]  [2018-05-21 15:30:11] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 214.0 failed 1 times, most recent failure: Lost task 0.0 in stage 214.0 (TID 140, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.scheduler.DAGScheduler]Got job 143 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 219 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 219 (Receiver 0 ParallelCollectionRDD[219] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_143 stored as values in memory (estimated size 34.1 KB, free 901.1 MB)
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_143_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.1 MB)
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_143_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.6 MB)
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.SparkContext]Created broadcast 143 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 219 (Receiver 0 ParallelCollectionRDD[219] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 219.0 with 1 tasks
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 219.0 (TID 143, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.executor.Executor]Running task 0.0 in stage 219.0 (TID 143)
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887811400
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[ERROR]  [2018-05-21 15:30:11] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:30:11] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887811600
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:30:11] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:30:11] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 219.0 (TID 143)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN]  [2018-05-21 15:30:11] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 219.0 (TID 143, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:30:11] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 219.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 219.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 219
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.scheduler.DAGScheduler]ResultStage 219 (start at SparkStreamingT.java:107) failed in 0.390 s
[ERROR]  [2018-05-21 15:30:11] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 219.0 failed 1 times, most recent failure: Lost task 0.0 in stage 219.0 (TID 143, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.scheduler.DAGScheduler]Got job 144 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 220 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 220 (Receiver 0 ParallelCollectionRDD[220] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_144 stored as values in memory (estimated size 34.1 KB, free 901.0 MB)
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_144_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.0 MB)
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_144_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.6 MB)
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.SparkContext]Created broadcast 144 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 220 (Receiver 0 ParallelCollectionRDD[220] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 220.0 with 1 tasks
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 220.0 (TID 144, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.executor.Executor]Running task 0.0 in stage 220.0 (TID 144)
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887811800
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[ERROR]  [2018-05-21 15:30:11] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:30:11] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:30:11] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887812000
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526887812000 ms
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526887812000 ms.0 from job set of time 1526887812000 ms
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:30:12] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:30:12] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 220.0 (TID 144)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 223 (mapToPair at SparkStreamingT.java:87)
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.scheduler.DAGScheduler]Got job 145 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 222 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 221)
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[WARN]  [2018-05-21 15:30:12] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 220.0 (TID 144, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:30:12] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 220.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 220.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 222 (ShuffledRDD[224] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_145 stored as values in memory (estimated size 3.8 KB, free 901.0 MB)
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_145_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.0 MB)
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_145_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.6 MB)
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.SparkContext]Created broadcast 145 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 222 (ShuffledRDD[224] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 222.0 with 1 tasks
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 220
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.scheduler.DAGScheduler]ResultStage 220 (start at SparkStreamingT.java:107) failed in 0.393 s
[ERROR]  [2018-05-21 15:30:12] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 220.0 failed 1 times, most recent failure: Lost task 0.0 in stage 220.0 (TID 144, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 222.0 (TID 145, localhost, partition 0, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.executor.Executor]Running task 0.0 in stage 222.0 (TID 145)
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.scheduler.DAGScheduler]Got job 146 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 223 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 223 (Receiver 0 ParallelCollectionRDD[225] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_146 stored as values in memory (estimated size 34.1 KB, free 901.0 MB)
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_146_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.0 MB)
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 222.0 (TID 145). 1549 bytes result sent to driver
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_146_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.6 MB)
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 222.0 (TID 145) in 20 ms on localhost (1/1)
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 222.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.SparkContext]Created broadcast 146 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 223 (Receiver 0 ParallelCollectionRDD[225] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 223.0 with 1 tasks
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.scheduler.DAGScheduler]ResultStage 222 (print at SparkStreamingT.java:101) finished in 0.022 s
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 223.0 (TID 146, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.scheduler.DAGScheduler]Job 145 finished: print at SparkStreamingT.java:101, took 0.035144 s
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 38 is 83 bytes
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.scheduler.DAGScheduler]Got job 147 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 225 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 224)
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 225 (ShuffledRDD[224] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_147 stored as values in memory (estimated size 3.8 KB, free 901.0 MB)
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.executor.Executor]Running task 0.0 in stage 223.0 (TID 146)
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_147_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.0 MB)
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_147_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.6 MB)
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.SparkContext]Created broadcast 147 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 225 (ShuffledRDD[224] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 225.0 with 1 tasks
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 225.0 (TID 147, localhost, partition 1, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.executor.Executor]Running task 0.0 in stage 225.0 (TID 147)
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 225.0 (TID 147). 1470 bytes result sent to driver
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887812200
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 225.0 (TID 147) in 5 ms on localhost (1/1)
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 225.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.scheduler.DAGScheduler]ResultStage 225 (print at SparkStreamingT.java:101) finished in 0.005 s
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.scheduler.DAGScheduler]Job 147 finished: print at SparkStreamingT.java:101, took 0.012321 s
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526887812000 ms.0 from job set of time 1526887812000 ms
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.067 s for time 1526887812000 ms (execution: 0.050 s)
[ERROR]  [2018-05-21 15:30:12] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:30:12] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 218 from persistence list
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.storage.BlockManager]Removing RDD 218
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 217 from persistence list
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.storage.BlockManager]Removing RDD 217
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 216 from persistence list
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.storage.BlockManager]Removing RDD 216
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.rdd.BlockRDD]Removing RDD 215 from persistence list
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.storage.BlockManager]Removing RDD 215
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.streaming.dstream.SocketInputDStream]Removing blocks of RDD BlockRDD[215] at socketTextStream at SparkStreamingT.java:72 of time 1526887812000 ms
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 1526887810000 ms
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1526887810000 ms
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887812400
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:30:12] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:30:12] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 223.0 (TID 146)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN]  [2018-05-21 15:30:12] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 223.0 (TID 146, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:30:12] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 223.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 223.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 223
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.scheduler.DAGScheduler]ResultStage 223 (start at SparkStreamingT.java:107) failed in 0.360 s
[ERROR]  [2018-05-21 15:30:12] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 223.0 failed 1 times, most recent failure: Lost task 0.0 in stage 223.0 (TID 146, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.scheduler.DAGScheduler]Got job 148 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 226 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 226 (Receiver 0 ParallelCollectionRDD[226] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_148 stored as values in memory (estimated size 34.1 KB, free 900.9 MB)
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_148_piece0 stored as bytes in memory (estimated size 11.4 KB, free 900.9 MB)
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_148_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.6 MB)
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.SparkContext]Created broadcast 148 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 226 (Receiver 0 ParallelCollectionRDD[226] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 226.0 with 1 tasks
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 226.0 (TID 148, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.executor.Executor]Running task 0.0 in stage 226.0 (TID 148)
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887812600
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[ERROR]  [2018-05-21 15:30:12] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:30:12] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887812800
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:30:12] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:30:12] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 226.0 (TID 148)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN]  [2018-05-21 15:30:12] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 226.0 (TID 148, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:30:12] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 226.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 226.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 226
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.scheduler.DAGScheduler]ResultStage 226 (start at SparkStreamingT.java:107) failed in 0.375 s
[ERROR]  [2018-05-21 15:30:12] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 226.0 failed 1 times, most recent failure: Lost task 0.0 in stage 226.0 (TID 148, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.scheduler.DAGScheduler]Got job 149 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 227 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 227 (Receiver 0 ParallelCollectionRDD[227] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_149 stored as values in memory (estimated size 34.1 KB, free 900.9 MB)
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_149_piece0 stored as bytes in memory (estimated size 11.4 KB, free 900.9 MB)
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_149_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.6 MB)
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.SparkContext]Created broadcast 149 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 227 (Receiver 0 ParallelCollectionRDD[227] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 227.0 with 1 tasks
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 227.0 (TID 149, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.executor.Executor]Running task 0.0 in stage 227.0 (TID 149)
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887813000
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[ERROR]  [2018-05-21 15:30:12] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:30:12] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.ContextCleaner]Cleaned shuffle 35
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.ContextCleaner]Cleaned shuffle 31
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.ContextCleaner]Cleaned shuffle 32
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_118_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.6 MB)
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_119_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.6 MB)
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_120_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.6 MB)
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_121_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.6 MB)
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_122_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.6 MB)
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.ContextCleaner]Cleaned shuffle 33
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_123_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.6 MB)
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_124_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.6 MB)
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_125_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.6 MB)
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_126_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.6 MB)
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_127_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.6 MB)
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.ContextCleaner]Cleaned shuffle 34
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_128_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.6 MB)
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_129_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.6 MB)
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_130_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.6 MB)
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_131_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_132_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_133_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_134_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_135_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.ContextCleaner]Cleaned shuffle 36
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_136_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_137_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_138_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_139_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_140_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.ContextCleaner]Cleaned shuffle 37
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_141_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_142_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_143_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_144_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_145_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_146_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_147_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:30:12] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_148_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526887813000 ms
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526887813000 ms.0 from job set of time 1526887813000 ms
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 230 (mapToPair at SparkStreamingT.java:87)
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.scheduler.DAGScheduler]Got job 150 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 229 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 228)
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 229 (ShuffledRDD[231] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_150 stored as values in memory (estimated size 3.8 KB, free 901.7 MB)
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_150_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.7 MB)
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_150_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.SparkContext]Created broadcast 150 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 229 (ShuffledRDD[231] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 229.0 with 1 tasks
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 229.0 (TID 150, localhost, partition 0, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.executor.Executor]Running task 0.0 in stage 229.0 (TID 150)
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 229.0 (TID 150). 1470 bytes result sent to driver
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 229.0 (TID 150) in 6 ms on localhost (1/1)
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 229.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.scheduler.DAGScheduler]ResultStage 229 (print at SparkStreamingT.java:101) finished in 0.008 s
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.scheduler.DAGScheduler]Job 150 finished: print at SparkStreamingT.java:101, took 0.024781 s
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 39 is 83 bytes
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.scheduler.DAGScheduler]Got job 151 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 231 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 230)
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 231 (ShuffledRDD[231] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_151 stored as values in memory (estimated size 3.8 KB, free 901.7 MB)
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_151_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.7 MB)
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_151_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.SparkContext]Created broadcast 151 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 231 (ShuffledRDD[231] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 231.0 with 1 tasks
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 231.0 (TID 151, localhost, partition 1, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.executor.Executor]Running task 0.0 in stage 231.0 (TID 151)
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 231.0 (TID 151). 1549 bytes result sent to driver
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 231.0 (TID 151) in 4 ms on localhost (1/1)
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 231.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.scheduler.DAGScheduler]ResultStage 231 (print at SparkStreamingT.java:101) finished in 0.004 s
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.scheduler.DAGScheduler]Job 151 finished: print at SparkStreamingT.java:101, took 0.015800 s
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526887813000 ms.0 from job set of time 1526887813000 ms
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.062 s for time 1526887813000 ms (execution: 0.051 s)
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 224 from persistence list
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 223 from persistence list
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.storage.BlockManager]Removing RDD 224
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.storage.BlockManager]Removing RDD 223
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 222 from persistence list
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.storage.BlockManager]Removing RDD 222
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.rdd.BlockRDD]Removing RDD 221 from persistence list
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.storage.BlockManager]Removing RDD 221
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.streaming.dstream.SocketInputDStream]Removing blocks of RDD BlockRDD[221] at socketTextStream at SparkStreamingT.java:72 of time 1526887813000 ms
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 1526887811000 ms
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1526887811000 ms
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887813200
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:30:13] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:30:13] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 227.0 (TID 149)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN]  [2018-05-21 15:30:13] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 227.0 (TID 149, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:30:13] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 227.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 227.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 227
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.scheduler.DAGScheduler]ResultStage 227 (start at SparkStreamingT.java:107) failed in 0.363 s
[ERROR]  [2018-05-21 15:30:13] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 227.0 failed 1 times, most recent failure: Lost task 0.0 in stage 227.0 (TID 149, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.scheduler.DAGScheduler]Got job 152 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 232 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 232 (Receiver 0 ParallelCollectionRDD[232] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_152 stored as values in memory (estimated size 34.1 KB, free 901.7 MB)
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_152_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.7 MB)
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_152_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.SparkContext]Created broadcast 152 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 232 (Receiver 0 ParallelCollectionRDD[232] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 232.0 with 1 tasks
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 232.0 (TID 152, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.executor.Executor]Running task 0.0 in stage 232.0 (TID 152)
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887813400
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[ERROR]  [2018-05-21 15:30:13] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:30:13] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887813600
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:30:13] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:30:13] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 232.0 (TID 152)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN]  [2018-05-21 15:30:13] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 232.0 (TID 152, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:30:13] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 232.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 232.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 232
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.scheduler.DAGScheduler]ResultStage 232 (start at SparkStreamingT.java:107) failed in 0.384 s
[ERROR]  [2018-05-21 15:30:13] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 232.0 failed 1 times, most recent failure: Lost task 0.0 in stage 232.0 (TID 152, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.scheduler.DAGScheduler]Got job 153 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 233 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 233 (Receiver 0 ParallelCollectionRDD[233] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_153 stored as values in memory (estimated size 34.1 KB, free 901.6 MB)
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_153_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.6 MB)
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_153_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.SparkContext]Created broadcast 153 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 233 (Receiver 0 ParallelCollectionRDD[233] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 233.0 with 1 tasks
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 233.0 (TID 153, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.executor.Executor]Running task 0.0 in stage 233.0 (TID 153)
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887813800
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[ERROR]  [2018-05-21 15:30:13] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:30:13] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:30:13] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526887814000 ms
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526887814000 ms.0 from job set of time 1526887814000 ms
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 236 (mapToPair at SparkStreamingT.java:87)
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.scheduler.DAGScheduler]Got job 154 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 235 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 234)
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 235 (ShuffledRDD[237] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_154 stored as values in memory (estimated size 3.8 KB, free 901.6 MB)
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_154_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.6 MB)
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_154_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.SparkContext]Created broadcast 154 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 235 (ShuffledRDD[237] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 235.0 with 1 tasks
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 235.0 (TID 154, localhost, partition 0, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.executor.Executor]Running task 0.0 in stage 235.0 (TID 154)
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 235.0 (TID 154). 1462 bytes result sent to driver
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 235.0 (TID 154) in 3 ms on localhost (1/1)
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 235.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887814000
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.scheduler.DAGScheduler]ResultStage 235 (print at SparkStreamingT.java:101) finished in 0.003 s
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.scheduler.DAGScheduler]Job 154 finished: print at SparkStreamingT.java:101, took 0.029781 s
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 40 is 83 bytes
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.scheduler.DAGScheduler]Got job 155 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 237 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 236)
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 237 (ShuffledRDD[237] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_155 stored as values in memory (estimated size 3.8 KB, free 901.6 MB)
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_155_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.6 MB)
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_155_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.SparkContext]Created broadcast 155 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 237 (ShuffledRDD[237] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 237.0 with 1 tasks
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:30:14] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:30:14] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 233.0 (TID 153)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 237.0 (TID 155, localhost, partition 1, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.executor.Executor]Running task 0.0 in stage 237.0 (TID 155)
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 237.0 (TID 155). 1462 bytes result sent to driver
[WARN]  [2018-05-21 15:30:14] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 233.0 (TID 153, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:30:14] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 233.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 233.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 233
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.scheduler.DAGScheduler]ResultStage 233 (start at SparkStreamingT.java:107) failed in 0.431 s
[ERROR]  [2018-05-21 15:30:14] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 233.0 failed 1 times, most recent failure: Lost task 0.0 in stage 233.0 (TID 153, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.scheduler.DAGScheduler]ResultStage 237 (print at SparkStreamingT.java:101) finished in 0.008 s
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 237.0 (TID 155) in 5 ms on localhost (1/1)
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 237.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.scheduler.DAGScheduler]Job 155 finished: print at SparkStreamingT.java:101, took 0.019964 s
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526887814000 ms.0 from job set of time 1526887814000 ms
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.074 s for time 1526887814000 ms (execution: 0.057 s)
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.scheduler.DAGScheduler]Got job 156 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 238 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 238 (Receiver 0 ParallelCollectionRDD[238] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_156 stored as values in memory (estimated size 34.1 KB, free 901.6 MB)
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_156_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.6 MB)
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 231 from persistence list
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_156_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.SparkContext]Created broadcast 156 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 238 (Receiver 0 ParallelCollectionRDD[238] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 238.0 with 1 tasks
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 230 from persistence list
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 238.0 (TID 156, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.storage.BlockManager]Removing RDD 230
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.executor.Executor]Running task 0.0 in stage 238.0 (TID 156)
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 229 from persistence list
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.storage.BlockManager]Removing RDD 229
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.rdd.BlockRDD]Removing RDD 228 from persistence list
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887814200
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.storage.BlockManager]Removing RDD 231
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.streaming.dstream.SocketInputDStream]Removing blocks of RDD BlockRDD[228] at socketTextStream at SparkStreamingT.java:72 of time 1526887814000 ms
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 1526887812000 ms
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1526887812000 ms
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[ERROR]  [2018-05-21 15:30:14] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:30:14] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.storage.BlockManager]Removing RDD 228
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887814200
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:30:14] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:30:14] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 238.0 (TID 156)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN]  [2018-05-21 15:30:14] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 238.0 (TID 156, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:30:14] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 238.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 238.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 238
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.scheduler.DAGScheduler]ResultStage 238 (start at SparkStreamingT.java:107) failed in 0.108 s
[ERROR]  [2018-05-21 15:30:14] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 238.0 failed 1 times, most recent failure: Lost task 0.0 in stage 238.0 (TID 156, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.scheduler.DAGScheduler]Got job 157 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 239 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 239 (Receiver 0 ParallelCollectionRDD[239] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_157 stored as values in memory (estimated size 34.1 KB, free 901.5 MB)
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_157_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.5 MB)
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_157_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.SparkContext]Created broadcast 157 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 239 (Receiver 0 ParallelCollectionRDD[239] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 239.0 with 1 tasks
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 239.0 (TID 157, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.executor.Executor]Running task 0.0 in stage 239.0 (TID 157)
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887814400
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[ERROR]  [2018-05-21 15:30:14] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:30:14] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887814600
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:30:14] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:30:14] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 239.0 (TID 157)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN]  [2018-05-21 15:30:14] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 239.0 (TID 157, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:30:14] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 239.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 239.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 239
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.scheduler.DAGScheduler]ResultStage 239 (start at SparkStreamingT.java:107) failed in 0.385 s
[ERROR]  [2018-05-21 15:30:14] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 239.0 failed 1 times, most recent failure: Lost task 0.0 in stage 239.0 (TID 157, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.scheduler.DAGScheduler]Got job 158 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 240 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 240 (Receiver 0 ParallelCollectionRDD[240] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_158 stored as values in memory (estimated size 34.1 KB, free 901.5 MB)
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_158_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.5 MB)
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_158_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.SparkContext]Created broadcast 158 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 240 (Receiver 0 ParallelCollectionRDD[240] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 240.0 with 1 tasks
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 240.0 (TID 158, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.executor.Executor]Running task 0.0 in stage 240.0 (TID 158)
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887814800
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[ERROR]  [2018-05-21 15:30:14] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:30:14] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:30:14] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887815000
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526887815000 ms
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526887815000 ms.0 from job set of time 1526887815000 ms
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:30:15] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:30:15] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 240.0 (TID 158)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN]  [2018-05-21 15:30:15] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 240.0 (TID 158, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:30:15] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 240.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 240.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 240
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.scheduler.DAGScheduler]ResultStage 240 (start at SparkStreamingT.java:107) failed in 0.360 s
[ERROR]  [2018-05-21 15:30:15] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 240.0 failed 1 times, most recent failure: Lost task 0.0 in stage 240.0 (TID 158, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 243 (mapToPair at SparkStreamingT.java:87)
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.scheduler.DAGScheduler]Got job 159 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 242 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 241)
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 242 (ShuffledRDD[244] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_159 stored as values in memory (estimated size 3.8 KB, free 901.5 MB)
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_159_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.5 MB)
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_159_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.SparkContext]Created broadcast 159 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 242 (ShuffledRDD[244] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 242.0 with 1 tasks
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 242.0 (TID 159, localhost, partition 0, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.executor.Executor]Running task 0.0 in stage 242.0 (TID 159)
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 242.0 (TID 159). 1470 bytes result sent to driver
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.scheduler.DAGScheduler]ResultStage 242 (print at SparkStreamingT.java:101) finished in 0.007 s
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.scheduler.DAGScheduler]Got job 160 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 243 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 243 (Receiver 0 ParallelCollectionRDD[245] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 242.0 (TID 159) in 5 ms on localhost (1/1)
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 242.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.scheduler.DAGScheduler]Job 159 finished: print at SparkStreamingT.java:101, took 0.018892 s
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_160 stored as values in memory (estimated size 34.1 KB, free 901.4 MB)
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_160_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.4 MB)
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_160_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.SparkContext]Created broadcast 160 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 243 (Receiver 0 ParallelCollectionRDD[245] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 243.0 with 1 tasks
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 41 is 83 bytes
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 243.0 (TID 160, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.scheduler.DAGScheduler]Got job 161 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 245 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 244)
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.executor.Executor]Running task 0.0 in stage 243.0 (TID 160)
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 245 (ShuffledRDD[244] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_161 stored as values in memory (estimated size 3.8 KB, free 901.4 MB)
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_161_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.4 MB)
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_161_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.SparkContext]Created broadcast 161 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 245 (ShuffledRDD[244] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 245.0 with 1 tasks
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887815200
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 245.0 (TID 161, localhost, partition 1, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.executor.Executor]Running task 0.0 in stage 245.0 (TID 161)
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 245.0 (TID 161). 1383 bytes result sent to driver
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 245.0 (TID 161) in 5 ms on localhost (1/1)
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.scheduler.DAGScheduler]ResultStage 245 (print at SparkStreamingT.java:101) finished in 0.005 s
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 245.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.scheduler.DAGScheduler]Job 161 finished: print at SparkStreamingT.java:101, took 0.010468 s
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526887815000 ms.0 from job set of time 1526887815000 ms
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.049 s for time 1526887815000 ms (execution: 0.040 s)
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 237 from persistence list
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.storage.BlockManager]Removing RDD 237
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 236 from persistence list
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.storage.BlockManager]Removing RDD 236
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 235 from persistence list
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.storage.BlockManager]Removing RDD 235
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.rdd.BlockRDD]Removing RDD 234 from persistence list
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.storage.BlockManager]Removing RDD 234
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.streaming.dstream.SocketInputDStream]Removing blocks of RDD BlockRDD[234] at socketTextStream at SparkStreamingT.java:72 of time 1526887815000 ms
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 1526887813000 ms
[INFO]  [2018-05-21 15:30:15] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1526887813000 ms
[INFO]  [2018-05-21 15:30:16] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526887816000 ms
[INFO]  [2018-05-21 15:30:16] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526887816000 ms.0 from job set of time 1526887816000 ms
[INFO]  [2018-05-21 15:30:16] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:30:16] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 248 (mapToPair at SparkStreamingT.java:87)
[INFO]  [2018-05-21 15:30:16] [org.apache.spark.scheduler.DAGScheduler]Got job 162 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:30:16] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 247 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:30:16] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 246)
[INFO]  [2018-05-21 15:30:16] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:16] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 247 (ShuffledRDD[249] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:30:16] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_162 stored as values in memory (estimated size 3.8 KB, free 901.4 MB)
[INFO]  [2018-05-21 15:30:16] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_162_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.4 MB)
[INFO]  [2018-05-21 15:30:16] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_162_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:16] [org.apache.spark.SparkContext]Created broadcast 162 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:16] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 247 (ShuffledRDD[249] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:30:16] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 247.0 with 1 tasks
[INFO]  [2018-05-21 15:30:16] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 247.0 (TID 162, localhost, partition 0, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:30:16] [org.apache.spark.executor.Executor]Running task 0.0 in stage 247.0 (TID 162)
[INFO]  [2018-05-21 15:30:16] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:30:16] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO]  [2018-05-21 15:30:16] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 247.0 (TID 162). 1549 bytes result sent to driver
[INFO]  [2018-05-21 15:30:16] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 247.0 (TID 162) in 3 ms on localhost (1/1)
[INFO]  [2018-05-21 15:30:16] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 247.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:16] [org.apache.spark.scheduler.DAGScheduler]ResultStage 247 (print at SparkStreamingT.java:101) finished in 0.003 s
[INFO]  [2018-05-21 15:30:16] [org.apache.spark.scheduler.DAGScheduler]Job 162 finished: print at SparkStreamingT.java:101, took 0.010515 s
[INFO]  [2018-05-21 15:30:16] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:30:16] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 42 is 83 bytes
[INFO]  [2018-05-21 15:30:16] [org.apache.spark.scheduler.DAGScheduler]Got job 163 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:30:16] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 249 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:30:16] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 248)
[INFO]  [2018-05-21 15:30:16] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:16] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 249 (ShuffledRDD[249] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:30:16] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_163 stored as values in memory (estimated size 3.8 KB, free 901.4 MB)
[INFO]  [2018-05-21 15:30:16] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_163_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.4 MB)
[INFO]  [2018-05-21 15:30:16] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_163_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:16] [org.apache.spark.SparkContext]Created broadcast 163 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:16] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 249 (ShuffledRDD[249] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:30:16] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 249.0 with 1 tasks
[INFO]  [2018-05-21 15:30:16] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 249.0 (TID 163, localhost, partition 1, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:30:16] [org.apache.spark.executor.Executor]Running task 0.0 in stage 249.0 (TID 163)
[INFO]  [2018-05-21 15:30:16] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:30:16] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:30:16] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 249.0 (TID 163). 1549 bytes result sent to driver
[INFO]  [2018-05-21 15:30:16] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 249.0 (TID 163) in 3 ms on localhost (1/1)
[INFO]  [2018-05-21 15:30:16] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 249.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:16] [org.apache.spark.scheduler.DAGScheduler]ResultStage 249 (print at SparkStreamingT.java:101) finished in 0.005 s
[INFO]  [2018-05-21 15:30:16] [org.apache.spark.scheduler.DAGScheduler]Job 163 finished: print at SparkStreamingT.java:101, took 0.011293 s
[INFO]  [2018-05-21 15:30:16] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526887816000 ms.0 from job set of time 1526887816000 ms
[INFO]  [2018-05-21 15:30:16] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.048 s for time 1526887816000 ms (execution: 0.036 s)
[INFO]  [2018-05-21 15:30:16] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 244 from persistence list
[INFO]  [2018-05-21 15:30:16] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 243 from persistence list
[INFO]  [2018-05-21 15:30:16] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 242 from persistence list
[INFO]  [2018-05-21 15:30:16] [org.apache.spark.storage.BlockManager]Removing RDD 244
[INFO]  [2018-05-21 15:30:16] [org.apache.spark.rdd.BlockRDD]Removing RDD 241 from persistence list
[INFO]  [2018-05-21 15:30:16] [org.apache.spark.storage.BlockManager]Removing RDD 242
[INFO]  [2018-05-21 15:30:16] [org.apache.spark.storage.BlockManager]Removing RDD 243
[INFO]  [2018-05-21 15:30:16] [org.apache.spark.streaming.dstream.SocketInputDStream]Removing blocks of RDD BlockRDD[241] at socketTextStream at SparkStreamingT.java:72 of time 1526887816000 ms
[INFO]  [2018-05-21 15:30:16] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 1526887814000 ms
[INFO]  [2018-05-21 15:30:16] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1526887814000 ms
[INFO]  [2018-05-21 15:30:16] [org.apache.spark.storage.BlockManager]Removing RDD 241
[INFO]  [2018-05-21 15:30:17] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526887817000 ms
[INFO]  [2018-05-21 15:30:17] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526887817000 ms.0 from job set of time 1526887817000 ms
[INFO]  [2018-05-21 15:30:17] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:30:17] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 252 (mapToPair at SparkStreamingT.java:87)
[INFO]  [2018-05-21 15:30:17] [org.apache.spark.scheduler.DAGScheduler]Got job 164 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:30:17] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 251 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:30:17] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 250)
[INFO]  [2018-05-21 15:30:17] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:17] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 251 (ShuffledRDD[253] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:30:17] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_164 stored as values in memory (estimated size 3.8 KB, free 901.4 MB)
[INFO]  [2018-05-21 15:30:17] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_164_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.4 MB)
[INFO]  [2018-05-21 15:30:17] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_164_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:17] [org.apache.spark.SparkContext]Created broadcast 164 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:17] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 251 (ShuffledRDD[253] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:30:17] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 251.0 with 1 tasks
[INFO]  [2018-05-21 15:30:17] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 251.0 (TID 164, localhost, partition 0, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:30:17] [org.apache.spark.executor.Executor]Running task 0.0 in stage 251.0 (TID 164)
[INFO]  [2018-05-21 15:30:17] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:30:17] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO]  [2018-05-21 15:30:17] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 251.0 (TID 164). 1462 bytes result sent to driver
[INFO]  [2018-05-21 15:30:17] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 251.0 (TID 164) in 4 ms on localhost (1/1)
[INFO]  [2018-05-21 15:30:17] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 251.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:17] [org.apache.spark.scheduler.DAGScheduler]ResultStage 251 (print at SparkStreamingT.java:101) finished in 0.004 s
[INFO]  [2018-05-21 15:30:17] [org.apache.spark.scheduler.DAGScheduler]Job 164 finished: print at SparkStreamingT.java:101, took 0.017810 s
[INFO]  [2018-05-21 15:30:17] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:30:17] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 43 is 83 bytes
[INFO]  [2018-05-21 15:30:17] [org.apache.spark.scheduler.DAGScheduler]Got job 165 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:30:17] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 253 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:30:17] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 252)
[INFO]  [2018-05-21 15:30:17] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:17] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 253 (ShuffledRDD[253] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:30:17] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_165 stored as values in memory (estimated size 3.8 KB, free 901.4 MB)
[INFO]  [2018-05-21 15:30:17] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_165_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.4 MB)
[INFO]  [2018-05-21 15:30:17] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_165_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:17] [org.apache.spark.SparkContext]Created broadcast 165 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:17] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 253 (ShuffledRDD[253] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:30:17] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 253.0 with 1 tasks
[INFO]  [2018-05-21 15:30:17] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 253.0 (TID 165, localhost, partition 1, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:30:17] [org.apache.spark.executor.Executor]Running task 0.0 in stage 253.0 (TID 165)
[INFO]  [2018-05-21 15:30:17] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:30:17] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:30:17] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 253.0 (TID 165). 1470 bytes result sent to driver
[INFO]  [2018-05-21 15:30:17] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 253.0 (TID 165) in 3 ms on localhost (1/1)
[INFO]  [2018-05-21 15:30:17] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 253.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:17] [org.apache.spark.scheduler.DAGScheduler]ResultStage 253 (print at SparkStreamingT.java:101) finished in 0.003 s
[INFO]  [2018-05-21 15:30:17] [org.apache.spark.scheduler.DAGScheduler]Job 165 finished: print at SparkStreamingT.java:101, took 0.011199 s
[INFO]  [2018-05-21 15:30:17] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526887817000 ms.0 from job set of time 1526887817000 ms
[INFO]  [2018-05-21 15:30:17] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.050 s for time 1526887817000 ms (execution: 0.040 s)
[INFO]  [2018-05-21 15:30:17] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 249 from persistence list
[INFO]  [2018-05-21 15:30:17] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 248 from persistence list
[INFO]  [2018-05-21 15:30:17] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 247 from persistence list
[INFO]  [2018-05-21 15:30:17] [org.apache.spark.storage.BlockManager]Removing RDD 249
[INFO]  [2018-05-21 15:30:17] [org.apache.spark.storage.BlockManager]Removing RDD 248
[INFO]  [2018-05-21 15:30:17] [org.apache.spark.rdd.BlockRDD]Removing RDD 246 from persistence list
[INFO]  [2018-05-21 15:30:17] [org.apache.spark.storage.BlockManager]Removing RDD 247
[INFO]  [2018-05-21 15:30:17] [org.apache.spark.streaming.dstream.SocketInputDStream]Removing blocks of RDD BlockRDD[246] at socketTextStream at SparkStreamingT.java:72 of time 1526887817000 ms
[INFO]  [2018-05-21 15:30:17] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 1526887815000 ms
[INFO]  [2018-05-21 15:30:17] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1526887815000 ms
[INFO]  [2018-05-21 15:30:17] [org.apache.spark.storage.BlockManager]Removing RDD 246
[INFO]  [2018-05-21 15:30:17] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:30:17] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:30:17] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[ERROR]  [2018-05-21 15:30:17] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:30:17] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:30:17] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887818000
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:30:18] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:30:18] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 243.0 (TID 160)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526887818000 ms
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526887818000 ms.0 from job set of time 1526887818000 ms
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 256 (mapToPair at SparkStreamingT.java:87)
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.scheduler.DAGScheduler]Got job 166 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 255 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 254)
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 255 (ShuffledRDD[257] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_166 stored as values in memory (estimated size 3.8 KB, free 901.4 MB)
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_166_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.4 MB)
[WARN]  [2018-05-21 15:30:18] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 243.0 (TID 160, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:30:18] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 243.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 243.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_166_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.SparkContext]Created broadcast 166 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 255 (ShuffledRDD[257] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 255.0 with 1 tasks
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 243
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.scheduler.DAGScheduler]ResultStage 243 (start at SparkStreamingT.java:107) failed in 2.988 s
[ERROR]  [2018-05-21 15:30:18] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 243.0 failed 1 times, most recent failure: Lost task 0.0 in stage 243.0 (TID 160, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 255.0 (TID 166, localhost, partition 0, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.executor.Executor]Running task 0.0 in stage 255.0 (TID 166)
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 255.0 (TID 166). 1470 bytes result sent to driver
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.scheduler.DAGScheduler]ResultStage 255 (print at SparkStreamingT.java:101) finished in 0.015 s
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 255.0 (TID 166) in 8 ms on localhost (1/1)
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 255.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.scheduler.DAGScheduler]Job 166 finished: print at SparkStreamingT.java:101, took 0.027617 s
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.scheduler.DAGScheduler]Got job 167 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 256 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 256 (Receiver 0 ParallelCollectionRDD[258] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_167 stored as values in memory (estimated size 34.1 KB, free 901.3 MB)
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_167_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.3 MB)
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_167_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.SparkContext]Created broadcast 167 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 256 (Receiver 0 ParallelCollectionRDD[258] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 256.0 with 1 tasks
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 44 is 83 bytes
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 256.0 (TID 167, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.scheduler.DAGScheduler]Got job 168 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.executor.Executor]Running task 0.0 in stage 256.0 (TID 167)
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 258 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 257)
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 258 (ShuffledRDD[257] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_168 stored as values in memory (estimated size 3.8 KB, free 901.3 MB)
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_168_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.3 MB)
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_168_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.SparkContext]Created broadcast 168 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 258 (ShuffledRDD[257] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 258.0 with 1 tasks
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 258.0 (TID 168, localhost, partition 1, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887818200
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.executor.Executor]Running task 0.0 in stage 258.0 (TID 168)
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 258.0 (TID 168). 1470 bytes result sent to driver
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[ERROR]  [2018-05-21 15:30:18] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:30:18] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 258.0 (TID 168) in 5 ms on localhost (1/1)
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 258.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.scheduler.DAGScheduler]ResultStage 258 (print at SparkStreamingT.java:101) finished in 0.005 s
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.scheduler.DAGScheduler]Job 168 finished: print at SparkStreamingT.java:101, took 0.013820 s
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526887818000 ms.0 from job set of time 1526887818000 ms
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.062 s for time 1526887818000 ms (execution: 0.050 s)
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 253 from persistence list
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.storage.BlockManager]Removing RDD 253
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 252 from persistence list
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.storage.BlockManager]Removing RDD 252
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 251 from persistence list
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.storage.BlockManager]Removing RDD 251
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.rdd.BlockRDD]Removing RDD 250 from persistence list
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.storage.BlockManager]Removing RDD 250
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.streaming.dstream.SocketInputDStream]Removing blocks of RDD BlockRDD[250] at socketTextStream at SparkStreamingT.java:72 of time 1526887818000 ms
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 1526887816000 ms
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1526887816000 ms
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887818400
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:30:18] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:30:18] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 256.0 (TID 167)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN]  [2018-05-21 15:30:18] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 256.0 (TID 167, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:30:18] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 256.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 256.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 256
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.scheduler.DAGScheduler]ResultStage 256 (start at SparkStreamingT.java:107) failed in 0.352 s
[ERROR]  [2018-05-21 15:30:18] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 256.0 failed 1 times, most recent failure: Lost task 0.0 in stage 256.0 (TID 167, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.scheduler.DAGScheduler]Got job 169 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 259 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 259 (Receiver 0 ParallelCollectionRDD[259] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_169 stored as values in memory (estimated size 34.1 KB, free 901.3 MB)
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_169_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.3 MB)
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_169_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.SparkContext]Created broadcast 169 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 259 (Receiver 0 ParallelCollectionRDD[259] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 259.0 with 1 tasks
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 259.0 (TID 169, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.executor.Executor]Running task 0.0 in stage 259.0 (TID 169)
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887818600
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[ERROR]  [2018-05-21 15:30:18] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:30:18] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887818800
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:30:18] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:30:18] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 259.0 (TID 169)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN]  [2018-05-21 15:30:18] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 259.0 (TID 169, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:30:18] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 259.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 259.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 259
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.scheduler.DAGScheduler]ResultStage 259 (start at SparkStreamingT.java:107) failed in 0.372 s
[ERROR]  [2018-05-21 15:30:18] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 259.0 failed 1 times, most recent failure: Lost task 0.0 in stage 259.0 (TID 169, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.scheduler.DAGScheduler]Got job 170 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 260 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 260 (Receiver 0 ParallelCollectionRDD[260] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_170 stored as values in memory (estimated size 34.1 KB, free 901.3 MB)
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_170_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.2 MB)
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_170_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.SparkContext]Created broadcast 170 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 260 (Receiver 0 ParallelCollectionRDD[260] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 260.0 with 1 tasks
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 260.0 (TID 170, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.executor.Executor]Running task 0.0 in stage 260.0 (TID 170)
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887819000
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[ERROR]  [2018-05-21 15:30:18] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:30:18] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:30:18] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887819000
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526887819000 ms
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526887819000 ms.0 from job set of time 1526887819000 ms
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 263 (mapToPair at SparkStreamingT.java:87)
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.scheduler.DAGScheduler]Got job 171 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 262 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 261)
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 262 (ShuffledRDD[264] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_171 stored as values in memory (estimated size 3.8 KB, free 901.2 MB)
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_171_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.2 MB)
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:30:19] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:30:19] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 260.0 (TID 170)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_171_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.6 MB)
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.SparkContext]Created broadcast 171 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 262 (ShuffledRDD[264] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 262.0 with 1 tasks
[WARN]  [2018-05-21 15:30:19] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 260.0 (TID 170, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:30:19] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 260.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 260.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 260
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.scheduler.DAGScheduler]ResultStage 260 (start at SparkStreamingT.java:107) failed in 0.212 s
[ERROR]  [2018-05-21 15:30:19] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 260.0 failed 1 times, most recent failure: Lost task 0.0 in stage 260.0 (TID 170, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 262.0 (TID 171, localhost, partition 0, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.executor.Executor]Running task 0.0 in stage 262.0 (TID 171)
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 262.0 (TID 171). 1470 bytes result sent to driver
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 262.0 (TID 171) in 5 ms on localhost (1/1)
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 262.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.scheduler.DAGScheduler]ResultStage 262 (print at SparkStreamingT.java:101) finished in 0.007 s
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.scheduler.DAGScheduler]Job 171 finished: print at SparkStreamingT.java:101, took 0.022739 s
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.scheduler.DAGScheduler]Got job 172 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 263 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 263 (Receiver 0 ParallelCollectionRDD[265] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_172 stored as values in memory (estimated size 34.1 KB, free 901.2 MB)
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_172_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.2 MB)
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_172_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.6 MB)
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.SparkContext]Created broadcast 172 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 263 (Receiver 0 ParallelCollectionRDD[265] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 263.0 with 1 tasks
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 45 is 83 bytes
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 263.0 (TID 172, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.executor.Executor]Running task 0.0 in stage 263.0 (TID 172)
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.scheduler.DAGScheduler]Got job 173 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 265 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 264)
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 265 (ShuffledRDD[264] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_173 stored as values in memory (estimated size 3.8 KB, free 901.2 MB)
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_173_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.2 MB)
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_173_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.6 MB)
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887819200
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.SparkContext]Created broadcast 173 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 265 (ShuffledRDD[264] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 265.0 with 1 tasks
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 265.0 (TID 173, localhost, partition 1, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.executor.Executor]Running task 0.0 in stage 265.0 (TID 173)
[ERROR]  [2018-05-21 15:30:19] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:30:19] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 265.0 (TID 173). 1383 bytes result sent to driver
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 265.0 (TID 173) in 3 ms on localhost (1/1)
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 265.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.scheduler.DAGScheduler]ResultStage 265 (print at SparkStreamingT.java:101) finished in 0.005 s
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.scheduler.DAGScheduler]Job 173 finished: print at SparkStreamingT.java:101, took 0.025406 s
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526887819000 ms.0 from job set of time 1526887819000 ms
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.069 s for time 1526887819000 ms (execution: 0.057 s)
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 257 from persistence list
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.storage.BlockManager]Removing RDD 257
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 256 from persistence list
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.storage.BlockManager]Removing RDD 256
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 255 from persistence list
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.storage.BlockManager]Removing RDD 255
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.rdd.BlockRDD]Removing RDD 254 from persistence list
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.storage.BlockManager]Removing RDD 254
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.streaming.dstream.SocketInputDStream]Removing blocks of RDD BlockRDD[254] at socketTextStream at SparkStreamingT.java:72 of time 1526887819000 ms
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 1526887817000 ms
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1526887817000 ms
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887819400
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:30:19] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:30:19] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 263.0 (TID 172)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN]  [2018-05-21 15:30:19] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 263.0 (TID 172, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:30:19] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 263.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 263.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 263
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.scheduler.DAGScheduler]ResultStage 263 (start at SparkStreamingT.java:107) failed in 0.350 s
[ERROR]  [2018-05-21 15:30:19] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 263.0 failed 1 times, most recent failure: Lost task 0.0 in stage 263.0 (TID 172, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.scheduler.DAGScheduler]Got job 174 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 266 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 266 (Receiver 0 ParallelCollectionRDD[266] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_174 stored as values in memory (estimated size 34.1 KB, free 901.2 MB)
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_174_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.1 MB)
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_174_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.6 MB)
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.SparkContext]Created broadcast 174 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 266 (Receiver 0 ParallelCollectionRDD[266] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 266.0 with 1 tasks
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 266.0 (TID 174, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.executor.Executor]Running task 0.0 in stage 266.0 (TID 174)
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887819600
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[ERROR]  [2018-05-21 15:30:19] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:30:19] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887819600
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:30:19] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:30:19] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 266.0 (TID 174)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN]  [2018-05-21 15:30:19] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 266.0 (TID 174, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:30:19] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 266.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 266.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 266
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.scheduler.DAGScheduler]ResultStage 266 (start at SparkStreamingT.java:107) failed in 0.175 s
[ERROR]  [2018-05-21 15:30:19] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 266.0 failed 1 times, most recent failure: Lost task 0.0 in stage 266.0 (TID 174, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.scheduler.DAGScheduler]Got job 175 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 267 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 267 (Receiver 0 ParallelCollectionRDD[267] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_175 stored as values in memory (estimated size 34.1 KB, free 901.1 MB)
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_175_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.1 MB)
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_175_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.6 MB)
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.SparkContext]Created broadcast 175 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 267 (Receiver 0 ParallelCollectionRDD[267] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 267.0 with 1 tasks
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 267.0 (TID 175, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.executor.Executor]Running task 0.0 in stage 267.0 (TID 175)
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887819800
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[ERROR]  [2018-05-21 15:30:19] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:30:19] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:30:19] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887820000
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:30:20] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:30:20] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 267.0 (TID 175)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN]  [2018-05-21 15:30:20] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 267.0 (TID 175, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:30:20] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 267.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 267.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 267
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.scheduler.DAGScheduler]ResultStage 267 (start at SparkStreamingT.java:107) failed in 0.360 s
[ERROR]  [2018-05-21 15:30:20] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 267.0 failed 1 times, most recent failure: Lost task 0.0 in stage 267.0 (TID 175, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526887820000 ms
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526887820000 ms.0 from job set of time 1526887820000 ms
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 270 (mapToPair at SparkStreamingT.java:87)
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.scheduler.DAGScheduler]Got job 176 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 269 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 268)
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 269 (ShuffledRDD[271] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_176 stored as values in memory (estimated size 3.8 KB, free 901.1 MB)
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_176_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.1 MB)
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_176_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.6 MB)
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.SparkContext]Created broadcast 176 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 269 (ShuffledRDD[271] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 269.0 with 1 tasks
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.scheduler.DAGScheduler]Got job 177 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 270 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 270 (Receiver 0 ParallelCollectionRDD[272] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 269.0 (TID 176, localhost, partition 0, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.executor.Executor]Running task 0.0 in stage 269.0 (TID 176)
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_177 stored as values in memory (estimated size 34.1 KB, free 901.1 MB)
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_177_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.0 MB)
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 269.0 (TID 176). 1462 bytes result sent to driver
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 269.0 (TID 176) in 3 ms on localhost (1/1)
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 269.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_177_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.6 MB)
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.SparkContext]Created broadcast 177 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 270 (Receiver 0 ParallelCollectionRDD[272] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 270.0 with 1 tasks
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.scheduler.DAGScheduler]ResultStage 269 (print at SparkStreamingT.java:101) finished in 0.015 s
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 270.0 (TID 177, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.scheduler.DAGScheduler]Job 176 finished: print at SparkStreamingT.java:101, took 0.023098 s
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.executor.Executor]Running task 0.0 in stage 270.0 (TID 177)
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887820200
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 46 is 83 bytes
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.scheduler.DAGScheduler]Got job 178 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 272 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 271)
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 272 (ShuffledRDD[271] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_178 stored as values in memory (estimated size 3.8 KB, free 901.0 MB)
[ERROR]  [2018-05-21 15:30:20] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:30:20] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_178_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.0 MB)
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_178_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.6 MB)
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.SparkContext]Created broadcast 178 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 272 (ShuffledRDD[271] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 272.0 with 1 tasks
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 272.0 (TID 178, localhost, partition 1, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.executor.Executor]Running task 0.0 in stage 272.0 (TID 178)
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 272.0 (TID 178). 1462 bytes result sent to driver
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 272.0 (TID 178) in 5 ms on localhost (1/1)
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 272.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.scheduler.DAGScheduler]ResultStage 272 (print at SparkStreamingT.java:101) finished in 0.006 s
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.scheduler.DAGScheduler]Job 178 finished: print at SparkStreamingT.java:101, took 0.009430 s
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526887820000 ms.0 from job set of time 1526887820000 ms
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.066 s for time 1526887820000 ms (execution: 0.044 s)
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 264 from persistence list
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 263 from persistence list
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.storage.BlockManager]Removing RDD 264
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 262 from persistence list
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.storage.BlockManager]Removing RDD 263
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.rdd.BlockRDD]Removing RDD 261 from persistence list
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.storage.BlockManager]Removing RDD 262
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.streaming.dstream.SocketInputDStream]Removing blocks of RDD BlockRDD[261] at socketTextStream at SparkStreamingT.java:72 of time 1526887820000 ms
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 1526887818000 ms
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1526887818000 ms
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.storage.BlockManager]Removing RDD 261
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887820400
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:30:20] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:30:20] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 270.0 (TID 177)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN]  [2018-05-21 15:30:20] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 270.0 (TID 177, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:30:20] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 270.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 270.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 270
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.scheduler.DAGScheduler]ResultStage 270 (start at SparkStreamingT.java:107) failed in 0.358 s
[ERROR]  [2018-05-21 15:30:20] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 270.0 failed 1 times, most recent failure: Lost task 0.0 in stage 270.0 (TID 177, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.scheduler.DAGScheduler]Got job 179 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 273 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 273 (Receiver 0 ParallelCollectionRDD[273] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_179 stored as values in memory (estimated size 34.1 KB, free 901.0 MB)
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_179_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.0 MB)
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_179_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.6 MB)
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.SparkContext]Created broadcast 179 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 273 (Receiver 0 ParallelCollectionRDD[273] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 273.0 with 1 tasks
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 273.0 (TID 179, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.executor.Executor]Running task 0.0 in stage 273.0 (TID 179)
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887820600
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[ERROR]  [2018-05-21 15:30:20] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:30:20] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887820800
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:30:20] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:30:20] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 273.0 (TID 179)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN]  [2018-05-21 15:30:20] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 273.0 (TID 179, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:30:20] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 273.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 273.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 273
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.scheduler.DAGScheduler]ResultStage 273 (start at SparkStreamingT.java:107) failed in 0.383 s
[ERROR]  [2018-05-21 15:30:20] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 273.0 failed 1 times, most recent failure: Lost task 0.0 in stage 273.0 (TID 179, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.scheduler.DAGScheduler]Got job 180 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 274 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 274 (Receiver 0 ParallelCollectionRDD[274] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_180 stored as values in memory (estimated size 34.1 KB, free 901.0 MB)
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_180_piece0 stored as bytes in memory (estimated size 11.4 KB, free 900.9 MB)
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_180_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.6 MB)
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.SparkContext]Created broadcast 180 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 274 (Receiver 0 ParallelCollectionRDD[274] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 274.0 with 1 tasks
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 274.0 (TID 180, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.executor.Executor]Running task 0.0 in stage 274.0 (TID 180)
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887821000
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[ERROR]  [2018-05-21 15:30:20] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:30:20] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:30:20] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526887821000 ms
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526887821000 ms.0 from job set of time 1526887821000 ms
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 277 (mapToPair at SparkStreamingT.java:87)
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.scheduler.DAGScheduler]Got job 181 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 276 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 275)
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 276 (ShuffledRDD[278] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_181 stored as values in memory (estimated size 3.8 KB, free 900.9 MB)
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_181_piece0 stored as bytes in memory (estimated size 2.2 KB, free 900.9 MB)
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_181_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.6 MB)
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.SparkContext]Created broadcast 181 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 276 (ShuffledRDD[278] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 276.0 with 1 tasks
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 276.0 (TID 181, localhost, partition 0, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.executor.Executor]Running task 0.0 in stage 276.0 (TID 181)
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 276.0 (TID 181). 1549 bytes result sent to driver
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 276.0 (TID 181) in 5 ms on localhost (1/1)
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 276.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.scheduler.DAGScheduler]ResultStage 276 (print at SparkStreamingT.java:101) finished in 0.007 s
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.scheduler.DAGScheduler]Job 181 finished: print at SparkStreamingT.java:101, took 0.013490 s
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.SparkContext]Starting job: print at SparkStreamingT.java:101
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 47 is 83 bytes
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.scheduler.DAGScheduler]Got job 182 (print at SparkStreamingT.java:101) with 1 output partitions
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 278 (print at SparkStreamingT.java:101)
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 277)
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 278 (ShuffledRDD[278] at reduceByKey at SparkStreamingT.java:94), which has no missing parents
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_182 stored as values in memory (estimated size 3.8 KB, free 900.9 MB)
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_182_piece0 stored as bytes in memory (estimated size 2.2 KB, free 900.9 MB)
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_182_piece0 in memory on 172.18.42.247:50121 (size: 2.2 KB, free: 901.6 MB)
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.SparkContext]Created broadcast 182 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 278 (ShuffledRDD[278] at reduceByKey at SparkStreamingT.java:94)
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 278.0 with 1 tasks
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 278.0 (TID 182, localhost, partition 1, PROCESS_LOCAL, 5717 bytes)
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.executor.Executor]Running task 0.0 in stage 278.0 (TID 182)
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 278.0 (TID 182). 1549 bytes result sent to driver
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.ContextCleaner]Cleaned shuffle 38
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 278.0 (TID 182) in 5 ms on localhost (1/1)
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 278.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.scheduler.DAGScheduler]ResultStage 278 (print at SparkStreamingT.java:101) finished in 0.005 s
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.scheduler.DAGScheduler]Job 182 finished: print at SparkStreamingT.java:101, took 0.036511 s
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526887821000 ms.0 from job set of time 1526887821000 ms
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.076 s for time 1526887821000 ms (execution: 0.061 s)
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 271 from persistence list
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_149_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.6 MB)
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 270 from persistence list
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.storage.BlockManager]Removing RDD 271
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.storage.BlockManager]Removing RDD 270
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 269 from persistence list
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.rdd.BlockRDD]Removing RDD 268 from persistence list
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.ContextCleaner]Cleaned shuffle 39
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.storage.BlockManager]Removing RDD 269
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.streaming.dstream.SocketInputDStream]Removing blocks of RDD BlockRDD[268] at socketTextStream at SparkStreamingT.java:72 of time 1526887821000 ms
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 1526887819000 ms
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.storage.BlockManager]Removing RDD 268
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_150_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.6 MB)
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1526887819000 ms
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_151_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.6 MB)
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_152_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.6 MB)
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_153_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.6 MB)
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.ContextCleaner]Cleaned shuffle 40
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_154_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.6 MB)
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_155_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.6 MB)
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_156_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.6 MB)
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_157_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.6 MB)
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_158_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.6 MB)
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.ContextCleaner]Cleaned shuffle 41
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_159_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.6 MB)
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_160_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_161_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.ContextCleaner]Cleaned shuffle 42
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_162_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_163_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.ContextCleaner]Cleaned shuffle 43
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_164_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_165_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.ContextCleaner]Cleaned shuffle 44
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_166_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_167_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_168_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_169_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_170_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.ContextCleaner]Cleaned shuffle 45
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_171_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_172_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_173_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_174_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_175_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_176_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.7 MB)
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_177_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_178_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_179_piece0 on 172.18.42.247:50121 in memory (size: 11.4 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_181_piece0 on 172.18.42.247:50121 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887821200
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:30:21] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:30:21] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 274.0 (TID 180)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN]  [2018-05-21 15:30:21] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 274.0 (TID 180, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:30:21] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 274.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 274.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 274
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.scheduler.DAGScheduler]ResultStage 274 (start at SparkStreamingT.java:107) failed in 0.372 s
[ERROR]  [2018-05-21 15:30:21] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 274.0 failed 1 times, most recent failure: Lost task 0.0 in stage 274.0 (TID 180, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.scheduler.DAGScheduler]Got job 183 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 279 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 279 (Receiver 0 ParallelCollectionRDD[279] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_183 stored as values in memory (estimated size 34.1 KB, free 901.7 MB)
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_183_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.7 MB)
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_183_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.SparkContext]Created broadcast 183 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 279 (Receiver 0 ParallelCollectionRDD[279] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 279.0 with 1 tasks
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 279.0 (TID 183, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.executor.Executor]Running task 0.0 in stage 279.0 (TID 183)
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887821400
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[ERROR]  [2018-05-21 15:30:21] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:30:21] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.streaming.util.RecurringTimer]Stopped timer for BlockGenerator after time 1526887821600
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.streaming.receiver.BlockGenerator]Waiting for block pushing thread to terminate
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.streaming.receiver.BlockGenerator]Pushing out the last 0 blocks
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped block pushing thread
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.streaming.receiver.BlockGenerator]Stopped BlockGenerator
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Waiting for receiver to be stopped
[ERROR]  [2018-05-21 15:30:21] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver with error: java.net.UnknownHostException: local
[ERROR]  [2018-05-21 15:30:21] [org.apache.spark.executor.Executor]Exception in task 0.0 in stage 279.0 (TID 183)
java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[WARN]  [2018-05-21 15:30:21] [org.apache.spark.scheduler.TaskSetManager]Lost task 0.0 in stage 279.0 (TID 183, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[ERROR]  [2018-05-21 15:30:21] [org.apache.spark.scheduler.TaskSetManager]Task 0 in stage 279.0 failed 1 times; aborting job
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 279.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.scheduler.TaskSchedulerImpl]Cancelling stage 279
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.scheduler.DAGScheduler]ResultStage 279 (start at SparkStreamingT.java:107) failed in 0.367 s
[ERROR]  [2018-05-21 15:30:21] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver has been stopped. Try to restart it.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 279.0 failed 1 times, most recent failure: Lost task 0.0 in stage 279.0 (TID 183, localhost): java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.streaming.scheduler.ReceiverTracker]Restarting Receiver 0
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.streaming.scheduler.ReceiverTracker]Receiver 0 started
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.scheduler.DAGScheduler]Got job 184 (start at SparkStreamingT.java:107) with 1 output partitions
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 280 (start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List()
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 280 (Receiver 0 ParallelCollectionRDD[280] at start at SparkStreamingT.java:107), which has no missing parents
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_184 stored as values in memory (estimated size 34.1 KB, free 901.6 MB)
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_184_piece0 stored as bytes in memory (estimated size 11.4 KB, free 901.6 MB)
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_184_piece0 in memory on 172.18.42.247:50121 (size: 11.4 KB, free: 901.8 MB)
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.SparkContext]Created broadcast 184 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 280 (Receiver 0 ParallelCollectionRDD[280] at start at SparkStreamingT.java:107)
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 280.0 with 1 tasks
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 280.0 (TID 184, localhost, partition 0, PROCESS_LOCAL, 6122 bytes)
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.executor.Executor]Running task 0.0 in stage 280.0 (TID 184)
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.streaming.util.RecurringTimer]Started timer for BlockGenerator at time 1526887821800
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.streaming.receiver.BlockGenerator]Started BlockGenerator
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.streaming.receiver.BlockGenerator]Started block pushing thread
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.streaming.scheduler.ReceiverTracker]Registered receiver for stream 0 from 172.18.42.247:50074
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Starting receiver 0
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.streaming.dstream.SocketReceiver]Connecting to local:9999
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopping receiver with message: Error starting receiver 0: java.net.UnknownHostException: local
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Called receiver onStop
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Deregistering receiver 0
[ERROR]  [2018-05-21 15:30:21] [org.apache.spark.streaming.scheduler.ReceiverTracker]Deregistered receiver for stream 0: Error starting receiver 0 - java.net.UnknownHostException: local
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:184)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:172)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:589)
	at java.net.Socket.connect(Socket.java:538)
	at java.net.Socket.<init>(Socket.java:434)
	at java.net.Socket.<init>(Socket.java:211)
	at org.apache.spark.streaming.dstream.SocketReceiver.onStart(SocketInputDStream.scala:61)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:149)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:131)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:597)
	at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverTrackerEndpoint$$anonfun$9.apply(ReceiverTracker.scala:587)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.SparkContext$$anonfun$33.apply(SparkContext.scala:1974)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[INFO]  [2018-05-21 15:30:21] [org.apache.spark.streaming.receiver.ReceiverSupervisorImpl]Stopped receiver 0
[INFO]  [2018-05-21 15:30:21] [org.apache.spark.streaming.receiver.BlockGenerator]Stopping BlockGenerator
[INFO]  [2018-05-21 16:04:04] [com.jiang.sparkstream.HdfsSparkStreamT]create JavaStreamingContext...
[INFO]  [2018-05-21 16:04:05] [org.apache.spark.SparkContext]Running Spark version 2.0.0
[WARN]  [2018-05-21 16:04:05] [org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO]  [2018-05-21 16:04:05] [org.apache.spark.SecurityManager]Changing view acls to: Bruin,hadoop
[INFO]  [2018-05-21 16:04:05] [org.apache.spark.SecurityManager]Changing modify acls to: Bruin,hadoop
[INFO]  [2018-05-21 16:04:05] [org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO]  [2018-05-21 16:04:05] [org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO]  [2018-05-21 16:04:05] [org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Bruin, hadoop); groups with view permissions: Set(); users  with modify permissions: Set(Bruin, hadoop); groups with modify permissions: Set()
[INFO]  [2018-05-21 16:04:07] [org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 50594.
[INFO]  [2018-05-21 16:04:07] [org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO]  [2018-05-21 16:04:07] [org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO]  [2018-05-21 16:04:07] [org.apache.spark.storage.DiskBlockManager]Created local directory at C:\Users\Bruin\AppData\Local\Temp\blockmgr-1b8e35d9-5e77-4add-82ca-18df18c28eb4
[INFO]  [2018-05-21 16:04:07] [org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 901.8 MB
[INFO]  [2018-05-21 16:04:07] [org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO]  [2018-05-21 16:04:07] [org.spark_project.jetty.util.log]Logging initialized @3693ms
[INFO]  [2018-05-21 16:04:07] [org.spark_project.jetty.server.Server]jetty-9.2.z-SNAPSHOT
[INFO]  [2018-05-21 16:04:07] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@41dd05a{/jobs,null,AVAILABLE}
[INFO]  [2018-05-21 16:04:07] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@613a8ee1{/jobs/json,null,AVAILABLE}
[INFO]  [2018-05-21 16:04:07] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@178213b{/jobs/job,null,AVAILABLE}
[INFO]  [2018-05-21 16:04:07] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7103cb56{/jobs/job/json,null,AVAILABLE}
[INFO]  [2018-05-21 16:04:07] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1b765a2c{/stages,null,AVAILABLE}
[INFO]  [2018-05-21 16:04:07] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2e8e8225{/stages/json,null,AVAILABLE}
[INFO]  [2018-05-21 16:04:07] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6ebf0f36{/stages/stage,null,AVAILABLE}
[INFO]  [2018-05-21 16:04:07] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@18920cc{/stages/stage/json,null,AVAILABLE}
[INFO]  [2018-05-21 16:04:07] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2807bdeb{/stages/pool,null,AVAILABLE}
[INFO]  [2018-05-21 16:04:07] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@72c28d64{/stages/pool/json,null,AVAILABLE}
[INFO]  [2018-05-21 16:04:07] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6492fab5{/storage,null,AVAILABLE}
[INFO]  [2018-05-21 16:04:07] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2c5529ab{/storage/json,null,AVAILABLE}
[INFO]  [2018-05-21 16:04:07] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@39a8312f{/storage/rdd,null,AVAILABLE}
[INFO]  [2018-05-21 16:04:07] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5f6722d3{/storage/rdd/json,null,AVAILABLE}
[INFO]  [2018-05-21 16:04:07] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2c532cd8{/environment,null,AVAILABLE}
[INFO]  [2018-05-21 16:04:07] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@294e5088{/environment/json,null,AVAILABLE}
[INFO]  [2018-05-21 16:04:07] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@51972dc7{/executors,null,AVAILABLE}
[INFO]  [2018-05-21 16:04:07] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3700ec9c{/executors/json,null,AVAILABLE}
[INFO]  [2018-05-21 16:04:07] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2002348{/executors/threadDump,null,AVAILABLE}
[INFO]  [2018-05-21 16:04:07] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5911e990{/executors/threadDump/json,null,AVAILABLE}
[INFO]  [2018-05-21 16:04:07] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@31000e60{/static,null,AVAILABLE}
[INFO]  [2018-05-21 16:04:07] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1d470d0{/,null,AVAILABLE}
[INFO]  [2018-05-21 16:04:07] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@24d09c1{/api,null,AVAILABLE}
[INFO]  [2018-05-21 16:04:07] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@54c62d71{/stages/stage/kill,null,AVAILABLE}
[INFO]  [2018-05-21 16:04:07] [org.spark_project.jetty.server.ServerConnector]Started ServerConnector@a43ce46{HTTP/1.1}{0.0.0.0:4040}
[INFO]  [2018-05-21 16:04:07] [org.spark_project.jetty.server.Server]Started @3830ms
[INFO]  [2018-05-21 16:04:07] [org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO]  [2018-05-21 16:04:07] [org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://172.18.42.247:4040
[INFO]  [2018-05-21 16:04:07] [org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO]  [2018-05-21 16:04:07] [org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50607.
[INFO]  [2018-05-21 16:04:07] [org.apache.spark.network.netty.NettyBlockTransferService]Server created on 172.18.42.247:50607
[INFO]  [2018-05-21 16:04:07] [org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 172.18.42.247, 50607)
[INFO]  [2018-05-21 16:04:07] [org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 172.18.42.247:50607 with 901.8 MB RAM, BlockManagerId(driver, 172.18.42.247, 50607)
[INFO]  [2018-05-21 16:04:07] [org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 172.18.42.247, 50607)
[INFO]  [2018-05-21 16:04:07] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@37ebc9d8{/metrics/json,null,AVAILABLE}
[ERROR]  [2018-05-21 16:04:11] [org.apache.hadoop.util.Shell]Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:278)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:300)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:293)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:76)
	at org.apache.hadoop.conf.Configuration.getTrimmedStrings(Configuration.java:1546)
	at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:519)
	at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:453)
	at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:136)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2433)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:88)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2467)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2449)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:367)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:287)
	at org.apache.spark.streaming.StreamingContext.checkpoint(StreamingContext.scala:238)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.checkpoint(JavaStreamingContext.scala:506)
	at com.jiang.sparkstream.HdfsSparkStreamT.context(HdfsSparkStreamT.java:41)
	at com.jiang.sparkstream.HdfsSparkStreamT.run(HdfsSparkStreamT.java:47)
	at com.jiang.sparkstream.HdfsSparkStreamT.main(HdfsSparkStreamT.java:73)
[INFO]  [2018-05-21 16:04:31] [org.apache.hadoop.ipc.Client]Retrying connect to server: hadoop1/192.168.0.51:9000. Already tried 0 time(s); maxRetries=45
[INFO]  [2018-05-21 16:04:32] [org.apache.spark.streaming.dstream.FileInputDStream]Duration for remembering RDDs set to 60000 ms for org.apache.spark.streaming.dstream.FileInputDStream@1a3e5f23
[INFO]  [2018-05-21 16:04:32] [org.apache.spark.streaming.dstream.FileInputDStream]Slide time = 15000 ms
[INFO]  [2018-05-21 16:04:32] [org.apache.spark.streaming.dstream.FileInputDStream]Storage level = Serialized 1x Replicated
[INFO]  [2018-05-21 16:04:32] [org.apache.spark.streaming.dstream.FileInputDStream]Checkpoint interval = null
[INFO]  [2018-05-21 16:04:32] [org.apache.spark.streaming.dstream.FileInputDStream]Remember interval = 60000 ms
[INFO]  [2018-05-21 16:04:32] [org.apache.spark.streaming.dstream.FileInputDStream]Initialized and validated org.apache.spark.streaming.dstream.FileInputDStream@1a3e5f23
[INFO]  [2018-05-21 16:04:32] [org.apache.spark.streaming.dstream.MappedDStream]Slide time = 15000 ms
[INFO]  [2018-05-21 16:04:32] [org.apache.spark.streaming.dstream.MappedDStream]Storage level = Serialized 1x Replicated
[INFO]  [2018-05-21 16:04:32] [org.apache.spark.streaming.dstream.MappedDStream]Checkpoint interval = null
[INFO]  [2018-05-21 16:04:32] [org.apache.spark.streaming.dstream.MappedDStream]Remember interval = 15000 ms
[INFO]  [2018-05-21 16:04:32] [org.apache.spark.streaming.dstream.MappedDStream]Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@63bde6c2
[INFO]  [2018-05-21 16:04:32] [org.apache.spark.streaming.dstream.FlatMappedDStream]Slide time = 15000 ms
[INFO]  [2018-05-21 16:04:32] [org.apache.spark.streaming.dstream.FlatMappedDStream]Storage level = Serialized 1x Replicated
[INFO]  [2018-05-21 16:04:32] [org.apache.spark.streaming.dstream.FlatMappedDStream]Checkpoint interval = null
[INFO]  [2018-05-21 16:04:32] [org.apache.spark.streaming.dstream.FlatMappedDStream]Remember interval = 15000 ms
[INFO]  [2018-05-21 16:04:32] [org.apache.spark.streaming.dstream.FlatMappedDStream]Initialized and validated org.apache.spark.streaming.dstream.FlatMappedDStream@f3021cb
[INFO]  [2018-05-21 16:04:32] [org.apache.spark.streaming.dstream.MappedDStream]Slide time = 15000 ms
[INFO]  [2018-05-21 16:04:32] [org.apache.spark.streaming.dstream.MappedDStream]Storage level = Serialized 1x Replicated
[INFO]  [2018-05-21 16:04:32] [org.apache.spark.streaming.dstream.MappedDStream]Checkpoint interval = null
[INFO]  [2018-05-21 16:04:32] [org.apache.spark.streaming.dstream.MappedDStream]Remember interval = 15000 ms
[INFO]  [2018-05-21 16:04:32] [org.apache.spark.streaming.dstream.MappedDStream]Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@4b7c4456
[INFO]  [2018-05-21 16:04:32] [org.apache.spark.streaming.dstream.ShuffledDStream]Slide time = 15000 ms
[INFO]  [2018-05-21 16:04:32] [org.apache.spark.streaming.dstream.ShuffledDStream]Storage level = Serialized 1x Replicated
[INFO]  [2018-05-21 16:04:32] [org.apache.spark.streaming.dstream.ShuffledDStream]Checkpoint interval = null
[INFO]  [2018-05-21 16:04:32] [org.apache.spark.streaming.dstream.ShuffledDStream]Remember interval = 15000 ms
[INFO]  [2018-05-21 16:04:32] [org.apache.spark.streaming.dstream.ShuffledDStream]Initialized and validated org.apache.spark.streaming.dstream.ShuffledDStream@1816e24a
[INFO]  [2018-05-21 16:04:32] [org.apache.spark.streaming.dstream.ForEachDStream]Slide time = 15000 ms
[INFO]  [2018-05-21 16:04:32] [org.apache.spark.streaming.dstream.ForEachDStream]Storage level = Serialized 1x Replicated
[INFO]  [2018-05-21 16:04:32] [org.apache.spark.streaming.dstream.ForEachDStream]Checkpoint interval = null
[INFO]  [2018-05-21 16:04:32] [org.apache.spark.streaming.dstream.ForEachDStream]Remember interval = 15000 ms
[INFO]  [2018-05-21 16:04:32] [org.apache.spark.streaming.dstream.ForEachDStream]Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@7dff6d05
[INFO]  [2018-05-21 16:04:32] [org.apache.spark.streaming.util.RecurringTimer]Started timer for JobGenerator at time 1526889885000
[INFO]  [2018-05-21 16:04:32] [org.apache.spark.streaming.scheduler.JobGenerator]Started JobGenerator at 1526889885000 ms
[INFO]  [2018-05-21 16:04:32] [org.apache.spark.streaming.scheduler.JobScheduler]Started JobScheduler
[INFO]  [2018-05-21 16:04:32] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@240f6c41{/streaming,null,AVAILABLE}
[INFO]  [2018-05-21 16:04:32] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2015b2cd{/streaming/json,null,AVAILABLE}
[INFO]  [2018-05-21 16:04:32] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@73a19967{/streaming/batch,null,AVAILABLE}
[INFO]  [2018-05-21 16:04:32] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6e1b9411{/streaming/batch/json,null,AVAILABLE}
[INFO]  [2018-05-21 16:04:32] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@112d1c8e{/static/streaming,null,AVAILABLE}
[INFO]  [2018-05-21 16:04:32] [org.apache.spark.streaming.StreamingContext]StreamingContext started
[WARN]  [2018-05-21 16:04:46] [org.apache.spark.streaming.dstream.FileInputDStream]Error finding new files
java.net.ConnectException: Call From hadoop/172.18.42.247 to hadoop:9000 failed on connection exception: java.net.ConnectException: Connection refused: no further information; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1351)
	at org.apache.hadoop.ipc.Client.call(Client.java:1300)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.getFileInfo(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:651)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1679)
	at org.apache.hadoop.hdfs.DistributedFileSystem$17.doCall(DistributedFileSystem.java:1106)
	at org.apache.hadoop.hdfs.DistributedFileSystem$17.doCall(DistributedFileSystem.java:1102)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1102)
	at org.apache.hadoop.fs.FileSystem.globStatusInternal(FileSystem.java:1701)
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1647)
	at org.apache.spark.streaming.dstream.FileInputDStream.findNewFiles(FileInputDStream.scala:205)
	at org.apache.spark.streaming.dstream.FileInputDStream.compute(FileInputDStream.scala:149)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:257)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.MappedDStream.compute(MappedDStream.scala:36)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:257)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.FlatMappedDStream.compute(FlatMappedDStream.scala:36)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:257)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.MappedDStream.compute(MappedDStream.scala:36)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:257)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.ShuffledDStream.compute(ShuffledDStream.scala:41)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:257)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.ForEachDStream.generateJob(ForEachDStream.scala:48)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:117)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:116)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:251)
	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:105)
	at org.apache.spark.streaming.DStreamGraph.generateJobs(DStreamGraph.scala:116)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$3.apply(JobGenerator.scala:248)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$3.apply(JobGenerator.scala:246)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.streaming.scheduler.JobGenerator.generateJobs(JobGenerator.scala:246)
	at org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:182)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:88)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:87)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.ConnectException: Connection refused: no further information
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:547)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:642)
	at org.apache.hadoop.ipc.Client$Connection.access$2600(Client.java:314)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1399)
	at org.apache.hadoop.ipc.Client.call(Client.java:1318)
	... 92 more
[INFO]  [2018-05-21 16:04:46] [org.apache.spark.streaming.dstream.FileInputDStream]New files at time 1526889885000 ms:

[INFO]  [2018-05-21 16:04:46] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526889885000 ms
[INFO]  [2018-05-21 16:04:46] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526889885000 ms
[INFO]  [2018-05-21 16:04:46] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526889885000 ms
[INFO]  [2018-05-21 16:04:46] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526889885000 ms.0 from job set of time 1526889885000 ms
[INFO]  [2018-05-21 16:04:46] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526889885000 ms
[INFO]  [2018-05-21 16:04:46] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526889885000 ms to writer queue
[INFO]  [2018-05-21 16:04:46] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526889885000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526889885000'
[INFO]  [2018-05-21 16:04:46] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:04:46] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 3 (mapToPair at HdfsSparkStreamT.java:56)
[INFO]  [2018-05-21 16:04:46] [org.apache.spark.scheduler.DAGScheduler]Got job 0 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:04:46] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 1 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:04:46] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 0)
[INFO]  [2018-05-21 16:04:46] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:04:46] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 1 (ShuffledRDD[4] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:04:46] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 3.8 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:04:46] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:04:46] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 172.18.42.247:50607 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:04:46] [org.apache.spark.SparkContext]Created broadcast 0 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:04:46] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 1 (ShuffledRDD[4] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:04:46] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 1.0 with 1 tasks
[INFO]  [2018-05-21 16:04:46] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 1.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:04:46] [org.apache.spark.executor.Executor]Running task 0.0 in stage 1.0 (TID 0)
[INFO]  [2018-05-21 16:04:46] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:04:46] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 7 ms
[INFO]  [2018-05-21 16:04:47] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 1.0 (TID 0). 1549 bytes result sent to driver
[INFO]  [2018-05-21 16:04:47] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 1.0 (TID 0) in 200 ms on localhost (1/1)
[INFO]  [2018-05-21 16:04:47] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:04:47] [org.apache.spark.scheduler.DAGScheduler]ResultStage 1 (print at HdfsSparkStreamT.java:59) finished in 0.336 s
[INFO]  [2018-05-21 16:04:47] [org.apache.spark.scheduler.DAGScheduler]Job 0 finished: print at HdfsSparkStreamT.java:59, took 0.842494 s
[INFO]  [2018-05-21 16:04:47] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:04:47] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 0 is 83 bytes
[INFO]  [2018-05-21 16:04:47] [org.apache.spark.scheduler.DAGScheduler]Got job 1 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:04:47] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 3 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:04:47] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 2)
[INFO]  [2018-05-21 16:04:47] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:04:47] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 3 (ShuffledRDD[4] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:04:47] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 3.8 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:04:47] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:04:47] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 172.18.42.247:50607 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:04:47] [org.apache.spark.SparkContext]Created broadcast 1 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:04:47] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 3 (ShuffledRDD[4] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:04:47] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 3.0 with 1 tasks
[INFO]  [2018-05-21 16:04:47] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 3.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:04:47] [org.apache.spark.executor.Executor]Running task 0.0 in stage 3.0 (TID 1)
[INFO]  [2018-05-21 16:04:47] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:04:47] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO]  [2018-05-21 16:04:47] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 3.0 (TID 1). 1549 bytes result sent to driver
[INFO]  [2018-05-21 16:04:47] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 3.0 (TID 1) in 11 ms on localhost (1/1)
[INFO]  [2018-05-21 16:04:47] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:04:47] [org.apache.spark.scheduler.DAGScheduler]ResultStage 3 (print at HdfsSparkStreamT.java:59) finished in 0.014 s
[INFO]  [2018-05-21 16:04:47] [org.apache.spark.scheduler.DAGScheduler]Job 1 finished: print at HdfsSparkStreamT.java:59, took 0.068614 s
[INFO]  [2018-05-21 16:04:47] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526889885000 ms.0 from job set of time 1526889885000 ms
[INFO]  [2018-05-21 16:04:47] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 2.243 s for time 1526889885000 ms (execution: 1.016 s)
[INFO]  [2018-05-21 16:04:47] [org.apache.spark.streaming.dstream.FileInputDStream]Cleared 0 old files that were older than 1526889825000 ms: 
[INFO]  [2018-05-21 16:04:47] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526889885000 ms
[INFO]  [2018-05-21 16:04:47] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526889885000 ms
[INFO]  [2018-05-21 16:04:47] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526889885000 ms
[INFO]  [2018-05-21 16:04:47] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526889885000 ms to writer queue
[INFO]  [2018-05-21 16:04:47] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_0_piece0 on 172.18.42.247:50607 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:04:47] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_1_piece0 on 172.18.42.247:50607 in memory (size: 2.2 KB, free: 901.8 MB)
[WARN]  [2018-05-21 16:05:01] [org.apache.spark.streaming.dstream.FileInputDStream]Error finding new files
java.net.ConnectException: Call From hadoop/172.18.42.247 to hadoop:9000 failed on connection exception: java.net.ConnectException: Connection refused: no further information; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1351)
	at org.apache.hadoop.ipc.Client.call(Client.java:1300)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.getFileInfo(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:651)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1679)
	at org.apache.hadoop.hdfs.DistributedFileSystem$17.doCall(DistributedFileSystem.java:1106)
	at org.apache.hadoop.hdfs.DistributedFileSystem$17.doCall(DistributedFileSystem.java:1102)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1102)
	at org.apache.hadoop.fs.FileSystem.globStatusInternal(FileSystem.java:1701)
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1647)
	at org.apache.spark.streaming.dstream.FileInputDStream.findNewFiles(FileInputDStream.scala:205)
	at org.apache.spark.streaming.dstream.FileInputDStream.compute(FileInputDStream.scala:149)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:257)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.MappedDStream.compute(MappedDStream.scala:36)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:257)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.FlatMappedDStream.compute(FlatMappedDStream.scala:36)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:257)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.MappedDStream.compute(MappedDStream.scala:36)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:257)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.ShuffledDStream.compute(ShuffledDStream.scala:41)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:257)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.ForEachDStream.generateJob(ForEachDStream.scala:48)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:117)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:116)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:251)
	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:105)
	at org.apache.spark.streaming.DStreamGraph.generateJobs(DStreamGraph.scala:116)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$3.apply(JobGenerator.scala:248)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$3.apply(JobGenerator.scala:246)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.streaming.scheduler.JobGenerator.generateJobs(JobGenerator.scala:246)
	at org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:182)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:88)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:87)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.ConnectException: Connection refused: no further information
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:547)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:642)
	at org.apache.hadoop.ipc.Client$Connection.access$2600(Client.java:314)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1399)
	at org.apache.hadoop.ipc.Client.call(Client.java:1318)
	... 92 more
[INFO]  [2018-05-21 16:05:01] [org.apache.spark.streaming.dstream.FileInputDStream]New files at time 1526889900000 ms:

[INFO]  [2018-05-21 16:05:01] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:05:01] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526889900000 ms.0 from job set of time 1526889900000 ms
[INFO]  [2018-05-21 16:05:01] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526889900000 ms
[INFO]  [2018-05-21 16:05:01] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526889900000 ms
[INFO]  [2018-05-21 16:05:01] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526889900000 ms
[INFO]  [2018-05-21 16:05:01] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526889900000 ms
[INFO]  [2018-05-21 16:05:01] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526889900000 ms to writer queue
[INFO]  [2018-05-21 16:05:01] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 8 (mapToPair at HdfsSparkStreamT.java:56)
[INFO]  [2018-05-21 16:05:01] [org.apache.spark.scheduler.DAGScheduler]Got job 2 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:05:01] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 5 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:05:01] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 4)
[INFO]  [2018-05-21 16:05:01] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:05:01] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 5 (ShuffledRDD[9] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:05:01] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_2 stored as values in memory (estimated size 3.8 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:05:01] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:05:01] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_2_piece0 in memory on 172.18.42.247:50607 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:05:01] [org.apache.spark.SparkContext]Created broadcast 2 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:05:01] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 5 (ShuffledRDD[9] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:05:01] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 5.0 with 1 tasks
[INFO]  [2018-05-21 16:05:01] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 5.0 (TID 2, localhost, partition 0, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:05:01] [org.apache.spark.executor.Executor]Running task 0.0 in stage 5.0 (TID 2)
[INFO]  [2018-05-21 16:05:01] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:05:01] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 16:05:01] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 5.0 (TID 2). 1549 bytes result sent to driver
[INFO]  [2018-05-21 16:05:01] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 5.0 (TID 2) in 10 ms on localhost (1/1)
[INFO]  [2018-05-21 16:05:01] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 5.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:05:01] [org.apache.spark.scheduler.DAGScheduler]ResultStage 5 (print at HdfsSparkStreamT.java:59) finished in 0.015 s
[INFO]  [2018-05-21 16:05:01] [org.apache.spark.scheduler.DAGScheduler]Job 2 finished: print at HdfsSparkStreamT.java:59, took 0.044451 s
[INFO]  [2018-05-21 16:05:01] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:05:01] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 1 is 83 bytes
[INFO]  [2018-05-21 16:05:01] [org.apache.spark.scheduler.DAGScheduler]Got job 3 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:05:01] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 7 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:05:01] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 6)
[INFO]  [2018-05-21 16:05:01] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:05:01] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 7 (ShuffledRDD[9] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:05:01] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_3 stored as values in memory (estimated size 3.8 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:05:01] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:05:01] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_3_piece0 in memory on 172.18.42.247:50607 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:05:01] [org.apache.spark.SparkContext]Created broadcast 3 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:05:01] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 7 (ShuffledRDD[9] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:05:01] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 7.0 with 1 tasks
[INFO]  [2018-05-21 16:05:01] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 7.0 (TID 3, localhost, partition 1, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:05:01] [org.apache.spark.executor.Executor]Running task 0.0 in stage 7.0 (TID 3)
[INFO]  [2018-05-21 16:05:01] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:05:01] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 3 ms
[INFO]  [2018-05-21 16:05:01] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 7.0 (TID 3). 1549 bytes result sent to driver
[INFO]  [2018-05-21 16:05:01] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 7.0 (TID 3) in 10 ms on localhost (1/1)
[INFO]  [2018-05-21 16:05:01] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 7.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:05:01] [org.apache.spark.scheduler.DAGScheduler]ResultStage 7 (print at HdfsSparkStreamT.java:59) finished in 0.010 s
[INFO]  [2018-05-21 16:05:01] [org.apache.spark.scheduler.DAGScheduler]Job 3 finished: print at HdfsSparkStreamT.java:59, took 0.071784 s
[INFO]  [2018-05-21 16:05:01] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526889900000 ms.0 from job set of time 1526889900000 ms
[INFO]  [2018-05-21 16:05:01] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 1.170 s for time 1526889900000 ms (execution: 0.120 s)
[INFO]  [2018-05-21 16:05:01] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 4 from persistence list
[INFO]  [2018-05-21 16:05:01] [org.apache.spark.storage.BlockManager]Removing RDD 4
[INFO]  [2018-05-21 16:05:01] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 3 from persistence list
[INFO]  [2018-05-21 16:05:01] [org.apache.spark.storage.BlockManager]Removing RDD 3
[INFO]  [2018-05-21 16:05:01] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 2 from persistence list
[INFO]  [2018-05-21 16:05:01] [org.apache.spark.storage.BlockManager]Removing RDD 2
[INFO]  [2018-05-21 16:05:01] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 1 from persistence list
[INFO]  [2018-05-21 16:05:01] [org.apache.spark.storage.BlockManager]Removing RDD 1
[INFO]  [2018-05-21 16:05:01] [org.apache.spark.streaming.dstream.FileInputDStream]Cleared 0 old files that were older than 1526889840000 ms: 
[INFO]  [2018-05-21 16:05:01] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526889900000 ms
[INFO]  [2018-05-21 16:05:01] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526889900000 ms
[INFO]  [2018-05-21 16:05:01] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526889900000 ms
[INFO]  [2018-05-21 16:05:01] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526889900000 ms to writer queue
[INFO]  [2018-05-21 16:05:08] [org.apache.hadoop.hdfs.DFSClient]Exception in createBlockOutputStream
java.net.ConnectException: Connection timed out: no further information
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.hdfs.DFSOutputStream.createSocketForPipeline(DFSOutputStream.java:1305)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1128)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1088)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:514)
[INFO]  [2018-05-21 16:05:08] [org.apache.hadoop.hdfs.DFSClient]Abandoning BP-424611100-127.0.0.1-1526315083190:blk_1073742201_1377
[INFO]  [2018-05-21 16:05:08] [org.apache.hadoop.hdfs.DFSClient]Excluding datanode 192.168.0.52:50010
[INFO]  [2018-05-21 16:05:12] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526889885000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526889885000', took 4393 bytes and 26332 ms
[INFO]  [2018-05-21 16:05:12] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526889885000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526889885000'
[INFO]  [2018-05-21 16:05:13] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526889885000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526889885000', took 4390 bytes and 483 ms
[INFO]  [2018-05-21 16:05:13] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526889900000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526889900000'
[INFO]  [2018-05-21 16:05:13] [org.apache.spark.streaming.DStreamGraph]Clearing checkpoint data for time 1526889885000 ms
[INFO]  [2018-05-21 16:05:13] [org.apache.spark.streaming.DStreamGraph]Cleared checkpoint data for time 1526889885000 ms
[INFO]  [2018-05-21 16:05:13] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO]  [2018-05-21 16:05:13] [org.apache.spark.streaming.util.FileBasedWriteAheadLog_ReceivedBlockTracker]Attempting to clear 0 old log files in hdfs://hadoop1:9000/lib/sparkstreaming/check_point/receivedBlockMetadata older than 1526889825000: 
[INFO]  [2018-05-21 16:05:13] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 
[INFO]  [2018-05-21 16:05:13] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526889900000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526889900000', took 4408 bytes and 467 ms
[INFO]  [2018-05-21 16:05:13] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526889900000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526889900000'
[INFO]  [2018-05-21 16:05:13] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526889900000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526889900000', took 4404 bytes and 71 ms
[INFO]  [2018-05-21 16:05:13] [org.apache.spark.streaming.DStreamGraph]Clearing checkpoint data for time 1526889900000 ms
[INFO]  [2018-05-21 16:05:13] [org.apache.spark.streaming.DStreamGraph]Cleared checkpoint data for time 1526889900000 ms
[INFO]  [2018-05-21 16:05:13] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO]  [2018-05-21 16:05:13] [org.apache.spark.streaming.util.FileBasedWriteAheadLog_ReceivedBlockTracker]Attempting to clear 0 old log files in hdfs://hadoop1:9000/lib/sparkstreaming/check_point/receivedBlockMetadata older than 1526889840000: 
[INFO]  [2018-05-21 16:05:13] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 
[WARN]  [2018-05-21 16:05:16] [org.apache.spark.streaming.dstream.FileInputDStream]Error finding new files
java.net.ConnectException: Call From hadoop/172.18.42.247 to hadoop:9000 failed on connection exception: java.net.ConnectException: Connection refused: no further information; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1351)
	at org.apache.hadoop.ipc.Client.call(Client.java:1300)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.getFileInfo(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:651)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1679)
	at org.apache.hadoop.hdfs.DistributedFileSystem$17.doCall(DistributedFileSystem.java:1106)
	at org.apache.hadoop.hdfs.DistributedFileSystem$17.doCall(DistributedFileSystem.java:1102)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1102)
	at org.apache.hadoop.fs.FileSystem.globStatusInternal(FileSystem.java:1701)
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1647)
	at org.apache.spark.streaming.dstream.FileInputDStream.findNewFiles(FileInputDStream.scala:205)
	at org.apache.spark.streaming.dstream.FileInputDStream.compute(FileInputDStream.scala:149)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:257)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.MappedDStream.compute(MappedDStream.scala:36)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:257)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.FlatMappedDStream.compute(FlatMappedDStream.scala:36)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:257)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.MappedDStream.compute(MappedDStream.scala:36)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:257)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.ShuffledDStream.compute(ShuffledDStream.scala:41)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:257)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.ForEachDStream.generateJob(ForEachDStream.scala:48)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:117)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:116)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:251)
	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:105)
	at org.apache.spark.streaming.DStreamGraph.generateJobs(DStreamGraph.scala:116)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$3.apply(JobGenerator.scala:248)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$3.apply(JobGenerator.scala:246)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.streaming.scheduler.JobGenerator.generateJobs(JobGenerator.scala:246)
	at org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:182)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:88)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:87)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.ConnectException: Connection refused: no further information
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:547)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:642)
	at org.apache.hadoop.ipc.Client$Connection.access$2600(Client.java:314)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1399)
	at org.apache.hadoop.ipc.Client.call(Client.java:1318)
	... 91 more
[INFO]  [2018-05-21 16:05:16] [org.apache.spark.streaming.dstream.FileInputDStream]New files at time 1526889915000 ms:

[INFO]  [2018-05-21 16:05:16] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526889915000 ms
[INFO]  [2018-05-21 16:05:16] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526889915000 ms
[INFO]  [2018-05-21 16:05:16] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526889915000 ms
[INFO]  [2018-05-21 16:05:16] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526889915000 ms
[INFO]  [2018-05-21 16:05:16] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526889915000 ms to writer queue
[INFO]  [2018-05-21 16:05:16] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526889915000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526889915000'
[INFO]  [2018-05-21 16:05:16] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526889915000 ms.0 from job set of time 1526889915000 ms
[INFO]  [2018-05-21 16:05:16] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:05:16] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 13 (mapToPair at HdfsSparkStreamT.java:56)
[INFO]  [2018-05-21 16:05:16] [org.apache.spark.scheduler.DAGScheduler]Got job 4 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:05:16] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 9 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:05:16] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 8)
[INFO]  [2018-05-21 16:05:16] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:05:16] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 9 (ShuffledRDD[14] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:05:16] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_4 stored as values in memory (estimated size 3.8 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:05:16] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:05:16] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_4_piece0 in memory on 172.18.42.247:50607 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:05:16] [org.apache.spark.SparkContext]Created broadcast 4 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:05:16] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 9 (ShuffledRDD[14] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:05:16] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 9.0 with 1 tasks
[INFO]  [2018-05-21 16:05:16] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 9.0 (TID 4, localhost, partition 0, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:05:16] [org.apache.spark.executor.Executor]Running task 0.0 in stage 9.0 (TID 4)
[INFO]  [2018-05-21 16:05:16] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:05:16] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 16:05:16] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 9.0 (TID 4). 1549 bytes result sent to driver
[INFO]  [2018-05-21 16:05:16] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 9.0 (TID 4) in 10 ms on localhost (1/1)
[INFO]  [2018-05-21 16:05:16] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 9.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:05:16] [org.apache.spark.scheduler.DAGScheduler]ResultStage 9 (print at HdfsSparkStreamT.java:59) finished in 0.010 s
[INFO]  [2018-05-21 16:05:16] [org.apache.spark.scheduler.DAGScheduler]Job 4 finished: print at HdfsSparkStreamT.java:59, took 0.027308 s
[INFO]  [2018-05-21 16:05:16] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:05:16] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 2 is 83 bytes
[INFO]  [2018-05-21 16:05:16] [org.apache.spark.scheduler.DAGScheduler]Got job 5 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:05:16] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 11 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:05:16] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 10)
[INFO]  [2018-05-21 16:05:16] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:05:16] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 11 (ShuffledRDD[14] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:05:16] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_5 stored as values in memory (estimated size 3.8 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:05:16] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:05:16] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_5_piece0 in memory on 172.18.42.247:50607 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:05:16] [org.apache.spark.SparkContext]Created broadcast 5 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:05:16] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 11 (ShuffledRDD[14] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:05:16] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 11.0 with 1 tasks
[INFO]  [2018-05-21 16:05:16] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 11.0 (TID 5, localhost, partition 1, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:05:16] [org.apache.spark.executor.Executor]Running task 0.0 in stage 11.0 (TID 5)
[INFO]  [2018-05-21 16:05:16] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:05:16] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 16:05:16] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 11.0 (TID 5). 1549 bytes result sent to driver
[INFO]  [2018-05-21 16:05:16] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 11.0 (TID 5) in 10 ms on localhost (1/1)
[INFO]  [2018-05-21 16:05:16] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 11.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:05:16] [org.apache.spark.scheduler.DAGScheduler]ResultStage 11 (print at HdfsSparkStreamT.java:59) finished in 0.010 s
[INFO]  [2018-05-21 16:05:16] [org.apache.spark.scheduler.DAGScheduler]Job 5 finished: print at HdfsSparkStreamT.java:59, took 0.056663 s
[INFO]  [2018-05-21 16:05:16] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526889915000 ms.0 from job set of time 1526889915000 ms
[INFO]  [2018-05-21 16:05:16] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 1.134 s for time 1526889915000 ms (execution: 0.100 s)
[INFO]  [2018-05-21 16:05:16] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 9 from persistence list
[INFO]  [2018-05-21 16:05:16] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 8 from persistence list
[INFO]  [2018-05-21 16:05:16] [org.apache.spark.storage.BlockManager]Removing RDD 9
[INFO]  [2018-05-21 16:05:16] [org.apache.spark.storage.BlockManager]Removing RDD 8
[INFO]  [2018-05-21 16:05:16] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 7 from persistence list
[INFO]  [2018-05-21 16:05:16] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 6 from persistence list
[INFO]  [2018-05-21 16:05:16] [org.apache.spark.storage.BlockManager]Removing RDD 7
[INFO]  [2018-05-21 16:05:16] [org.apache.spark.storage.BlockManager]Removing RDD 6
[INFO]  [2018-05-21 16:05:16] [org.apache.spark.streaming.dstream.FileInputDStream]Cleared 0 old files that were older than 1526889855000 ms: 
[INFO]  [2018-05-21 16:05:16] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526889915000 ms
[INFO]  [2018-05-21 16:05:16] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526889915000 ms
[INFO]  [2018-05-21 16:05:16] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526889915000 ms
[INFO]  [2018-05-21 16:05:16] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526889915000 ms to writer queue
[INFO]  [2018-05-21 16:05:16] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526889915000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526889915000', took 4424 bytes and 557 ms
[INFO]  [2018-05-21 16:05:16] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526889915000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526889915000'
[INFO]  [2018-05-21 16:05:16] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526889915000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526889915000', took 4420 bytes and 73 ms
[INFO]  [2018-05-21 16:05:16] [org.apache.spark.streaming.DStreamGraph]Clearing checkpoint data for time 1526889915000 ms
[INFO]  [2018-05-21 16:05:16] [org.apache.spark.streaming.DStreamGraph]Cleared checkpoint data for time 1526889915000 ms
[INFO]  [2018-05-21 16:05:16] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO]  [2018-05-21 16:05:16] [org.apache.spark.streaming.util.FileBasedWriteAheadLog_ReceivedBlockTracker]Attempting to clear 0 old log files in hdfs://hadoop1:9000/lib/sparkstreaming/check_point/receivedBlockMetadata older than 1526889855000: 
[INFO]  [2018-05-21 16:05:16] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 
[WARN]  [2018-05-21 16:05:31] [org.apache.spark.streaming.dstream.FileInputDStream]Error finding new files
java.net.ConnectException: Call From hadoop/172.18.42.247 to hadoop:9000 failed on connection exception: java.net.ConnectException: Connection refused: no further information; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1351)
	at org.apache.hadoop.ipc.Client.call(Client.java:1300)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.getFileInfo(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:651)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1679)
	at org.apache.hadoop.hdfs.DistributedFileSystem$17.doCall(DistributedFileSystem.java:1106)
	at org.apache.hadoop.hdfs.DistributedFileSystem$17.doCall(DistributedFileSystem.java:1102)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1102)
	at org.apache.hadoop.fs.FileSystem.globStatusInternal(FileSystem.java:1701)
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1647)
	at org.apache.spark.streaming.dstream.FileInputDStream.findNewFiles(FileInputDStream.scala:205)
	at org.apache.spark.streaming.dstream.FileInputDStream.compute(FileInputDStream.scala:149)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:257)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.MappedDStream.compute(MappedDStream.scala:36)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:257)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.FlatMappedDStream.compute(FlatMappedDStream.scala:36)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:257)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.MappedDStream.compute(MappedDStream.scala:36)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:257)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.ShuffledDStream.compute(ShuffledDStream.scala:41)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:257)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.ForEachDStream.generateJob(ForEachDStream.scala:48)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:117)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:116)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:251)
	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:105)
	at org.apache.spark.streaming.DStreamGraph.generateJobs(DStreamGraph.scala:116)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$3.apply(JobGenerator.scala:248)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$3.apply(JobGenerator.scala:246)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.streaming.scheduler.JobGenerator.generateJobs(JobGenerator.scala:246)
	at org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:182)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:88)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:87)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.ConnectException: Connection refused: no further information
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:547)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:642)
	at org.apache.hadoop.ipc.Client$Connection.access$2600(Client.java:314)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1399)
	at org.apache.hadoop.ipc.Client.call(Client.java:1318)
	... 91 more
[INFO]  [2018-05-21 16:05:31] [org.apache.spark.streaming.dstream.FileInputDStream]New files at time 1526889930000 ms:

[INFO]  [2018-05-21 16:05:31] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526889930000 ms
[INFO]  [2018-05-21 16:05:31] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526889930000 ms
[INFO]  [2018-05-21 16:05:31] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526889930000 ms
[INFO]  [2018-05-21 16:05:31] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526889930000 ms
[INFO]  [2018-05-21 16:05:31] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526889930000 ms to writer queue
[INFO]  [2018-05-21 16:05:31] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526889930000 ms.0 from job set of time 1526889930000 ms
[INFO]  [2018-05-21 16:05:31] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526889930000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526889930000'
[INFO]  [2018-05-21 16:05:31] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:05:31] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 18 (mapToPair at HdfsSparkStreamT.java:56)
[INFO]  [2018-05-21 16:05:31] [org.apache.spark.scheduler.DAGScheduler]Got job 6 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:05:31] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 13 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:05:31] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 12)
[INFO]  [2018-05-21 16:05:31] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:05:31] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 13 (ShuffledRDD[19] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:05:31] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_6 stored as values in memory (estimated size 3.8 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:05:31] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:05:31] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_6_piece0 in memory on 172.18.42.247:50607 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:05:31] [org.apache.spark.SparkContext]Created broadcast 6 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:05:31] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 13 (ShuffledRDD[19] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:05:31] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 13.0 with 1 tasks
[INFO]  [2018-05-21 16:05:31] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 13.0 (TID 6, localhost, partition 0, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:05:31] [org.apache.spark.executor.Executor]Running task 0.0 in stage 13.0 (TID 6)
[INFO]  [2018-05-21 16:05:31] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:05:31] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 16:05:31] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 13.0 (TID 6). 1549 bytes result sent to driver
[INFO]  [2018-05-21 16:05:31] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 13.0 (TID 6) in 8 ms on localhost (1/1)
[INFO]  [2018-05-21 16:05:31] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 13.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:05:31] [org.apache.spark.scheduler.DAGScheduler]ResultStage 13 (print at HdfsSparkStreamT.java:59) finished in 0.008 s
[INFO]  [2018-05-21 16:05:31] [org.apache.spark.scheduler.DAGScheduler]Job 6 finished: print at HdfsSparkStreamT.java:59, took 0.023240 s
[INFO]  [2018-05-21 16:05:31] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:05:31] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 3 is 83 bytes
[INFO]  [2018-05-21 16:05:31] [org.apache.spark.scheduler.DAGScheduler]Got job 7 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:05:31] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 15 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:05:31] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 14)
[INFO]  [2018-05-21 16:05:31] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:05:31] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 15 (ShuffledRDD[19] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:05:31] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_7 stored as values in memory (estimated size 3.8 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:05:31] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:05:31] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_7_piece0 in memory on 172.18.42.247:50607 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:05:31] [org.apache.spark.SparkContext]Created broadcast 7 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:05:31] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 15 (ShuffledRDD[19] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:05:31] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 15.0 with 1 tasks
[INFO]  [2018-05-21 16:05:31] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526889930000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526889930000', took 4436 bytes and 102 ms
[INFO]  [2018-05-21 16:05:31] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 15.0 (TID 7, localhost, partition 1, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:05:31] [org.apache.spark.executor.Executor]Running task 0.0 in stage 15.0 (TID 7)
[INFO]  [2018-05-21 16:05:31] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:05:31] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 16:05:31] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 15.0 (TID 7). 1549 bytes result sent to driver
[INFO]  [2018-05-21 16:05:31] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 15.0 (TID 7) in 15 ms on localhost (1/1)
[INFO]  [2018-05-21 16:05:31] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 15.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:05:31] [org.apache.spark.scheduler.DAGScheduler]ResultStage 15 (print at HdfsSparkStreamT.java:59) finished in 0.015 s
[INFO]  [2018-05-21 16:05:31] [org.apache.spark.scheduler.DAGScheduler]Job 7 finished: print at HdfsSparkStreamT.java:59, took 0.076880 s
[INFO]  [2018-05-21 16:05:31] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526889930000 ms.0 from job set of time 1526889930000 ms
[INFO]  [2018-05-21 16:05:31] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 1.150 s for time 1526889930000 ms (execution: 0.115 s)
[INFO]  [2018-05-21 16:05:31] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 14 from persistence list
[INFO]  [2018-05-21 16:05:31] [org.apache.spark.storage.BlockManager]Removing RDD 14
[INFO]  [2018-05-21 16:05:31] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 13 from persistence list
[INFO]  [2018-05-21 16:05:31] [org.apache.spark.storage.BlockManager]Removing RDD 13
[INFO]  [2018-05-21 16:05:31] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 12 from persistence list
[INFO]  [2018-05-21 16:05:31] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 11 from persistence list
[INFO]  [2018-05-21 16:05:31] [org.apache.spark.storage.BlockManager]Removing RDD 12
[INFO]  [2018-05-21 16:05:31] [org.apache.spark.storage.BlockManager]Removing RDD 11
[INFO]  [2018-05-21 16:05:31] [org.apache.spark.streaming.dstream.FileInputDStream]Cleared 0 old files that were older than 1526889870000 ms: 
[INFO]  [2018-05-21 16:05:31] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526889930000 ms
[INFO]  [2018-05-21 16:05:31] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526889930000 ms
[INFO]  [2018-05-21 16:05:31] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526889930000 ms
[INFO]  [2018-05-21 16:05:31] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526889930000 ms to writer queue
[INFO]  [2018-05-21 16:05:31] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526889930000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526889930000'
[INFO]  [2018-05-21 16:05:31] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526889930000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526889930000', took 4432 bytes and 87 ms
[INFO]  [2018-05-21 16:05:31] [org.apache.spark.streaming.DStreamGraph]Clearing checkpoint data for time 1526889930000 ms
[INFO]  [2018-05-21 16:05:31] [org.apache.spark.streaming.DStreamGraph]Cleared checkpoint data for time 1526889930000 ms
[INFO]  [2018-05-21 16:05:31] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO]  [2018-05-21 16:05:31] [org.apache.spark.streaming.util.FileBasedWriteAheadLog_ReceivedBlockTracker]Attempting to clear 0 old log files in hdfs://hadoop1:9000/lib/sparkstreaming/check_point/receivedBlockMetadata older than 1526889870000: 
[INFO]  [2018-05-21 16:05:31] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 
[WARN]  [2018-05-21 16:05:46] [org.apache.spark.streaming.dstream.FileInputDStream]Error finding new files
java.net.ConnectException: Call From hadoop/172.18.42.247 to hadoop:9000 failed on connection exception: java.net.ConnectException: Connection refused: no further information; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1351)
	at org.apache.hadoop.ipc.Client.call(Client.java:1300)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.getFileInfo(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:651)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1679)
	at org.apache.hadoop.hdfs.DistributedFileSystem$17.doCall(DistributedFileSystem.java:1106)
	at org.apache.hadoop.hdfs.DistributedFileSystem$17.doCall(DistributedFileSystem.java:1102)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1102)
	at org.apache.hadoop.fs.FileSystem.globStatusInternal(FileSystem.java:1701)
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1647)
	at org.apache.spark.streaming.dstream.FileInputDStream.findNewFiles(FileInputDStream.scala:205)
	at org.apache.spark.streaming.dstream.FileInputDStream.compute(FileInputDStream.scala:149)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:257)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.MappedDStream.compute(MappedDStream.scala:36)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:257)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.FlatMappedDStream.compute(FlatMappedDStream.scala:36)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:257)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.MappedDStream.compute(MappedDStream.scala:36)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:257)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.ShuffledDStream.compute(ShuffledDStream.scala:41)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:257)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.ForEachDStream.generateJob(ForEachDStream.scala:48)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:117)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:116)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:251)
	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:105)
	at org.apache.spark.streaming.DStreamGraph.generateJobs(DStreamGraph.scala:116)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$3.apply(JobGenerator.scala:248)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$3.apply(JobGenerator.scala:246)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.streaming.scheduler.JobGenerator.generateJobs(JobGenerator.scala:246)
	at org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:182)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:88)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:87)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.ConnectException: Connection refused: no further information
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:547)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:642)
	at org.apache.hadoop.ipc.Client$Connection.access$2600(Client.java:314)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1399)
	at org.apache.hadoop.ipc.Client.call(Client.java:1318)
	... 91 more
[INFO]  [2018-05-21 16:05:46] [org.apache.spark.streaming.dstream.FileInputDStream]New files at time 1526889945000 ms:

[INFO]  [2018-05-21 16:05:46] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526889945000 ms
[INFO]  [2018-05-21 16:05:46] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526889945000 ms
[INFO]  [2018-05-21 16:05:46] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526889945000 ms
[INFO]  [2018-05-21 16:05:46] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526889945000 ms
[INFO]  [2018-05-21 16:05:46] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526889945000 ms to writer queue
[INFO]  [2018-05-21 16:05:46] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:05:46] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526889945000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526889945000'
[INFO]  [2018-05-21 16:05:46] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526889945000 ms.0 from job set of time 1526889945000 ms
[INFO]  [2018-05-21 16:05:46] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 23 (mapToPair at HdfsSparkStreamT.java:56)
[INFO]  [2018-05-21 16:05:46] [org.apache.spark.scheduler.DAGScheduler]Got job 8 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:05:46] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 17 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:05:46] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 16)
[INFO]  [2018-05-21 16:05:46] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:05:46] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 17 (ShuffledRDD[24] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:05:46] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_8 stored as values in memory (estimated size 3.8 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:05:46] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_8_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:05:46] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_8_piece0 in memory on 172.18.42.247:50607 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:05:46] [org.apache.spark.SparkContext]Created broadcast 8 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:05:46] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 17 (ShuffledRDD[24] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:05:46] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 17.0 with 1 tasks
[INFO]  [2018-05-21 16:05:46] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 17.0 (TID 8, localhost, partition 0, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:05:46] [org.apache.spark.executor.Executor]Running task 0.0 in stage 17.0 (TID 8)
[INFO]  [2018-05-21 16:05:46] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:05:46] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 16:05:46] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 17.0 (TID 8). 1549 bytes result sent to driver
[INFO]  [2018-05-21 16:05:46] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 17.0 (TID 8) in 24 ms on localhost (1/1)
[INFO]  [2018-05-21 16:05:46] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 17.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:05:46] [org.apache.spark.scheduler.DAGScheduler]ResultStage 17 (print at HdfsSparkStreamT.java:59) finished in 0.026 s
[INFO]  [2018-05-21 16:05:46] [org.apache.spark.scheduler.DAGScheduler]Job 8 finished: print at HdfsSparkStreamT.java:59, took 0.049511 s
[INFO]  [2018-05-21 16:05:46] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:05:46] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 4 is 83 bytes
[INFO]  [2018-05-21 16:05:46] [org.apache.spark.scheduler.DAGScheduler]Got job 9 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:05:46] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 19 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:05:46] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 18)
[INFO]  [2018-05-21 16:05:46] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:05:46] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 19 (ShuffledRDD[24] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:05:46] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_9 stored as values in memory (estimated size 3.8 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:05:46] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_9_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:05:46] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_9_piece0 in memory on 172.18.42.247:50607 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:05:46] [org.apache.spark.SparkContext]Created broadcast 9 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:05:46] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 19 (ShuffledRDD[24] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:05:46] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 19.0 with 1 tasks
[INFO]  [2018-05-21 16:05:46] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 19.0 (TID 9, localhost, partition 1, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:05:46] [org.apache.spark.executor.Executor]Running task 0.0 in stage 19.0 (TID 9)
[INFO]  [2018-05-21 16:05:46] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:05:46] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 16:05:46] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 19.0 (TID 9). 1549 bytes result sent to driver
[INFO]  [2018-05-21 16:05:46] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 19.0 (TID 9) in 5 ms on localhost (1/1)
[INFO]  [2018-05-21 16:05:46] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 19.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:05:46] [org.apache.spark.scheduler.DAGScheduler]ResultStage 19 (print at HdfsSparkStreamT.java:59) finished in 0.006 s
[INFO]  [2018-05-21 16:05:46] [org.apache.spark.scheduler.DAGScheduler]Job 9 finished: print at HdfsSparkStreamT.java:59, took 0.050484 s
[INFO]  [2018-05-21 16:05:46] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526889945000 ms.0 from job set of time 1526889945000 ms
[INFO]  [2018-05-21 16:05:46] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 19 from persistence list
[INFO]  [2018-05-21 16:05:46] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 1.152 s for time 1526889945000 ms (execution: 0.107 s)
[INFO]  [2018-05-21 16:05:46] [org.apache.spark.storage.BlockManager]Removing RDD 19
[INFO]  [2018-05-21 16:05:46] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 18 from persistence list
[INFO]  [2018-05-21 16:05:46] [org.apache.spark.storage.BlockManager]Removing RDD 18
[INFO]  [2018-05-21 16:05:46] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 17 from persistence list
[INFO]  [2018-05-21 16:05:46] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 16 from persistence list
[INFO]  [2018-05-21 16:05:46] [org.apache.spark.storage.BlockManager]Removing RDD 16
[INFO]  [2018-05-21 16:05:46] [org.apache.spark.storage.BlockManager]Removing RDD 17
[INFO]  [2018-05-21 16:05:46] [org.apache.spark.streaming.dstream.FileInputDStream]Cleared 0 old files that were older than 1526889885000 ms: 
[INFO]  [2018-05-21 16:05:46] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526889945000 ms
[INFO]  [2018-05-21 16:05:46] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526889945000 ms
[INFO]  [2018-05-21 16:05:46] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526889945000 ms
[INFO]  [2018-05-21 16:05:46] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526889945000 ms to writer queue
[INFO]  [2018-05-21 16:05:46] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526889945000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526889945000', took 4442 bytes and 142 ms
[INFO]  [2018-05-21 16:05:46] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526889945000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526889945000'
[INFO]  [2018-05-21 16:05:46] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526889945000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526889945000', took 4438 bytes and 61 ms
[INFO]  [2018-05-21 16:05:46] [org.apache.spark.streaming.DStreamGraph]Clearing checkpoint data for time 1526889945000 ms
[INFO]  [2018-05-21 16:05:46] [org.apache.spark.streaming.DStreamGraph]Cleared checkpoint data for time 1526889945000 ms
[INFO]  [2018-05-21 16:05:46] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO]  [2018-05-21 16:05:46] [org.apache.spark.streaming.util.FileBasedWriteAheadLog_ReceivedBlockTracker]Attempting to clear 0 old log files in hdfs://hadoop1:9000/lib/sparkstreaming/check_point/receivedBlockMetadata older than 1526889885000: 
[INFO]  [2018-05-21 16:05:46] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 
[WARN]  [2018-05-21 16:06:01] [org.apache.spark.streaming.dstream.FileInputDStream]Error finding new files
java.net.ConnectException: Call From hadoop/172.18.42.247 to hadoop:9000 failed on connection exception: java.net.ConnectException: Connection refused: no further information; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1351)
	at org.apache.hadoop.ipc.Client.call(Client.java:1300)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.getFileInfo(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:651)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1679)
	at org.apache.hadoop.hdfs.DistributedFileSystem$17.doCall(DistributedFileSystem.java:1106)
	at org.apache.hadoop.hdfs.DistributedFileSystem$17.doCall(DistributedFileSystem.java:1102)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1102)
	at org.apache.hadoop.fs.FileSystem.globStatusInternal(FileSystem.java:1701)
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1647)
	at org.apache.spark.streaming.dstream.FileInputDStream.findNewFiles(FileInputDStream.scala:205)
	at org.apache.spark.streaming.dstream.FileInputDStream.compute(FileInputDStream.scala:149)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:257)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.MappedDStream.compute(MappedDStream.scala:36)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:257)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.FlatMappedDStream.compute(FlatMappedDStream.scala:36)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:257)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.MappedDStream.compute(MappedDStream.scala:36)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:257)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.ShuffledDStream.compute(ShuffledDStream.scala:41)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:257)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.ForEachDStream.generateJob(ForEachDStream.scala:48)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:117)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:116)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:251)
	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:105)
	at org.apache.spark.streaming.DStreamGraph.generateJobs(DStreamGraph.scala:116)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$3.apply(JobGenerator.scala:248)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$3.apply(JobGenerator.scala:246)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.streaming.scheduler.JobGenerator.generateJobs(JobGenerator.scala:246)
	at org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:182)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:88)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:87)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.ConnectException: Connection refused: no further information
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:547)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:642)
	at org.apache.hadoop.ipc.Client$Connection.access$2600(Client.java:314)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1399)
	at org.apache.hadoop.ipc.Client.call(Client.java:1318)
	... 91 more
[INFO]  [2018-05-21 16:06:01] [org.apache.spark.streaming.dstream.FileInputDStream]New files at time 1526889960000 ms:

[INFO]  [2018-05-21 16:06:01] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526889960000 ms
[INFO]  [2018-05-21 16:06:01] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526889960000 ms
[INFO]  [2018-05-21 16:06:01] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526889960000 ms
[INFO]  [2018-05-21 16:06:01] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526889960000 ms
[INFO]  [2018-05-21 16:06:01] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526889960000 ms to writer queue
[INFO]  [2018-05-21 16:06:01] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526889960000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526889960000'
[INFO]  [2018-05-21 16:06:01] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526889960000 ms.0 from job set of time 1526889960000 ms
[INFO]  [2018-05-21 16:06:01] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:06:01] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 28 (mapToPair at HdfsSparkStreamT.java:56)
[INFO]  [2018-05-21 16:06:01] [org.apache.spark.scheduler.DAGScheduler]Got job 10 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:06:01] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 21 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:06:01] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 20)
[INFO]  [2018-05-21 16:06:01] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:06:01] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 21 (ShuffledRDD[29] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:06:01] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_10 stored as values in memory (estimated size 3.8 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:06:01] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_10_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:06:01] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_10_piece0 in memory on 172.18.42.247:50607 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:06:01] [org.apache.spark.SparkContext]Created broadcast 10 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:06:01] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 21 (ShuffledRDD[29] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:06:01] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 21.0 with 1 tasks
[INFO]  [2018-05-21 16:06:01] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 21.0 (TID 10, localhost, partition 0, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:06:01] [org.apache.spark.executor.Executor]Running task 0.0 in stage 21.0 (TID 10)
[INFO]  [2018-05-21 16:06:01] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:06:01] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 16:06:01] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 21.0 (TID 10). 1470 bytes result sent to driver
[INFO]  [2018-05-21 16:06:01] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 21.0 (TID 10) in 12 ms on localhost (1/1)
[INFO]  [2018-05-21 16:06:01] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 21.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:06:01] [org.apache.spark.scheduler.DAGScheduler]ResultStage 21 (print at HdfsSparkStreamT.java:59) finished in 0.012 s
[INFO]  [2018-05-21 16:06:01] [org.apache.spark.scheduler.DAGScheduler]Job 10 finished: print at HdfsSparkStreamT.java:59, took 0.026226 s
[INFO]  [2018-05-21 16:06:01] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:06:01] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 5 is 83 bytes
[INFO]  [2018-05-21 16:06:01] [org.apache.spark.scheduler.DAGScheduler]Got job 11 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:06:01] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 23 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:06:01] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 22)
[INFO]  [2018-05-21 16:06:01] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:06:01] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 23 (ShuffledRDD[29] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:06:01] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_11 stored as values in memory (estimated size 3.8 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:06:01] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_11_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:06:01] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_11_piece0 in memory on 172.18.42.247:50607 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:06:01] [org.apache.spark.SparkContext]Created broadcast 11 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:06:01] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 23 (ShuffledRDD[29] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:06:01] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 23.0 with 1 tasks
[INFO]  [2018-05-21 16:06:01] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 23.0 (TID 11, localhost, partition 1, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:06:01] [org.apache.spark.executor.Executor]Running task 0.0 in stage 23.0 (TID 11)
[INFO]  [2018-05-21 16:06:01] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:06:01] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 16:06:01] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 23.0 (TID 11). 1470 bytes result sent to driver
[INFO]  [2018-05-21 16:06:01] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 23.0 (TID 11) in 10 ms on localhost (1/1)
[INFO]  [2018-05-21 16:06:01] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 23.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:06:01] [org.apache.spark.scheduler.DAGScheduler]ResultStage 23 (print at HdfsSparkStreamT.java:59) finished in 0.013 s
[INFO]  [2018-05-21 16:06:01] [org.apache.spark.scheduler.DAGScheduler]Job 11 finished: print at HdfsSparkStreamT.java:59, took 0.027045 s
[INFO]  [2018-05-21 16:06:01] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526889960000 ms.0 from job set of time 1526889960000 ms
[INFO]  [2018-05-21 16:06:01] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 1.096 s for time 1526889960000 ms (execution: 0.063 s)
[INFO]  [2018-05-21 16:06:01] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 24 from persistence list
[INFO]  [2018-05-21 16:06:01] [org.apache.spark.storage.BlockManager]Removing RDD 24
[INFO]  [2018-05-21 16:06:01] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 23 from persistence list
[INFO]  [2018-05-21 16:06:01] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 22 from persistence list
[INFO]  [2018-05-21 16:06:01] [org.apache.spark.storage.BlockManager]Removing RDD 23
[INFO]  [2018-05-21 16:06:01] [org.apache.spark.storage.BlockManager]Removing RDD 22
[INFO]  [2018-05-21 16:06:01] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 21 from persistence list
[INFO]  [2018-05-21 16:06:01] [org.apache.spark.streaming.dstream.FileInputDStream]Cleared 1 old files that were older than 1526889900000 ms: 1526889885000 ms
[INFO]  [2018-05-21 16:06:01] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526889960000 ms
[INFO]  [2018-05-21 16:06:01] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526889960000 ms
[INFO]  [2018-05-21 16:06:01] [org.apache.spark.storage.BlockManager]Removing RDD 21
[INFO]  [2018-05-21 16:06:01] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526889960000 ms
[INFO]  [2018-05-21 16:06:01] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526889960000 ms to writer queue
[INFO]  [2018-05-21 16:06:01] [org.apache.spark.streaming.CheckpointWriter]Deleting hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526889885000.bk
[INFO]  [2018-05-21 16:06:01] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526889960000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526889960000', took 4436 bytes and 335 ms
[INFO]  [2018-05-21 16:06:01] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526889960000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526889960000'
[INFO]  [2018-05-21 16:06:01] [org.apache.spark.streaming.CheckpointWriter]Deleting hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526889885000
[INFO]  [2018-05-21 16:06:01] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526889960000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526889960000', took 4430 bytes and 122 ms
[INFO]  [2018-05-21 16:06:01] [org.apache.spark.streaming.DStreamGraph]Clearing checkpoint data for time 1526889960000 ms
[INFO]  [2018-05-21 16:06:01] [org.apache.spark.streaming.DStreamGraph]Cleared checkpoint data for time 1526889960000 ms
[INFO]  [2018-05-21 16:06:01] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO]  [2018-05-21 16:06:01] [org.apache.spark.streaming.util.FileBasedWriteAheadLog_ReceivedBlockTracker]Attempting to clear 0 old log files in hdfs://hadoop1:9000/lib/sparkstreaming/check_point/receivedBlockMetadata older than 1526889900000: 
[INFO]  [2018-05-21 16:06:01] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1526889885000 ms
[WARN]  [2018-05-21 16:06:16] [org.apache.spark.streaming.dstream.FileInputDStream]Error finding new files
java.net.ConnectException: Call From hadoop/172.18.42.247 to hadoop:9000 failed on connection exception: java.net.ConnectException: Connection refused: no further information; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1351)
	at org.apache.hadoop.ipc.Client.call(Client.java:1300)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.getFileInfo(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:651)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1679)
	at org.apache.hadoop.hdfs.DistributedFileSystem$17.doCall(DistributedFileSystem.java:1106)
	at org.apache.hadoop.hdfs.DistributedFileSystem$17.doCall(DistributedFileSystem.java:1102)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1102)
	at org.apache.hadoop.fs.FileSystem.globStatusInternal(FileSystem.java:1701)
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1647)
	at org.apache.spark.streaming.dstream.FileInputDStream.findNewFiles(FileInputDStream.scala:205)
	at org.apache.spark.streaming.dstream.FileInputDStream.compute(FileInputDStream.scala:149)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:257)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.MappedDStream.compute(MappedDStream.scala:36)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:257)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.FlatMappedDStream.compute(FlatMappedDStream.scala:36)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:257)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.MappedDStream.compute(MappedDStream.scala:36)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:257)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.ShuffledDStream.compute(ShuffledDStream.scala:41)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:257)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.ForEachDStream.generateJob(ForEachDStream.scala:48)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:117)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:116)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:251)
	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:105)
	at org.apache.spark.streaming.DStreamGraph.generateJobs(DStreamGraph.scala:116)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$3.apply(JobGenerator.scala:248)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$3.apply(JobGenerator.scala:246)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.streaming.scheduler.JobGenerator.generateJobs(JobGenerator.scala:246)
	at org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:182)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:88)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:87)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.ConnectException: Connection refused: no further information
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:547)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:642)
	at org.apache.hadoop.ipc.Client$Connection.access$2600(Client.java:314)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1399)
	at org.apache.hadoop.ipc.Client.call(Client.java:1318)
	... 91 more
[INFO]  [2018-05-21 16:06:16] [org.apache.spark.streaming.dstream.FileInputDStream]New files at time 1526889975000 ms:

[INFO]  [2018-05-21 16:06:16] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526889975000 ms
[INFO]  [2018-05-21 16:06:16] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526889975000 ms
[INFO]  [2018-05-21 16:06:16] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526889975000 ms
[INFO]  [2018-05-21 16:06:16] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526889975000 ms
[INFO]  [2018-05-21 16:06:16] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526889975000 ms to writer queue
[INFO]  [2018-05-21 16:06:16] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526889975000 ms.0 from job set of time 1526889975000 ms
[INFO]  [2018-05-21 16:06:16] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526889975000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526889975000'
[INFO]  [2018-05-21 16:06:16] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:06:16] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 33 (mapToPair at HdfsSparkStreamT.java:56)
[INFO]  [2018-05-21 16:06:16] [org.apache.spark.scheduler.DAGScheduler]Got job 12 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:06:16] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 25 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:06:16] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 24)
[INFO]  [2018-05-21 16:06:16] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:06:16] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 25 (ShuffledRDD[34] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:06:16] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_12 stored as values in memory (estimated size 3.8 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:06:16] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_12_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:06:16] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_12_piece0 in memory on 172.18.42.247:50607 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:06:16] [org.apache.spark.SparkContext]Created broadcast 12 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:06:16] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 25 (ShuffledRDD[34] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:06:16] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 25.0 with 1 tasks
[INFO]  [2018-05-21 16:06:16] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 25.0 (TID 12, localhost, partition 0, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:06:16] [org.apache.spark.executor.Executor]Running task 0.0 in stage 25.0 (TID 12)
[INFO]  [2018-05-21 16:06:16] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:06:16] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 16:06:16] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 25.0 (TID 12). 1462 bytes result sent to driver
[INFO]  [2018-05-21 16:06:16] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 25.0 (TID 12) in 5 ms on localhost (1/1)
[INFO]  [2018-05-21 16:06:16] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 25.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:06:16] [org.apache.spark.scheduler.DAGScheduler]ResultStage 25 (print at HdfsSparkStreamT.java:59) finished in 0.007 s
[INFO]  [2018-05-21 16:06:16] [org.apache.spark.scheduler.DAGScheduler]Job 12 finished: print at HdfsSparkStreamT.java:59, took 0.019812 s
[INFO]  [2018-05-21 16:06:16] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:06:16] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 6 is 83 bytes
[INFO]  [2018-05-21 16:06:16] [org.apache.spark.scheduler.DAGScheduler]Got job 13 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:06:16] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 27 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:06:16] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 26)
[INFO]  [2018-05-21 16:06:16] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:06:16] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 27 (ShuffledRDD[34] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:06:16] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_13 stored as values in memory (estimated size 3.8 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:06:16] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_13_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:06:16] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_13_piece0 in memory on 172.18.42.247:50607 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:06:16] [org.apache.spark.SparkContext]Created broadcast 13 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:06:16] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 27 (ShuffledRDD[34] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:06:16] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 27.0 with 1 tasks
[INFO]  [2018-05-21 16:06:16] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 27.0 (TID 13, localhost, partition 1, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:06:16] [org.apache.spark.executor.Executor]Running task 0.0 in stage 27.0 (TID 13)
[INFO]  [2018-05-21 16:06:16] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:06:16] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 16:06:16] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 27.0 (TID 13). 1470 bytes result sent to driver
[INFO]  [2018-05-21 16:06:16] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 27.0 (TID 13) in 8 ms on localhost (1/1)
[INFO]  [2018-05-21 16:06:16] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 27.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:06:16] [org.apache.spark.scheduler.DAGScheduler]ResultStage 27 (print at HdfsSparkStreamT.java:59) finished in 0.017 s
[INFO]  [2018-05-21 16:06:16] [org.apache.spark.scheduler.DAGScheduler]Job 13 finished: print at HdfsSparkStreamT.java:59, took 0.194582 s
[INFO]  [2018-05-21 16:06:16] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526889975000 ms.0 from job set of time 1526889975000 ms
[INFO]  [2018-05-21 16:06:16] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 1.254 s for time 1526889975000 ms (execution: 0.225 s)
[INFO]  [2018-05-21 16:06:16] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 29 from persistence list
[INFO]  [2018-05-21 16:06:16] [org.apache.spark.storage.BlockManager]Removing RDD 29
[INFO]  [2018-05-21 16:06:16] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 28 from persistence list
[INFO]  [2018-05-21 16:06:16] [org.apache.spark.storage.BlockManager]Removing RDD 28
[INFO]  [2018-05-21 16:06:16] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 27 from persistence list
[INFO]  [2018-05-21 16:06:16] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 26 from persistence list
[INFO]  [2018-05-21 16:06:16] [org.apache.spark.storage.BlockManager]Removing RDD 27
[INFO]  [2018-05-21 16:06:16] [org.apache.spark.streaming.dstream.FileInputDStream]Cleared 1 old files that were older than 1526889915000 ms: 1526889900000 ms
[INFO]  [2018-05-21 16:06:16] [org.apache.spark.storage.BlockManager]Removing RDD 26
[INFO]  [2018-05-21 16:06:16] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526889975000 ms
[INFO]  [2018-05-21 16:06:16] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526889975000 ms
[INFO]  [2018-05-21 16:06:16] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526889975000 ms
[INFO]  [2018-05-21 16:06:16] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526889975000 ms to writer queue
[INFO]  [2018-05-21 16:06:16] [org.apache.spark.streaming.CheckpointWriter]Deleting hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526889900000.bk
[INFO]  [2018-05-21 16:06:16] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526889975000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526889975000', took 4430 bytes and 258 ms
[INFO]  [2018-05-21 16:06:16] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526889975000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526889975000'
[INFO]  [2018-05-21 16:06:16] [org.apache.spark.streaming.CheckpointWriter]Deleting hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526889900000
[INFO]  [2018-05-21 16:06:16] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526889975000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526889975000', took 4429 bytes and 75 ms
[INFO]  [2018-05-21 16:06:16] [org.apache.spark.streaming.DStreamGraph]Clearing checkpoint data for time 1526889975000 ms
[INFO]  [2018-05-21 16:06:16] [org.apache.spark.streaming.DStreamGraph]Cleared checkpoint data for time 1526889975000 ms
[INFO]  [2018-05-21 16:06:16] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO]  [2018-05-21 16:06:16] [org.apache.spark.streaming.util.FileBasedWriteAheadLog_ReceivedBlockTracker]Attempting to clear 0 old log files in hdfs://hadoop1:9000/lib/sparkstreaming/check_point/receivedBlockMetadata older than 1526889915000: 
[INFO]  [2018-05-21 16:06:16] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1526889900000 ms
[WARN]  [2018-05-21 16:06:31] [org.apache.spark.streaming.dstream.FileInputDStream]Error finding new files
java.net.ConnectException: Call From hadoop/172.18.42.247 to hadoop:9000 failed on connection exception: java.net.ConnectException: Connection refused: no further information; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1351)
	at org.apache.hadoop.ipc.Client.call(Client.java:1300)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.getFileInfo(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:651)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1679)
	at org.apache.hadoop.hdfs.DistributedFileSystem$17.doCall(DistributedFileSystem.java:1106)
	at org.apache.hadoop.hdfs.DistributedFileSystem$17.doCall(DistributedFileSystem.java:1102)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1102)
	at org.apache.hadoop.fs.FileSystem.globStatusInternal(FileSystem.java:1701)
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1647)
	at org.apache.spark.streaming.dstream.FileInputDStream.findNewFiles(FileInputDStream.scala:205)
	at org.apache.spark.streaming.dstream.FileInputDStream.compute(FileInputDStream.scala:149)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:257)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.MappedDStream.compute(MappedDStream.scala:36)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:257)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.FlatMappedDStream.compute(FlatMappedDStream.scala:36)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:257)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.MappedDStream.compute(MappedDStream.scala:36)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:257)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.ShuffledDStream.compute(ShuffledDStream.scala:41)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:257)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.ForEachDStream.generateJob(ForEachDStream.scala:48)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:117)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:116)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:251)
	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:105)
	at org.apache.spark.streaming.DStreamGraph.generateJobs(DStreamGraph.scala:116)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$3.apply(JobGenerator.scala:248)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$3.apply(JobGenerator.scala:246)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.streaming.scheduler.JobGenerator.generateJobs(JobGenerator.scala:246)
	at org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:182)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:88)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:87)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.ConnectException: Connection refused: no further information
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:547)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:642)
	at org.apache.hadoop.ipc.Client$Connection.access$2600(Client.java:314)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1399)
	at org.apache.hadoop.ipc.Client.call(Client.java:1318)
	... 91 more
[INFO]  [2018-05-21 16:06:31] [org.apache.spark.streaming.dstream.FileInputDStream]New files at time 1526889990000 ms:

[INFO]  [2018-05-21 16:06:31] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526889990000 ms
[INFO]  [2018-05-21 16:06:31] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526889990000 ms
[INFO]  [2018-05-21 16:06:31] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526889990000 ms
[INFO]  [2018-05-21 16:06:31] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526889990000 ms
[INFO]  [2018-05-21 16:06:31] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526889990000 ms to writer queue
[INFO]  [2018-05-21 16:06:31] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526889990000 ms.0 from job set of time 1526889990000 ms
[INFO]  [2018-05-21 16:06:31] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526889990000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526889990000'
[INFO]  [2018-05-21 16:06:31] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:06:31] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 38 (mapToPair at HdfsSparkStreamT.java:56)
[INFO]  [2018-05-21 16:06:31] [org.apache.spark.scheduler.DAGScheduler]Got job 14 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:06:31] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 29 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:06:31] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 28)
[INFO]  [2018-05-21 16:06:31] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:06:31] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 29 (ShuffledRDD[39] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:06:31] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_14 stored as values in memory (estimated size 3.8 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:06:31] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_14_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:06:31] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_14_piece0 in memory on 172.18.42.247:50607 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:06:31] [org.apache.spark.SparkContext]Created broadcast 14 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:06:31] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 29 (ShuffledRDD[39] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:06:31] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 29.0 with 1 tasks
[INFO]  [2018-05-21 16:06:31] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 29.0 (TID 14, localhost, partition 0, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:06:31] [org.apache.spark.executor.Executor]Running task 0.0 in stage 29.0 (TID 14)
[INFO]  [2018-05-21 16:06:31] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:06:31] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 16:06:31] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 29.0 (TID 14). 1549 bytes result sent to driver
[INFO]  [2018-05-21 16:06:31] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 29.0 (TID 14) in 12 ms on localhost (1/1)
[INFO]  [2018-05-21 16:06:31] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 29.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:06:31] [org.apache.spark.scheduler.DAGScheduler]ResultStage 29 (print at HdfsSparkStreamT.java:59) finished in 0.012 s
[INFO]  [2018-05-21 16:06:31] [org.apache.spark.scheduler.DAGScheduler]Job 14 finished: print at HdfsSparkStreamT.java:59, took 0.030020 s
[INFO]  [2018-05-21 16:06:31] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:06:31] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 7 is 83 bytes
[INFO]  [2018-05-21 16:06:31] [org.apache.spark.scheduler.DAGScheduler]Got job 15 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:06:31] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 31 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:06:31] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 30)
[INFO]  [2018-05-21 16:06:31] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:06:31] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 31 (ShuffledRDD[39] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:06:31] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_15 stored as values in memory (estimated size 3.8 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:06:31] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_15_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:06:31] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_15_piece0 in memory on 172.18.42.247:50607 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:06:31] [org.apache.spark.SparkContext]Created broadcast 15 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:06:31] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 31 (ShuffledRDD[39] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:06:31] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 31.0 with 1 tasks
[INFO]  [2018-05-21 16:06:31] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 31.0 (TID 15, localhost, partition 1, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:06:31] [org.apache.spark.executor.Executor]Running task 0.0 in stage 31.0 (TID 15)
[INFO]  [2018-05-21 16:06:31] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:06:31] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 16:06:31] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 31.0 (TID 15). 1549 bytes result sent to driver
[INFO]  [2018-05-21 16:06:31] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 31.0 (TID 15) in 10 ms on localhost (1/1)
[INFO]  [2018-05-21 16:06:31] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 31.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:06:31] [org.apache.spark.scheduler.DAGScheduler]ResultStage 31 (print at HdfsSparkStreamT.java:59) finished in 0.010 s
[INFO]  [2018-05-21 16:06:31] [org.apache.spark.scheduler.DAGScheduler]Job 15 finished: print at HdfsSparkStreamT.java:59, took 0.018297 s
[INFO]  [2018-05-21 16:06:31] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526889990000 ms.0 from job set of time 1526889990000 ms
[INFO]  [2018-05-21 16:06:31] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 1.095 s for time 1526889990000 ms (execution: 0.065 s)
[INFO]  [2018-05-21 16:06:31] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 34 from persistence list
[INFO]  [2018-05-21 16:06:31] [org.apache.spark.storage.BlockManager]Removing RDD 34
[INFO]  [2018-05-21 16:06:31] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 33 from persistence list
[INFO]  [2018-05-21 16:06:31] [org.apache.spark.storage.BlockManager]Removing RDD 33
[INFO]  [2018-05-21 16:06:31] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 32 from persistence list
[INFO]  [2018-05-21 16:06:31] [org.apache.spark.storage.BlockManager]Removing RDD 32
[INFO]  [2018-05-21 16:06:31] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 31 from persistence list
[INFO]  [2018-05-21 16:06:31] [org.apache.spark.storage.BlockManager]Removing RDD 31
[INFO]  [2018-05-21 16:06:31] [org.apache.spark.streaming.dstream.FileInputDStream]Cleared 1 old files that were older than 1526889930000 ms: 1526889915000 ms
[INFO]  [2018-05-21 16:06:31] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526889990000 ms
[INFO]  [2018-05-21 16:06:31] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526889990000 ms
[INFO]  [2018-05-21 16:06:31] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526889990000 ms
[INFO]  [2018-05-21 16:06:31] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526889990000 ms to writer queue
[INFO]  [2018-05-21 16:06:31] [org.apache.spark.streaming.CheckpointWriter]Deleting hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526889915000.bk
[INFO]  [2018-05-21 16:06:31] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526889990000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526889990000', took 4435 bytes and 302 ms
[INFO]  [2018-05-21 16:06:31] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526889990000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526889990000'
[INFO]  [2018-05-21 16:06:31] [org.apache.spark.streaming.CheckpointWriter]Deleting hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526889915000
[INFO]  [2018-05-21 16:06:31] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526889990000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526889990000', took 4434 bytes and 88 ms
[INFO]  [2018-05-21 16:06:31] [org.apache.spark.streaming.DStreamGraph]Clearing checkpoint data for time 1526889990000 ms
[INFO]  [2018-05-21 16:06:31] [org.apache.spark.streaming.DStreamGraph]Cleared checkpoint data for time 1526889990000 ms
[INFO]  [2018-05-21 16:06:31] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO]  [2018-05-21 16:06:31] [org.apache.spark.streaming.util.FileBasedWriteAheadLog_ReceivedBlockTracker]Attempting to clear 0 old log files in hdfs://hadoop1:9000/lib/sparkstreaming/check_point/receivedBlockMetadata older than 1526889930000: 
[INFO]  [2018-05-21 16:06:31] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1526889915000 ms
[WARN]  [2018-05-21 16:06:46] [org.apache.spark.streaming.dstream.FileInputDStream]Error finding new files
java.net.ConnectException: Call From hadoop/172.18.42.247 to hadoop:9000 failed on connection exception: java.net.ConnectException: Connection refused: no further information; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1351)
	at org.apache.hadoop.ipc.Client.call(Client.java:1300)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.getFileInfo(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:651)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1679)
	at org.apache.hadoop.hdfs.DistributedFileSystem$17.doCall(DistributedFileSystem.java:1106)
	at org.apache.hadoop.hdfs.DistributedFileSystem$17.doCall(DistributedFileSystem.java:1102)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1102)
	at org.apache.hadoop.fs.FileSystem.globStatusInternal(FileSystem.java:1701)
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1647)
	at org.apache.spark.streaming.dstream.FileInputDStream.findNewFiles(FileInputDStream.scala:205)
	at org.apache.spark.streaming.dstream.FileInputDStream.compute(FileInputDStream.scala:149)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:257)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.MappedDStream.compute(MappedDStream.scala:36)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:257)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.FlatMappedDStream.compute(FlatMappedDStream.scala:36)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:257)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.MappedDStream.compute(MappedDStream.scala:36)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:257)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.ShuffledDStream.compute(ShuffledDStream.scala:41)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:257)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.ForEachDStream.generateJob(ForEachDStream.scala:48)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:117)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:116)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:251)
	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:105)
	at org.apache.spark.streaming.DStreamGraph.generateJobs(DStreamGraph.scala:116)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$3.apply(JobGenerator.scala:248)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$3.apply(JobGenerator.scala:246)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.streaming.scheduler.JobGenerator.generateJobs(JobGenerator.scala:246)
	at org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:182)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:88)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:87)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.ConnectException: Connection refused: no further information
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:547)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:642)
	at org.apache.hadoop.ipc.Client$Connection.access$2600(Client.java:314)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1399)
	at org.apache.hadoop.ipc.Client.call(Client.java:1318)
	... 91 more
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.streaming.dstream.FileInputDStream]New files at time 1526890005000 ms:

[INFO]  [2018-05-21 16:06:46] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526890005000 ms
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526890005000 ms
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526890005000 ms
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526890005000 ms
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526890005000 ms to writer queue
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526890005000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890005000'
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526890005000 ms.0 from job set of time 1526890005000 ms
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.ContextCleaner]Cleaned shuffle 0
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.ContextCleaner]Cleaned shuffle 1
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 43 (mapToPair at HdfsSparkStreamT.java:56)
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.scheduler.DAGScheduler]Got job 16 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 33 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 32)
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 33 (ShuffledRDD[44] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_16 stored as values in memory (estimated size 3.8 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_16_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_16_piece0 in memory on 172.18.42.247:50607 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_2_piece0 on 172.18.42.247:50607 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.SparkContext]Created broadcast 16 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 33 (ShuffledRDD[44] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 33.0 with 1 tasks
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 33.0 (TID 16, localhost, partition 0, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_3_piece0 on 172.18.42.247:50607 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.executor.Executor]Running task 0.0 in stage 33.0 (TID 16)
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 33.0 (TID 16). 1470 bytes result sent to driver
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.ContextCleaner]Cleaned shuffle 2
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 33.0 (TID 16) in 10 ms on localhost (1/1)
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 33.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_4_piece0 on 172.18.42.247:50607 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.scheduler.DAGScheduler]ResultStage 33 (print at HdfsSparkStreamT.java:59) finished in 0.013 s
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.scheduler.DAGScheduler]Job 16 finished: print at HdfsSparkStreamT.java:59, took 0.028352 s
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 8 is 83 bytes
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.scheduler.DAGScheduler]Got job 17 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 35 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 34)
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 35 (ShuffledRDD[44] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_17 stored as values in memory (estimated size 3.8 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_17_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_17_piece0 in memory on 172.18.42.247:50607 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.SparkContext]Created broadcast 17 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 35 (ShuffledRDD[44] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 35.0 with 1 tasks
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 35.0 (TID 17, localhost, partition 1, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.executor.Executor]Running task 0.0 in stage 35.0 (TID 17)
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 35.0 (TID 17). 1470 bytes result sent to driver
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_5_piece0 on 172.18.42.247:50607 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 35.0 (TID 17) in 13 ms on localhost (1/1)
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 35.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.scheduler.DAGScheduler]ResultStage 35 (print at HdfsSparkStreamT.java:59) finished in 0.015 s
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.scheduler.DAGScheduler]Job 17 finished: print at HdfsSparkStreamT.java:59, took 0.020459 s
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.ContextCleaner]Cleaned shuffle 3
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526890005000 ms.0 from job set of time 1526890005000 ms
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 1.132 s for time 1526890005000 ms (execution: 0.062 s)
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 39 from persistence list
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_6_piece0 on 172.18.42.247:50607 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 38 from persistence list
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 37 from persistence list
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.storage.BlockManager]Removing RDD 38
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.storage.BlockManager]Removing RDD 39
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.storage.BlockManager]Removing RDD 37
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 36 from persistence list
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.storage.BlockManager]Removing RDD 36
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_7_piece0 on 172.18.42.247:50607 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.streaming.dstream.FileInputDStream]Cleared 1 old files that were older than 1526889945000 ms: 1526889930000 ms
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526890005000 ms
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526890005000 ms
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526890005000 ms
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.ContextCleaner]Cleaned shuffle 4
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_8_piece0 on 172.18.42.247:50607 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526890005000 ms to writer queue
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_14_piece0 on 172.18.42.247:50607 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_15_piece0 on 172.18.42.247:50607 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_9_piece0 on 172.18.42.247:50607 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.ContextCleaner]Cleaned shuffle 5
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_10_piece0 on 172.18.42.247:50607 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_11_piece0 on 172.18.42.247:50607 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.ContextCleaner]Cleaned shuffle 6
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_12_piece0 on 172.18.42.247:50607 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_13_piece0 on 172.18.42.247:50607 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.streaming.CheckpointWriter]Deleting hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526889930000.bk
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526890005000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890005000', took 4440 bytes and 255 ms
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526890005000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890005000'
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.streaming.CheckpointWriter]Deleting hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526889930000
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526890005000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890005000', took 4439 bytes and 75 ms
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.streaming.DStreamGraph]Clearing checkpoint data for time 1526890005000 ms
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.streaming.DStreamGraph]Cleared checkpoint data for time 1526890005000 ms
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.streaming.util.FileBasedWriteAheadLog_ReceivedBlockTracker]Attempting to clear 0 old log files in hdfs://hadoop1:9000/lib/sparkstreaming/check_point/receivedBlockMetadata older than 1526889945000: 
[INFO]  [2018-05-21 16:06:46] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1526889930000 ms
[WARN]  [2018-05-21 16:07:01] [org.apache.spark.streaming.dstream.FileInputDStream]Error finding new files
java.net.ConnectException: Call From hadoop/172.18.42.247 to hadoop:9000 failed on connection exception: java.net.ConnectException: Connection refused: no further information; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:730)
	at org.apache.hadoop.ipc.Client.call(Client.java:1351)
	at org.apache.hadoop.ipc.Client.call(Client.java:1300)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy10.getFileInfo(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:651)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1679)
	at org.apache.hadoop.hdfs.DistributedFileSystem$17.doCall(DistributedFileSystem.java:1106)
	at org.apache.hadoop.hdfs.DistributedFileSystem$17.doCall(DistributedFileSystem.java:1102)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1102)
	at org.apache.hadoop.fs.FileSystem.globStatusInternal(FileSystem.java:1701)
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1647)
	at org.apache.spark.streaming.dstream.FileInputDStream.findNewFiles(FileInputDStream.scala:205)
	at org.apache.spark.streaming.dstream.FileInputDStream.compute(FileInputDStream.scala:149)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:257)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.MappedDStream.compute(MappedDStream.scala:36)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:257)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.FlatMappedDStream.compute(FlatMappedDStream.scala:36)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:257)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.MappedDStream.compute(MappedDStream.scala:36)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:257)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.ShuffledDStream.compute(ShuffledDStream.scala:41)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:257)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.ForEachDStream.generateJob(ForEachDStream.scala:48)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:117)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:116)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:251)
	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:105)
	at org.apache.spark.streaming.DStreamGraph.generateJobs(DStreamGraph.scala:116)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$3.apply(JobGenerator.scala:248)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$3.apply(JobGenerator.scala:246)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.streaming.scheduler.JobGenerator.generateJobs(JobGenerator.scala:246)
	at org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:182)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:88)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:87)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Caused by: java.net.ConnectException: Connection refused: no further information
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:529)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:493)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:547)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:642)
	at org.apache.hadoop.ipc.Client$Connection.access$2600(Client.java:314)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1399)
	at org.apache.hadoop.ipc.Client.call(Client.java:1318)
	... 91 more
[INFO]  [2018-05-21 16:07:01] [org.apache.spark.streaming.dstream.FileInputDStream]New files at time 1526890020000 ms:

[INFO]  [2018-05-21 16:07:01] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526890020000 ms
[INFO]  [2018-05-21 16:07:01] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526890020000 ms
[INFO]  [2018-05-21 16:07:01] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526890020000 ms
[INFO]  [2018-05-21 16:07:01] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526890020000 ms
[INFO]  [2018-05-21 16:07:01] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526890020000 ms to writer queue
[INFO]  [2018-05-21 16:07:01] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:07:01] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526890020000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890020000'
[INFO]  [2018-05-21 16:07:01] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526890020000 ms.0 from job set of time 1526890020000 ms
[INFO]  [2018-05-21 16:07:01] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 48 (mapToPair at HdfsSparkStreamT.java:56)
[INFO]  [2018-05-21 16:07:01] [org.apache.spark.scheduler.DAGScheduler]Got job 18 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:07:01] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 37 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:07:01] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 36)
[INFO]  [2018-05-21 16:07:01] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:07:01] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 37 (ShuffledRDD[49] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:07:01] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_18 stored as values in memory (estimated size 3.8 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:07:01] [org.apache.spark.streaming.CheckpointWriter]Deleting hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526889945000.bk
[INFO]  [2018-05-21 16:07:01] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526890020000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890020000', took 4439 bytes and 75 ms
[INFO]  [2018-05-21 16:07:01] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_18_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:07:01] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_18_piece0 in memory on 172.18.42.247:50607 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:07:01] [org.apache.spark.SparkContext]Created broadcast 18 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:07:01] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 37 (ShuffledRDD[49] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:07:01] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 37.0 with 1 tasks
[INFO]  [2018-05-21 16:07:01] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 37.0 (TID 18, localhost, partition 0, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:07:01] [org.apache.spark.executor.Executor]Running task 0.0 in stage 37.0 (TID 18)
[INFO]  [2018-05-21 16:07:01] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:07:01] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 16:07:01] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 37.0 (TID 18). 1470 bytes result sent to driver
[INFO]  [2018-05-21 16:07:01] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 37.0 (TID 18) in 10 ms on localhost (1/1)
[INFO]  [2018-05-21 16:07:01] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 37.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:07:01] [org.apache.spark.scheduler.DAGScheduler]ResultStage 37 (print at HdfsSparkStreamT.java:59) finished in 0.013 s
[INFO]  [2018-05-21 16:07:01] [org.apache.spark.scheduler.DAGScheduler]Job 18 finished: print at HdfsSparkStreamT.java:59, took 0.130352 s
[INFO]  [2018-05-21 16:07:01] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:07:01] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 9 is 83 bytes
[INFO]  [2018-05-21 16:07:01] [org.apache.spark.scheduler.DAGScheduler]Got job 19 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:07:01] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 39 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:07:01] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 38)
[INFO]  [2018-05-21 16:07:01] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:07:01] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 39 (ShuffledRDD[49] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:07:01] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_19 stored as values in memory (estimated size 3.8 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:07:01] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_19_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:07:01] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_19_piece0 in memory on 172.18.42.247:50607 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:07:01] [org.apache.spark.SparkContext]Created broadcast 19 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:07:01] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 39 (ShuffledRDD[49] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:07:01] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 39.0 with 1 tasks
[INFO]  [2018-05-21 16:07:01] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 39.0 (TID 19, localhost, partition 1, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:07:01] [org.apache.spark.executor.Executor]Running task 0.0 in stage 39.0 (TID 19)
[INFO]  [2018-05-21 16:07:01] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:07:01] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 16:07:01] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 39.0 (TID 19). 1462 bytes result sent to driver
[INFO]  [2018-05-21 16:07:01] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 39.0 (TID 19) in 8 ms on localhost (1/1)
[INFO]  [2018-05-21 16:07:01] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 39.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:07:01] [org.apache.spark.scheduler.DAGScheduler]ResultStage 39 (print at HdfsSparkStreamT.java:59) finished in 0.008 s
[INFO]  [2018-05-21 16:07:01] [org.apache.spark.scheduler.DAGScheduler]Job 19 finished: print at HdfsSparkStreamT.java:59, took 0.023350 s
[INFO]  [2018-05-21 16:07:01] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526890020000 ms.0 from job set of time 1526890020000 ms
[INFO]  [2018-05-21 16:07:01] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 1.210 s for time 1526890020000 ms (execution: 0.158 s)
[INFO]  [2018-05-21 16:07:01] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 44 from persistence list
[INFO]  [2018-05-21 16:07:01] [org.apache.spark.storage.BlockManager]Removing RDD 44
[INFO]  [2018-05-21 16:07:01] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 43 from persistence list
[INFO]  [2018-05-21 16:07:01] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 42 from persistence list
[INFO]  [2018-05-21 16:07:01] [org.apache.spark.storage.BlockManager]Removing RDD 42
[INFO]  [2018-05-21 16:07:01] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 41 from persistence list
[INFO]  [2018-05-21 16:07:01] [org.apache.spark.storage.BlockManager]Removing RDD 43
[INFO]  [2018-05-21 16:07:01] [org.apache.spark.streaming.dstream.FileInputDStream]Cleared 1 old files that were older than 1526889960000 ms: 1526889945000 ms
[INFO]  [2018-05-21 16:07:01] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526890020000 ms
[INFO]  [2018-05-21 16:07:01] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526890020000 ms
[INFO]  [2018-05-21 16:07:01] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526890020000 ms
[INFO]  [2018-05-21 16:07:01] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526890020000 ms to writer queue
[INFO]  [2018-05-21 16:07:01] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526890020000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890020000'
[INFO]  [2018-05-21 16:07:01] [org.apache.spark.storage.BlockManager]Removing RDD 41
[INFO]  [2018-05-21 16:07:01] [org.apache.spark.streaming.CheckpointWriter]Deleting hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526889945000
[INFO]  [2018-05-21 16:07:01] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526890020000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890020000', took 4436 bytes and 157 ms
[INFO]  [2018-05-21 16:07:01] [org.apache.spark.streaming.DStreamGraph]Clearing checkpoint data for time 1526890020000 ms
[INFO]  [2018-05-21 16:07:01] [org.apache.spark.streaming.DStreamGraph]Cleared checkpoint data for time 1526890020000 ms
[INFO]  [2018-05-21 16:07:01] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO]  [2018-05-21 16:07:01] [org.apache.spark.streaming.util.FileBasedWriteAheadLog_ReceivedBlockTracker]Attempting to clear 0 old log files in hdfs://hadoop1:9000/lib/sparkstreaming/check_point/receivedBlockMetadata older than 1526889960000: 
[INFO]  [2018-05-21 16:07:01] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1526889945000 ms
[INFO]  [2018-05-21 16:07:32] [com.jiang.sparkstream.HdfsSparkStreamT]create JavaStreamingContext...
[INFO]  [2018-05-21 16:07:32] [org.apache.spark.SparkContext]Running Spark version 2.0.0
[WARN]  [2018-05-21 16:07:33] [org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO]  [2018-05-21 16:07:33] [org.apache.spark.SecurityManager]Changing view acls to: Bruin,hadoop
[INFO]  [2018-05-21 16:07:33] [org.apache.spark.SecurityManager]Changing modify acls to: Bruin,hadoop
[INFO]  [2018-05-21 16:07:33] [org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO]  [2018-05-21 16:07:33] [org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO]  [2018-05-21 16:07:33] [org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Bruin, hadoop); groups with view permissions: Set(); users  with modify permissions: Set(Bruin, hadoop); groups with modify permissions: Set()
[INFO]  [2018-05-21 16:07:34] [org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 50725.
[INFO]  [2018-05-21 16:07:34] [org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO]  [2018-05-21 16:07:34] [org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO]  [2018-05-21 16:07:34] [org.apache.spark.storage.DiskBlockManager]Created local directory at C:\Users\Bruin\AppData\Local\Temp\blockmgr-ae31f465-5ae2-4e3b-87eb-60f9c8eea9af
[INFO]  [2018-05-21 16:07:34] [org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 901.8 MB
[INFO]  [2018-05-21 16:07:34] [org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO]  [2018-05-21 16:07:35] [org.spark_project.jetty.util.log]Logging initialized @3773ms
[INFO]  [2018-05-21 16:07:35] [org.spark_project.jetty.server.Server]jetty-9.2.z-SNAPSHOT
[INFO]  [2018-05-21 16:07:35] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@fade1fc{/jobs,null,AVAILABLE}
[INFO]  [2018-05-21 16:07:35] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@67c2e933{/jobs/json,null,AVAILABLE}
[INFO]  [2018-05-21 16:07:35] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@41dd05a{/jobs/job,null,AVAILABLE}
[INFO]  [2018-05-21 16:07:35] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@613a8ee1{/jobs/job/json,null,AVAILABLE}
[INFO]  [2018-05-21 16:07:35] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@178213b{/stages,null,AVAILABLE}
[INFO]  [2018-05-21 16:07:35] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7103cb56{/stages/json,null,AVAILABLE}
[INFO]  [2018-05-21 16:07:35] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1b765a2c{/stages/stage,null,AVAILABLE}
[INFO]  [2018-05-21 16:07:35] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2e8e8225{/stages/stage/json,null,AVAILABLE}
[INFO]  [2018-05-21 16:07:35] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6ebf0f36{/stages/pool,null,AVAILABLE}
[INFO]  [2018-05-21 16:07:35] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@18920cc{/stages/pool/json,null,AVAILABLE}
[INFO]  [2018-05-21 16:07:35] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2807bdeb{/storage,null,AVAILABLE}
[INFO]  [2018-05-21 16:07:35] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@72c28d64{/storage/json,null,AVAILABLE}
[INFO]  [2018-05-21 16:07:35] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6492fab5{/storage/rdd,null,AVAILABLE}
[INFO]  [2018-05-21 16:07:35] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2c5529ab{/storage/rdd/json,null,AVAILABLE}
[INFO]  [2018-05-21 16:07:35] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@39a8312f{/environment,null,AVAILABLE}
[INFO]  [2018-05-21 16:07:35] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5f6722d3{/environment/json,null,AVAILABLE}
[INFO]  [2018-05-21 16:07:35] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2c532cd8{/executors,null,AVAILABLE}
[INFO]  [2018-05-21 16:07:35] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@294e5088{/executors/json,null,AVAILABLE}
[INFO]  [2018-05-21 16:07:35] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@51972dc7{/executors/threadDump,null,AVAILABLE}
[INFO]  [2018-05-21 16:07:35] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3700ec9c{/executors/threadDump/json,null,AVAILABLE}
[INFO]  [2018-05-21 16:07:35] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2002348{/static,null,AVAILABLE}
[INFO]  [2018-05-21 16:07:35] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5911e990{/,null,AVAILABLE}
[INFO]  [2018-05-21 16:07:35] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@31000e60{/api,null,AVAILABLE}
[INFO]  [2018-05-21 16:07:35] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1d470d0{/stages/stage/kill,null,AVAILABLE}
[INFO]  [2018-05-21 16:07:35] [org.spark_project.jetty.server.ServerConnector]Started ServerConnector@6b53bcc2{HTTP/1.1}{0.0.0.0:4040}
[INFO]  [2018-05-21 16:07:35] [org.spark_project.jetty.server.Server]Started @3912ms
[INFO]  [2018-05-21 16:07:35] [org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO]  [2018-05-21 16:07:35] [org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://172.18.42.247:4040
[INFO]  [2018-05-21 16:07:35] [org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO]  [2018-05-21 16:07:35] [org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50738.
[INFO]  [2018-05-21 16:07:35] [org.apache.spark.network.netty.NettyBlockTransferService]Server created on 172.18.42.247:50738
[INFO]  [2018-05-21 16:07:35] [org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 172.18.42.247, 50738)
[INFO]  [2018-05-21 16:07:35] [org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 172.18.42.247:50738 with 901.8 MB RAM, BlockManagerId(driver, 172.18.42.247, 50738)
[INFO]  [2018-05-21 16:07:35] [org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 172.18.42.247, 50738)
[INFO]  [2018-05-21 16:07:35] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@290b1b2e{/metrics/json,null,AVAILABLE}
[ERROR]  [2018-05-21 16:07:36] [org.apache.hadoop.util.Shell]Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:278)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:300)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:293)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:76)
	at org.apache.hadoop.conf.Configuration.getTrimmedStrings(Configuration.java:1546)
	at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:519)
	at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:453)
	at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:136)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2433)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:88)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2467)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2449)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:367)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:287)
	at org.apache.spark.streaming.StreamingContext.checkpoint(StreamingContext.scala:238)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.checkpoint(JavaStreamingContext.scala:506)
	at com.jiang.sparkstream.HdfsSparkStreamT.context(HdfsSparkStreamT.java:41)
	at com.jiang.sparkstream.HdfsSparkStreamT.run(HdfsSparkStreamT.java:47)
	at com.jiang.sparkstream.HdfsSparkStreamT.main(HdfsSparkStreamT.java:73)
[INFO]  [2018-05-21 16:07:36] [org.apache.spark.streaming.dstream.FileInputDStream]Duration for remembering RDDs set to 60000 ms for org.apache.spark.streaming.dstream.FileInputDStream@4cbf4f53
[INFO]  [2018-05-21 16:07:36] [org.apache.spark.streaming.util.FileBasedWriteAheadLog_ReceivedBlockTracker]Recovered 2 write ahead log files from hdfs://hadoop1:9000/lib/sparkstreaming/check_point/receivedBlockMetadata
[INFO]  [2018-05-21 16:07:37] [org.apache.spark.streaming.dstream.FileInputDStream]Slide time = 15000 ms
[INFO]  [2018-05-21 16:07:37] [org.apache.spark.streaming.dstream.FileInputDStream]Storage level = Serialized 1x Replicated
[INFO]  [2018-05-21 16:07:37] [org.apache.spark.streaming.dstream.FileInputDStream]Checkpoint interval = null
[INFO]  [2018-05-21 16:07:37] [org.apache.spark.streaming.dstream.FileInputDStream]Remember interval = 60000 ms
[INFO]  [2018-05-21 16:07:37] [org.apache.spark.streaming.dstream.FileInputDStream]Initialized and validated org.apache.spark.streaming.dstream.FileInputDStream@4cbf4f53
[INFO]  [2018-05-21 16:07:37] [org.apache.spark.streaming.dstream.MappedDStream]Slide time = 15000 ms
[INFO]  [2018-05-21 16:07:37] [org.apache.spark.streaming.dstream.MappedDStream]Storage level = Serialized 1x Replicated
[INFO]  [2018-05-21 16:07:37] [org.apache.spark.streaming.dstream.MappedDStream]Checkpoint interval = null
[INFO]  [2018-05-21 16:07:37] [org.apache.spark.streaming.dstream.MappedDStream]Remember interval = 15000 ms
[INFO]  [2018-05-21 16:07:37] [org.apache.spark.streaming.dstream.MappedDStream]Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@3e5d4f6b
[INFO]  [2018-05-21 16:07:37] [org.apache.spark.streaming.dstream.FlatMappedDStream]Slide time = 15000 ms
[INFO]  [2018-05-21 16:07:37] [org.apache.spark.streaming.dstream.FlatMappedDStream]Storage level = Serialized 1x Replicated
[INFO]  [2018-05-21 16:07:37] [org.apache.spark.streaming.dstream.FlatMappedDStream]Checkpoint interval = null
[INFO]  [2018-05-21 16:07:37] [org.apache.spark.streaming.dstream.FlatMappedDStream]Remember interval = 15000 ms
[INFO]  [2018-05-21 16:07:37] [org.apache.spark.streaming.dstream.FlatMappedDStream]Initialized and validated org.apache.spark.streaming.dstream.FlatMappedDStream@5a00eb1e
[INFO]  [2018-05-21 16:07:37] [org.apache.spark.streaming.dstream.MappedDStream]Slide time = 15000 ms
[INFO]  [2018-05-21 16:07:37] [org.apache.spark.streaming.dstream.MappedDStream]Storage level = Serialized 1x Replicated
[INFO]  [2018-05-21 16:07:37] [org.apache.spark.streaming.dstream.MappedDStream]Checkpoint interval = null
[INFO]  [2018-05-21 16:07:37] [org.apache.spark.streaming.dstream.MappedDStream]Remember interval = 15000 ms
[INFO]  [2018-05-21 16:07:37] [org.apache.spark.streaming.dstream.MappedDStream]Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@75769ab0
[INFO]  [2018-05-21 16:07:37] [org.apache.spark.streaming.dstream.ShuffledDStream]Slide time = 15000 ms
[INFO]  [2018-05-21 16:07:37] [org.apache.spark.streaming.dstream.ShuffledDStream]Storage level = Serialized 1x Replicated
[INFO]  [2018-05-21 16:07:37] [org.apache.spark.streaming.dstream.ShuffledDStream]Checkpoint interval = null
[INFO]  [2018-05-21 16:07:37] [org.apache.spark.streaming.dstream.ShuffledDStream]Remember interval = 15000 ms
[INFO]  [2018-05-21 16:07:37] [org.apache.spark.streaming.dstream.ShuffledDStream]Initialized and validated org.apache.spark.streaming.dstream.ShuffledDStream@396e6d9
[INFO]  [2018-05-21 16:07:37] [org.apache.spark.streaming.dstream.ForEachDStream]Slide time = 15000 ms
[INFO]  [2018-05-21 16:07:37] [org.apache.spark.streaming.dstream.ForEachDStream]Storage level = Serialized 1x Replicated
[INFO]  [2018-05-21 16:07:37] [org.apache.spark.streaming.dstream.ForEachDStream]Checkpoint interval = null
[INFO]  [2018-05-21 16:07:37] [org.apache.spark.streaming.dstream.ForEachDStream]Remember interval = 15000 ms
[INFO]  [2018-05-21 16:07:37] [org.apache.spark.streaming.dstream.ForEachDStream]Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@736048ed
[INFO]  [2018-05-21 16:07:37] [org.apache.spark.streaming.util.RecurringTimer]Started timer for JobGenerator at time 1526890065000
[INFO]  [2018-05-21 16:07:37] [org.apache.spark.streaming.scheduler.JobGenerator]Started JobGenerator at 1526890065000 ms
[INFO]  [2018-05-21 16:07:37] [org.apache.spark.streaming.scheduler.JobScheduler]Started JobScheduler
[INFO]  [2018-05-21 16:07:37] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3ffb3598{/streaming,null,AVAILABLE}
[INFO]  [2018-05-21 16:07:37] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3b2f4a93{/streaming/json,null,AVAILABLE}
[INFO]  [2018-05-21 16:07:37] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7b78ed6a{/streaming/batch,null,AVAILABLE}
[INFO]  [2018-05-21 16:07:37] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@6ec65b5e{/streaming/batch/json,null,AVAILABLE}
[INFO]  [2018-05-21 16:07:37] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@263bbfeb{/static/streaming,null,AVAILABLE}
[INFO]  [2018-05-21 16:07:37] [org.apache.spark.streaming.StreamingContext]StreamingContext started
[INFO]  [2018-05-21 16:07:45] [org.apache.spark.streaming.dstream.FileInputDStream]Finding new files took 103 ms
[INFO]  [2018-05-21 16:07:45] [org.apache.spark.streaming.dstream.FileInputDStream]New files at time 1526890065000 ms:

[INFO]  [2018-05-21 16:07:45] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526890065000 ms
[INFO]  [2018-05-21 16:07:45] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526890065000 ms.0 from job set of time 1526890065000 ms
[INFO]  [2018-05-21 16:07:45] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526890065000 ms
[INFO]  [2018-05-21 16:07:45] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526890065000 ms
[INFO]  [2018-05-21 16:07:45] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526890065000 ms
[INFO]  [2018-05-21 16:07:45] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526890065000 ms to writer queue
[INFO]  [2018-05-21 16:07:45] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526890065000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890065000'
[INFO]  [2018-05-21 16:07:45] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:07:45] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 3 (mapToPair at HdfsSparkStreamT.java:56)
[INFO]  [2018-05-21 16:07:45] [org.apache.spark.scheduler.DAGScheduler]Got job 0 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:07:45] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 1 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:07:45] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 0)
[INFO]  [2018-05-21 16:07:45] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:07:45] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 1 (ShuffledRDD[4] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:07:45] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_0 stored as values in memory (estimated size 3.8 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:07:45] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_0_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:07:45] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_0_piece0 in memory on 172.18.42.247:50738 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:07:45] [org.apache.spark.SparkContext]Created broadcast 0 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:07:45] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 1 (ShuffledRDD[4] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:07:45] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 1.0 with 1 tasks
[INFO]  [2018-05-21 16:07:45] [org.apache.spark.streaming.CheckpointWriter]Deleting hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526889960000.bk
[INFO]  [2018-05-21 16:07:45] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526890065000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890065000', took 4390 bytes and 478 ms
[INFO]  [2018-05-21 16:07:45] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 1.0 (TID 0, localhost, partition 0, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:07:45] [org.apache.spark.executor.Executor]Running task 0.0 in stage 1.0 (TID 0)
[INFO]  [2018-05-21 16:07:45] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:07:45] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 4 ms
[INFO]  [2018-05-21 16:07:45] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 1.0 (TID 0). 1549 bytes result sent to driver
[INFO]  [2018-05-21 16:07:45] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 1.0 (TID 0) in 149 ms on localhost (1/1)
[INFO]  [2018-05-21 16:07:45] [org.apache.spark.scheduler.DAGScheduler]ResultStage 1 (print at HdfsSparkStreamT.java:59) finished in 0.194 s
[INFO]  [2018-05-21 16:07:45] [org.apache.spark.scheduler.DAGScheduler]Job 0 finished: print at HdfsSparkStreamT.java:59, took 0.576725 s
[INFO]  [2018-05-21 16:07:45] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 1.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:07:45] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:07:45] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 0 is 83 bytes
[INFO]  [2018-05-21 16:07:45] [org.apache.spark.scheduler.DAGScheduler]Got job 1 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:07:45] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 3 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:07:45] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 2)
[INFO]  [2018-05-21 16:07:45] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:07:45] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 3 (ShuffledRDD[4] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:07:45] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_1 stored as values in memory (estimated size 3.8 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:07:45] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:07:45] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_1_piece0 in memory on 172.18.42.247:50738 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:07:45] [org.apache.spark.SparkContext]Created broadcast 1 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:07:45] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 3 (ShuffledRDD[4] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:07:45] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 3.0 with 1 tasks
[INFO]  [2018-05-21 16:07:45] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 3.0 (TID 1, localhost, partition 1, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:07:45] [org.apache.spark.executor.Executor]Running task 0.0 in stage 3.0 (TID 1)
[INFO]  [2018-05-21 16:07:45] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:07:45] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 16:07:45] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 3.0 (TID 1). 1549 bytes result sent to driver
[INFO]  [2018-05-21 16:07:45] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_0_piece0 on 172.18.42.247:50738 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:07:45] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 3.0 (TID 1) in 32 ms on localhost (1/1)
[INFO]  [2018-05-21 16:07:45] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 3.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:07:45] [org.apache.spark.scheduler.DAGScheduler]ResultStage 3 (print at HdfsSparkStreamT.java:59) finished in 0.037 s
[INFO]  [2018-05-21 16:07:45] [org.apache.spark.scheduler.DAGScheduler]Job 1 finished: print at HdfsSparkStreamT.java:59, took 0.085131 s
[INFO]  [2018-05-21 16:07:45] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526890065000 ms.0 from job set of time 1526890065000 ms
[INFO]  [2018-05-21 16:07:46] [org.apache.spark.streaming.dstream.FileInputDStream]Cleared 0 old files that were older than 1526890005000 ms: 
[INFO]  [2018-05-21 16:07:46] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526890065000 ms
[INFO]  [2018-05-21 16:07:46] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526890065000 ms
[INFO]  [2018-05-21 16:07:46] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.996 s for time 1526890065000 ms (execution: 0.805 s)
[INFO]  [2018-05-21 16:07:46] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526890065000 ms
[INFO]  [2018-05-21 16:07:46] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526890065000 ms to writer queue
[INFO]  [2018-05-21 16:07:46] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526890065000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890065000'
[INFO]  [2018-05-21 16:07:46] [org.apache.spark.streaming.CheckpointWriter]Deleting hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526889960000
[INFO]  [2018-05-21 16:07:46] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526890065000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890065000', took 4387 bytes and 136 ms
[INFO]  [2018-05-21 16:07:46] [org.apache.spark.streaming.DStreamGraph]Clearing checkpoint data for time 1526890065000 ms
[INFO]  [2018-05-21 16:07:46] [org.apache.spark.streaming.DStreamGraph]Cleared checkpoint data for time 1526890065000 ms
[INFO]  [2018-05-21 16:07:46] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO]  [2018-05-21 16:07:46] [org.apache.spark.streaming.util.FileBasedWriteAheadLog_ReceivedBlockTracker]Attempting to clear 1 old log files in hdfs://hadoop1:9000/lib/sparkstreaming/check_point/receivedBlockMetadata older than 1526890005000: hdfs://hadoop1:9000/lib/sparkstreaming/check_point/receivedBlockMetadata/log-1526889913116-1526889973116
[INFO]  [2018-05-21 16:07:46] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 
[INFO]  [2018-05-21 16:07:46] [org.apache.spark.streaming.util.FileBasedWriteAheadLog_ReceivedBlockTracker]Cleared log files in hdfs://hadoop1:9000/lib/sparkstreaming/check_point/receivedBlockMetadata older than 1526890005000
[INFO]  [2018-05-21 16:08:00] [org.apache.spark.streaming.dstream.FileInputDStream]Finding new files took 28 ms
[INFO]  [2018-05-21 16:08:00] [org.apache.spark.streaming.dstream.FileInputDStream]New files at time 1526890080000 ms:

[INFO]  [2018-05-21 16:08:00] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526890080000 ms
[INFO]  [2018-05-21 16:08:00] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526890080000 ms
[INFO]  [2018-05-21 16:08:00] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526890080000 ms
[INFO]  [2018-05-21 16:08:00] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526890080000 ms
[INFO]  [2018-05-21 16:08:00] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526890080000 ms to writer queue
[INFO]  [2018-05-21 16:08:00] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526890080000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890080000'
[INFO]  [2018-05-21 16:08:00] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:08:00] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 8 (mapToPair at HdfsSparkStreamT.java:56)
[INFO]  [2018-05-21 16:08:00] [org.apache.spark.scheduler.DAGScheduler]Got job 2 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:08:00] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 5 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:08:00] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 4)
[INFO]  [2018-05-21 16:08:00] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:08:00] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 5 (ShuffledRDD[9] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:08:00] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_2 stored as values in memory (estimated size 3.8 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:08:00] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:08:00] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_2_piece0 in memory on 172.18.42.247:50738 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:08:00] [org.apache.spark.SparkContext]Created broadcast 2 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:08:00] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 5 (ShuffledRDD[9] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:08:00] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 5.0 with 1 tasks
[INFO]  [2018-05-21 16:08:00] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526890080000 ms.0 from job set of time 1526890080000 ms
[INFO]  [2018-05-21 16:08:00] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 5.0 (TID 2, localhost, partition 0, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:08:00] [org.apache.spark.executor.Executor]Running task 0.0 in stage 5.0 (TID 2)
[INFO]  [2018-05-21 16:08:00] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:08:00] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 16:08:00] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 5.0 (TID 2). 1549 bytes result sent to driver
[INFO]  [2018-05-21 16:08:00] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 5.0 (TID 2) in 13 ms on localhost (1/1)
[INFO]  [2018-05-21 16:08:00] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 5.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:08:00] [org.apache.spark.scheduler.DAGScheduler]ResultStage 5 (print at HdfsSparkStreamT.java:59) finished in 0.019 s
[INFO]  [2018-05-21 16:08:00] [org.apache.spark.scheduler.DAGScheduler]Job 2 finished: print at HdfsSparkStreamT.java:59, took 0.027591 s
[INFO]  [2018-05-21 16:08:00] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:08:00] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 1 is 83 bytes
[INFO]  [2018-05-21 16:08:00] [org.apache.spark.scheduler.DAGScheduler]Got job 3 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:08:00] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 7 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:08:00] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 6)
[INFO]  [2018-05-21 16:08:00] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:08:00] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 7 (ShuffledRDD[9] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:08:00] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_3 stored as values in memory (estimated size 3.8 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:08:00] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:08:00] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_3_piece0 in memory on 172.18.42.247:50738 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:08:00] [org.apache.spark.SparkContext]Created broadcast 3 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:08:00] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 7 (ShuffledRDD[9] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:08:00] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 7.0 with 1 tasks
[INFO]  [2018-05-21 16:08:00] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 7.0 (TID 3, localhost, partition 1, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:08:00] [org.apache.spark.executor.Executor]Running task 0.0 in stage 7.0 (TID 3)
[INFO]  [2018-05-21 16:08:00] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:08:00] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 16:08:00] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 7.0 (TID 3). 1549 bytes result sent to driver
[INFO]  [2018-05-21 16:08:00] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 7.0 (TID 3) in 11 ms on localhost (1/1)
[INFO]  [2018-05-21 16:08:00] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 7.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:08:00] [org.apache.spark.scheduler.DAGScheduler]ResultStage 7 (print at HdfsSparkStreamT.java:59) finished in 0.012 s
[INFO]  [2018-05-21 16:08:00] [org.apache.spark.scheduler.DAGScheduler]Job 3 finished: print at HdfsSparkStreamT.java:59, took 0.026545 s
[INFO]  [2018-05-21 16:08:00] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526890080000 ms.0 from job set of time 1526890080000 ms
[INFO]  [2018-05-21 16:08:00] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.138 s for time 1526890080000 ms (execution: 0.056 s)
[INFO]  [2018-05-21 16:08:00] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 4 from persistence list
[INFO]  [2018-05-21 16:08:00] [org.apache.spark.storage.BlockManager]Removing RDD 4
[INFO]  [2018-05-21 16:08:00] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 3 from persistence list
[INFO]  [2018-05-21 16:08:00] [org.apache.spark.storage.BlockManager]Removing RDD 3
[INFO]  [2018-05-21 16:08:00] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 2 from persistence list
[INFO]  [2018-05-21 16:08:00] [org.apache.spark.storage.BlockManager]Removing RDD 2
[INFO]  [2018-05-21 16:08:00] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 1 from persistence list
[INFO]  [2018-05-21 16:08:00] [org.apache.spark.streaming.dstream.FileInputDStream]Cleared 0 old files that were older than 1526890020000 ms: 
[INFO]  [2018-05-21 16:08:00] [org.apache.spark.storage.BlockManager]Removing RDD 1
[INFO]  [2018-05-21 16:08:00] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526890080000 ms
[INFO]  [2018-05-21 16:08:00] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526890080000 ms
[INFO]  [2018-05-21 16:08:00] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526890080000 ms
[INFO]  [2018-05-21 16:08:00] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526890080000 ms to writer queue
[INFO]  [2018-05-21 16:08:00] [org.apache.spark.streaming.CheckpointWriter]Deleting hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526889975000.bk
[INFO]  [2018-05-21 16:08:00] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526890080000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890080000', took 4402 bytes and 158 ms
[INFO]  [2018-05-21 16:08:00] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526890080000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890080000'
[INFO]  [2018-05-21 16:08:00] [org.apache.spark.streaming.CheckpointWriter]Deleting hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526889975000
[INFO]  [2018-05-21 16:08:00] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526890080000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890080000', took 4399 bytes and 99 ms
[INFO]  [2018-05-21 16:08:00] [org.apache.spark.streaming.DStreamGraph]Clearing checkpoint data for time 1526890080000 ms
[INFO]  [2018-05-21 16:08:00] [org.apache.spark.streaming.DStreamGraph]Cleared checkpoint data for time 1526890080000 ms
[INFO]  [2018-05-21 16:08:00] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO]  [2018-05-21 16:08:00] [org.apache.spark.streaming.util.FileBasedWriteAheadLog_ReceivedBlockTracker]Attempting to clear 0 old log files in hdfs://hadoop1:9000/lib/sparkstreaming/check_point/receivedBlockMetadata older than 1526890020000: 
[INFO]  [2018-05-21 16:08:00] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 
[INFO]  [2018-05-21 16:08:15] [org.apache.spark.streaming.dstream.FileInputDStream]Finding new files took 8 ms
[INFO]  [2018-05-21 16:08:15] [org.apache.spark.streaming.dstream.FileInputDStream]New files at time 1526890095000 ms:

[INFO]  [2018-05-21 16:08:15] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526890095000 ms
[INFO]  [2018-05-21 16:08:15] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526890095000 ms
[INFO]  [2018-05-21 16:08:15] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526890095000 ms
[INFO]  [2018-05-21 16:08:15] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526890095000 ms
[INFO]  [2018-05-21 16:08:15] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526890095000 ms to writer queue
[INFO]  [2018-05-21 16:08:15] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526890095000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890095000'
[INFO]  [2018-05-21 16:08:15] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526890095000 ms.0 from job set of time 1526890095000 ms
[INFO]  [2018-05-21 16:08:15] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:08:15] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 13 (mapToPair at HdfsSparkStreamT.java:56)
[INFO]  [2018-05-21 16:08:15] [org.apache.spark.scheduler.DAGScheduler]Got job 4 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:08:15] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 9 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:08:15] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 8)
[INFO]  [2018-05-21 16:08:15] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:08:15] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 9 (ShuffledRDD[14] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:08:15] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_4 stored as values in memory (estimated size 3.8 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:08:15] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_4_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:08:15] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_4_piece0 in memory on 172.18.42.247:50738 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:08:15] [org.apache.spark.SparkContext]Created broadcast 4 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:08:15] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 9 (ShuffledRDD[14] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:08:15] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 9.0 with 1 tasks
[INFO]  [2018-05-21 16:08:15] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 9.0 (TID 4, localhost, partition 0, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:08:15] [org.apache.spark.executor.Executor]Running task 0.0 in stage 9.0 (TID 4)
[INFO]  [2018-05-21 16:08:15] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:08:15] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO]  [2018-05-21 16:08:15] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 9.0 (TID 4). 1549 bytes result sent to driver
[INFO]  [2018-05-21 16:08:15] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 9.0 (TID 4) in 11 ms on localhost (1/1)
[INFO]  [2018-05-21 16:08:15] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 9.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:08:15] [org.apache.spark.scheduler.DAGScheduler]ResultStage 9 (print at HdfsSparkStreamT.java:59) finished in 0.013 s
[INFO]  [2018-05-21 16:08:15] [org.apache.spark.scheduler.DAGScheduler]Job 4 finished: print at HdfsSparkStreamT.java:59, took 0.026050 s
[INFO]  [2018-05-21 16:08:15] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:08:15] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 2 is 83 bytes
[INFO]  [2018-05-21 16:08:15] [org.apache.spark.scheduler.DAGScheduler]Got job 5 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:08:15] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 11 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:08:15] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 10)
[INFO]  [2018-05-21 16:08:15] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:08:15] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 11 (ShuffledRDD[14] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:08:15] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_5 stored as values in memory (estimated size 3.8 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:08:15] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:08:15] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_5_piece0 in memory on 172.18.42.247:50738 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:08:15] [org.apache.spark.SparkContext]Created broadcast 5 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:08:15] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 11 (ShuffledRDD[14] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:08:15] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 11.0 with 1 tasks
[INFO]  [2018-05-21 16:08:15] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 11.0 (TID 5, localhost, partition 1, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:08:15] [org.apache.spark.executor.Executor]Running task 0.0 in stage 11.0 (TID 5)
[INFO]  [2018-05-21 16:08:15] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:08:15] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 16:08:15] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 11.0 (TID 5). 1549 bytes result sent to driver
[INFO]  [2018-05-21 16:08:15] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 11.0 (TID 5) in 22 ms on localhost (1/1)
[INFO]  [2018-05-21 16:08:15] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 11.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:08:15] [org.apache.spark.scheduler.DAGScheduler]ResultStage 11 (print at HdfsSparkStreamT.java:59) finished in 0.024 s
[INFO]  [2018-05-21 16:08:15] [org.apache.spark.scheduler.DAGScheduler]Job 5 finished: print at HdfsSparkStreamT.java:59, took 0.074158 s
[INFO]  [2018-05-21 16:08:15] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526890095000 ms.0 from job set of time 1526890095000 ms
[INFO]  [2018-05-21 16:08:15] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.142 s for time 1526890095000 ms (execution: 0.116 s)
[INFO]  [2018-05-21 16:08:15] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 9 from persistence list
[INFO]  [2018-05-21 16:08:15] [org.apache.spark.storage.BlockManager]Removing RDD 9
[INFO]  [2018-05-21 16:08:15] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 8 from persistence list
[INFO]  [2018-05-21 16:08:15] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 7 from persistence list
[INFO]  [2018-05-21 16:08:15] [org.apache.spark.storage.BlockManager]Removing RDD 8
[INFO]  [2018-05-21 16:08:15] [org.apache.spark.storage.BlockManager]Removing RDD 7
[INFO]  [2018-05-21 16:08:15] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 6 from persistence list
[INFO]  [2018-05-21 16:08:15] [org.apache.spark.storage.BlockManager]Removing RDD 6
[INFO]  [2018-05-21 16:08:15] [org.apache.spark.streaming.dstream.FileInputDStream]Cleared 0 old files that were older than 1526890035000 ms: 
[INFO]  [2018-05-21 16:08:15] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526890095000 ms
[INFO]  [2018-05-21 16:08:15] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526890095000 ms
[INFO]  [2018-05-21 16:08:15] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526890095000 ms
[INFO]  [2018-05-21 16:08:15] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526890095000 ms to writer queue
[INFO]  [2018-05-21 16:08:15] [org.apache.spark.streaming.CheckpointWriter]Deleting hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526889990000.bk
[INFO]  [2018-05-21 16:08:15] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526890095000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890095000', took 4414 bytes and 226 ms
[INFO]  [2018-05-21 16:08:15] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526890095000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890095000'
[INFO]  [2018-05-21 16:08:15] [org.apache.spark.streaming.CheckpointWriter]Deleting hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526889990000
[INFO]  [2018-05-21 16:08:15] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526890095000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890095000', took 4411 bytes and 77 ms
[INFO]  [2018-05-21 16:08:15] [org.apache.spark.streaming.DStreamGraph]Clearing checkpoint data for time 1526890095000 ms
[INFO]  [2018-05-21 16:08:15] [org.apache.spark.streaming.DStreamGraph]Cleared checkpoint data for time 1526890095000 ms
[INFO]  [2018-05-21 16:08:15] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO]  [2018-05-21 16:08:15] [org.apache.spark.streaming.util.FileBasedWriteAheadLog_ReceivedBlockTracker]Attempting to clear 0 old log files in hdfs://hadoop1:9000/lib/sparkstreaming/check_point/receivedBlockMetadata older than 1526890035000: 
[INFO]  [2018-05-21 16:08:15] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 
[INFO]  [2018-05-21 16:08:30] [org.apache.spark.streaming.dstream.FileInputDStream]Finding new files took 26 ms
[INFO]  [2018-05-21 16:08:30] [org.apache.spark.streaming.dstream.FileInputDStream]New files at time 1526890110000 ms:

[INFO]  [2018-05-21 16:08:30] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526890110000 ms
[INFO]  [2018-05-21 16:08:30] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526890110000 ms
[INFO]  [2018-05-21 16:08:30] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526890110000 ms
[INFO]  [2018-05-21 16:08:30] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526890110000 ms
[INFO]  [2018-05-21 16:08:30] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526890110000 ms.0 from job set of time 1526890110000 ms
[INFO]  [2018-05-21 16:08:30] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526890110000 ms to writer queue
[INFO]  [2018-05-21 16:08:30] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526890110000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890110000'
[INFO]  [2018-05-21 16:08:30] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:08:30] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 18 (mapToPair at HdfsSparkStreamT.java:56)
[INFO]  [2018-05-21 16:08:30] [org.apache.spark.scheduler.DAGScheduler]Got job 6 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:08:30] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 13 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:08:30] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 12)
[INFO]  [2018-05-21 16:08:30] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:08:30] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 13 (ShuffledRDD[19] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:08:30] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_6 stored as values in memory (estimated size 3.8 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:08:30] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:08:30] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_6_piece0 in memory on 172.18.42.247:50738 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:08:30] [org.apache.spark.SparkContext]Created broadcast 6 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:08:30] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 13 (ShuffledRDD[19] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:08:30] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 13.0 with 1 tasks
[INFO]  [2018-05-21 16:08:30] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 13.0 (TID 6, localhost, partition 0, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:08:30] [org.apache.spark.executor.Executor]Running task 0.0 in stage 13.0 (TID 6)
[INFO]  [2018-05-21 16:08:30] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:08:30] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 16:08:30] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 13.0 (TID 6). 1549 bytes result sent to driver
[INFO]  [2018-05-21 16:08:30] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 13.0 (TID 6) in 15 ms on localhost (1/1)
[INFO]  [2018-05-21 16:08:30] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 13.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:08:30] [org.apache.spark.scheduler.DAGScheduler]ResultStage 13 (print at HdfsSparkStreamT.java:59) finished in 0.015 s
[INFO]  [2018-05-21 16:08:30] [org.apache.spark.scheduler.DAGScheduler]Job 6 finished: print at HdfsSparkStreamT.java:59, took 0.034183 s
[INFO]  [2018-05-21 16:08:30] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:08:30] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 3 is 83 bytes
[INFO]  [2018-05-21 16:08:30] [org.apache.spark.scheduler.DAGScheduler]Got job 7 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:08:30] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 15 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:08:30] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 14)
[INFO]  [2018-05-21 16:08:30] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:08:30] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 15 (ShuffledRDD[19] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:08:30] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_7 stored as values in memory (estimated size 3.8 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:08:30] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:08:30] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_7_piece0 in memory on 172.18.42.247:50738 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:08:30] [org.apache.spark.SparkContext]Created broadcast 7 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:08:30] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 15 (ShuffledRDD[19] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:08:30] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 15.0 with 1 tasks
[INFO]  [2018-05-21 16:08:30] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 15.0 (TID 7, localhost, partition 1, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:08:30] [org.apache.spark.executor.Executor]Running task 0.0 in stage 15.0 (TID 7)
[INFO]  [2018-05-21 16:08:30] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:08:30] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 16:08:30] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 15.0 (TID 7). 1549 bytes result sent to driver
[INFO]  [2018-05-21 16:08:30] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 15.0 (TID 7) in 12 ms on localhost (1/1)
[INFO]  [2018-05-21 16:08:30] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 15.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:08:30] [org.apache.spark.scheduler.DAGScheduler]ResultStage 15 (print at HdfsSparkStreamT.java:59) finished in 0.017 s
[INFO]  [2018-05-21 16:08:30] [org.apache.spark.scheduler.DAGScheduler]Job 7 finished: print at HdfsSparkStreamT.java:59, took 0.062909 s
[INFO]  [2018-05-21 16:08:30] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526890110000 ms.0 from job set of time 1526890110000 ms
[INFO]  [2018-05-21 16:08:30] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.157 s for time 1526890110000 ms (execution: 0.112 s)
[INFO]  [2018-05-21 16:08:30] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 14 from persistence list
[INFO]  [2018-05-21 16:08:30] [org.apache.spark.storage.BlockManager]Removing RDD 14
[INFO]  [2018-05-21 16:08:30] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 13 from persistence list
[INFO]  [2018-05-21 16:08:30] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 12 from persistence list
[INFO]  [2018-05-21 16:08:30] [org.apache.spark.storage.BlockManager]Removing RDD 13
[INFO]  [2018-05-21 16:08:30] [org.apache.spark.storage.BlockManager]Removing RDD 12
[INFO]  [2018-05-21 16:08:30] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 11 from persistence list
[INFO]  [2018-05-21 16:08:30] [org.apache.spark.streaming.dstream.FileInputDStream]Cleared 0 old files that were older than 1526890050000 ms: 
[INFO]  [2018-05-21 16:08:30] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526890110000 ms
[INFO]  [2018-05-21 16:08:30] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526890110000 ms
[INFO]  [2018-05-21 16:08:30] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526890110000 ms
[INFO]  [2018-05-21 16:08:30] [org.apache.spark.storage.BlockManager]Removing RDD 11
[INFO]  [2018-05-21 16:08:30] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526890110000 ms to writer queue
[INFO]  [2018-05-21 16:08:30] [org.apache.spark.streaming.CheckpointWriter]Deleting hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890005000.bk
[INFO]  [2018-05-21 16:08:30] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526890110000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890110000', took 4429 bytes and 157 ms
[INFO]  [2018-05-21 16:08:30] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526890110000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890110000'
[INFO]  [2018-05-21 16:08:30] [org.apache.spark.streaming.CheckpointWriter]Deleting hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890005000
[INFO]  [2018-05-21 16:08:30] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526890110000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890110000', took 4425 bytes and 143 ms
[INFO]  [2018-05-21 16:08:30] [org.apache.spark.streaming.DStreamGraph]Clearing checkpoint data for time 1526890110000 ms
[INFO]  [2018-05-21 16:08:30] [org.apache.spark.streaming.DStreamGraph]Cleared checkpoint data for time 1526890110000 ms
[INFO]  [2018-05-21 16:08:30] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO]  [2018-05-21 16:08:30] [org.apache.spark.streaming.util.FileBasedWriteAheadLog_ReceivedBlockTracker]Attempting to clear 1 old log files in hdfs://hadoop1:9000/lib/sparkstreaming/check_point/receivedBlockMetadata older than 1526890050000: hdfs://hadoop1:9000/lib/sparkstreaming/check_point/receivedBlockMetadata/log-1526889976364-1526890036364
[INFO]  [2018-05-21 16:08:30] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 
[INFO]  [2018-05-21 16:08:30] [org.apache.spark.streaming.util.FileBasedWriteAheadLog_ReceivedBlockTracker]Cleared log files in hdfs://hadoop1:9000/lib/sparkstreaming/check_point/receivedBlockMetadata older than 1526890050000
[INFO]  [2018-05-21 16:08:45] [org.apache.spark.streaming.dstream.FileInputDStream]Finding new files took 20 ms
[INFO]  [2018-05-21 16:08:45] [org.apache.spark.streaming.dstream.FileInputDStream]New files at time 1526890125000 ms:

[INFO]  [2018-05-21 16:08:45] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526890125000 ms
[INFO]  [2018-05-21 16:08:45] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526890125000 ms
[INFO]  [2018-05-21 16:08:45] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526890125000 ms
[INFO]  [2018-05-21 16:08:45] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526890125000 ms
[INFO]  [2018-05-21 16:08:45] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526890125000 ms to writer queue
[INFO]  [2018-05-21 16:08:45] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526890125000 ms.0 from job set of time 1526890125000 ms
[INFO]  [2018-05-21 16:08:45] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526890125000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890125000'
[INFO]  [2018-05-21 16:08:45] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:08:45] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 23 (mapToPair at HdfsSparkStreamT.java:56)
[INFO]  [2018-05-21 16:08:45] [org.apache.spark.scheduler.DAGScheduler]Got job 8 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:08:45] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 17 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:08:45] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 16)
[INFO]  [2018-05-21 16:08:45] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:08:45] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 17 (ShuffledRDD[24] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:08:45] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_8 stored as values in memory (estimated size 3.8 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:08:45] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_8_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:08:45] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_8_piece0 in memory on 172.18.42.247:50738 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:08:45] [org.apache.spark.SparkContext]Created broadcast 8 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:08:45] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 17 (ShuffledRDD[24] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:08:45] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 17.0 with 1 tasks
[INFO]  [2018-05-21 16:08:45] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 17.0 (TID 8, localhost, partition 0, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:08:45] [org.apache.spark.executor.Executor]Running task 0.0 in stage 17.0 (TID 8)
[INFO]  [2018-05-21 16:08:45] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:08:45] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 16:08:45] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 17.0 (TID 8). 1549 bytes result sent to driver
[INFO]  [2018-05-21 16:08:45] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 17.0 (TID 8) in 7 ms on localhost (1/1)
[INFO]  [2018-05-21 16:08:45] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 17.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:08:45] [org.apache.spark.scheduler.DAGScheduler]ResultStage 17 (print at HdfsSparkStreamT.java:59) finished in 0.007 s
[INFO]  [2018-05-21 16:08:45] [org.apache.spark.scheduler.DAGScheduler]Job 8 finished: print at HdfsSparkStreamT.java:59, took 0.030149 s
[INFO]  [2018-05-21 16:08:45] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:08:45] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 4 is 83 bytes
[INFO]  [2018-05-21 16:08:45] [org.apache.spark.scheduler.DAGScheduler]Got job 9 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:08:45] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 19 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:08:45] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 18)
[INFO]  [2018-05-21 16:08:45] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:08:45] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 19 (ShuffledRDD[24] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:08:45] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_9 stored as values in memory (estimated size 3.8 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:08:45] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_9_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:08:45] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_9_piece0 in memory on 172.18.42.247:50738 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:08:45] [org.apache.spark.SparkContext]Created broadcast 9 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:08:45] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 19 (ShuffledRDD[24] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:08:45] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 19.0 with 1 tasks
[INFO]  [2018-05-21 16:08:45] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 19.0 (TID 9, localhost, partition 1, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:08:45] [org.apache.spark.executor.Executor]Running task 0.0 in stage 19.0 (TID 9)
[INFO]  [2018-05-21 16:08:45] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:08:45] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 3 ms
[INFO]  [2018-05-21 16:08:45] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 19.0 (TID 9). 1549 bytes result sent to driver
[INFO]  [2018-05-21 16:08:45] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 19.0 (TID 9) in 11 ms on localhost (1/1)
[INFO]  [2018-05-21 16:08:45] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 19.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:08:45] [org.apache.spark.scheduler.DAGScheduler]ResultStage 19 (print at HdfsSparkStreamT.java:59) finished in 0.012 s
[INFO]  [2018-05-21 16:08:45] [org.apache.spark.scheduler.DAGScheduler]Job 9 finished: print at HdfsSparkStreamT.java:59, took 0.040215 s
[INFO]  [2018-05-21 16:08:45] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526890125000 ms.0 from job set of time 1526890125000 ms
[INFO]  [2018-05-21 16:08:45] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.135 s for time 1526890125000 ms (execution: 0.088 s)
[INFO]  [2018-05-21 16:08:45] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 19 from persistence list
[INFO]  [2018-05-21 16:08:45] [org.apache.spark.storage.BlockManager]Removing RDD 19
[INFO]  [2018-05-21 16:08:45] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 18 from persistence list
[INFO]  [2018-05-21 16:08:45] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 17 from persistence list
[INFO]  [2018-05-21 16:08:45] [org.apache.spark.storage.BlockManager]Removing RDD 18
[INFO]  [2018-05-21 16:08:45] [org.apache.spark.storage.BlockManager]Removing RDD 17
[INFO]  [2018-05-21 16:08:45] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 16 from persistence list
[INFO]  [2018-05-21 16:08:45] [org.apache.spark.storage.BlockManager]Removing RDD 16
[INFO]  [2018-05-21 16:08:45] [org.apache.spark.streaming.dstream.FileInputDStream]Cleared 0 old files that were older than 1526890065000 ms: 
[INFO]  [2018-05-21 16:08:45] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526890125000 ms
[INFO]  [2018-05-21 16:08:45] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526890125000 ms
[INFO]  [2018-05-21 16:08:45] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526890125000 ms
[INFO]  [2018-05-21 16:08:45] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526890125000 ms to writer queue
[INFO]  [2018-05-21 16:08:45] [org.apache.spark.streaming.CheckpointWriter]Deleting hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890020000.bk
[INFO]  [2018-05-21 16:08:45] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526890125000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890125000', took 4437 bytes and 143 ms
[INFO]  [2018-05-21 16:08:45] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526890125000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890125000'
[INFO]  [2018-05-21 16:08:45] [org.apache.spark.streaming.CheckpointWriter]Deleting hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890020000
[INFO]  [2018-05-21 16:08:45] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526890125000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890125000', took 4433 bytes and 60 ms
[INFO]  [2018-05-21 16:08:45] [org.apache.spark.streaming.DStreamGraph]Clearing checkpoint data for time 1526890125000 ms
[INFO]  [2018-05-21 16:08:45] [org.apache.spark.streaming.DStreamGraph]Cleared checkpoint data for time 1526890125000 ms
[INFO]  [2018-05-21 16:08:45] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO]  [2018-05-21 16:08:45] [org.apache.spark.streaming.util.FileBasedWriteAheadLog_ReceivedBlockTracker]Attempting to clear 0 old log files in hdfs://hadoop1:9000/lib/sparkstreaming/check_point/receivedBlockMetadata older than 1526890065000: 
[INFO]  [2018-05-21 16:08:45] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 
[INFO]  [2018-05-21 16:09:00] [org.apache.spark.streaming.dstream.FileInputDStream]Finding new files took 19 ms
[INFO]  [2018-05-21 16:09:00] [org.apache.spark.streaming.dstream.FileInputDStream]New files at time 1526890140000 ms:

[INFO]  [2018-05-21 16:09:00] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526890140000 ms
[INFO]  [2018-05-21 16:09:00] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526890140000 ms
[INFO]  [2018-05-21 16:09:00] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526890140000 ms
[INFO]  [2018-05-21 16:09:00] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526890140000 ms
[INFO]  [2018-05-21 16:09:00] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526890140000 ms to writer queue
[INFO]  [2018-05-21 16:09:00] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526890140000 ms.0 from job set of time 1526890140000 ms
[INFO]  [2018-05-21 16:09:00] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526890140000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890140000'
[INFO]  [2018-05-21 16:09:00] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:09:00] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 28 (mapToPair at HdfsSparkStreamT.java:56)
[INFO]  [2018-05-21 16:09:00] [org.apache.spark.scheduler.DAGScheduler]Got job 10 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:09:00] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 21 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:09:00] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 20)
[INFO]  [2018-05-21 16:09:00] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:09:00] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 21 (ShuffledRDD[29] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:09:00] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_10 stored as values in memory (estimated size 3.8 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:09:00] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_10_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:09:00] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_10_piece0 in memory on 172.18.42.247:50738 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:09:00] [org.apache.spark.SparkContext]Created broadcast 10 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:09:00] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 21 (ShuffledRDD[29] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:09:00] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 21.0 with 1 tasks
[INFO]  [2018-05-21 16:09:00] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 21.0 (TID 10, localhost, partition 0, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:09:00] [org.apache.spark.executor.Executor]Running task 0.0 in stage 21.0 (TID 10)
[INFO]  [2018-05-21 16:09:00] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:09:00] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 16:09:00] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 21.0 (TID 10). 1549 bytes result sent to driver
[INFO]  [2018-05-21 16:09:00] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 21.0 (TID 10) in 9 ms on localhost (1/1)
[INFO]  [2018-05-21 16:09:00] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 21.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:09:00] [org.apache.spark.scheduler.DAGScheduler]ResultStage 21 (print at HdfsSparkStreamT.java:59) finished in 0.010 s
[INFO]  [2018-05-21 16:09:00] [org.apache.spark.scheduler.DAGScheduler]Job 10 finished: print at HdfsSparkStreamT.java:59, took 0.021977 s
[INFO]  [2018-05-21 16:09:00] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:09:00] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 5 is 83 bytes
[INFO]  [2018-05-21 16:09:00] [org.apache.spark.scheduler.DAGScheduler]Got job 11 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:09:00] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 23 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:09:00] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 22)
[INFO]  [2018-05-21 16:09:00] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:09:00] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 23 (ShuffledRDD[29] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:09:00] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_11 stored as values in memory (estimated size 3.8 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:09:00] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_11_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:09:00] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_11_piece0 in memory on 172.18.42.247:50738 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:09:00] [org.apache.spark.SparkContext]Created broadcast 11 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:09:00] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 23 (ShuffledRDD[29] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:09:00] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 23.0 with 1 tasks
[INFO]  [2018-05-21 16:09:00] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 23.0 (TID 11, localhost, partition 1, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:09:00] [org.apache.spark.executor.Executor]Running task 0.0 in stage 23.0 (TID 11)
[INFO]  [2018-05-21 16:09:00] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:09:00] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 16:09:00] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 23.0 (TID 11). 1549 bytes result sent to driver
[INFO]  [2018-05-21 16:09:00] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 23.0 (TID 11) in 8 ms on localhost (1/1)
[INFO]  [2018-05-21 16:09:00] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 23.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:09:00] [org.apache.spark.scheduler.DAGScheduler]ResultStage 23 (print at HdfsSparkStreamT.java:59) finished in 0.010 s
[INFO]  [2018-05-21 16:09:00] [org.apache.spark.scheduler.DAGScheduler]Job 11 finished: print at HdfsSparkStreamT.java:59, took 0.022859 s
[INFO]  [2018-05-21 16:09:00] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526890140000 ms.0 from job set of time 1526890140000 ms
[INFO]  [2018-05-21 16:09:00] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.088 s for time 1526890140000 ms (execution: 0.059 s)
[INFO]  [2018-05-21 16:09:00] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 24 from persistence list
[INFO]  [2018-05-21 16:09:00] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 23 from persistence list
[INFO]  [2018-05-21 16:09:00] [org.apache.spark.storage.BlockManager]Removing RDD 24
[INFO]  [2018-05-21 16:09:00] [org.apache.spark.storage.BlockManager]Removing RDD 23
[INFO]  [2018-05-21 16:09:00] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 22 from persistence list
[INFO]  [2018-05-21 16:09:00] [org.apache.spark.storage.BlockManager]Removing RDD 22
[INFO]  [2018-05-21 16:09:00] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 21 from persistence list
[INFO]  [2018-05-21 16:09:00] [org.apache.spark.streaming.dstream.FileInputDStream]Cleared 1 old files that were older than 1526890080000 ms: 1526890065000 ms
[INFO]  [2018-05-21 16:09:00] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526890140000 ms
[INFO]  [2018-05-21 16:09:00] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526890140000 ms
[INFO]  [2018-05-21 16:09:00] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526890140000 ms
[INFO]  [2018-05-21 16:09:00] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526890140000 ms to writer queue
[INFO]  [2018-05-21 16:09:00] [org.apache.spark.storage.BlockManager]Removing RDD 21
[INFO]  [2018-05-21 16:09:00] [org.apache.spark.streaming.CheckpointWriter]Deleting hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890065000.bk
[INFO]  [2018-05-21 16:09:00] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526890140000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890140000', took 4428 bytes and 104 ms
[INFO]  [2018-05-21 16:09:00] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526890140000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890140000'
[INFO]  [2018-05-21 16:09:00] [org.apache.spark.streaming.CheckpointWriter]Deleting hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890065000
[INFO]  [2018-05-21 16:09:00] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526890140000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890140000', took 4426 bytes and 130 ms
[INFO]  [2018-05-21 16:09:00] [org.apache.spark.streaming.DStreamGraph]Clearing checkpoint data for time 1526890140000 ms
[INFO]  [2018-05-21 16:09:00] [org.apache.spark.streaming.DStreamGraph]Cleared checkpoint data for time 1526890140000 ms
[INFO]  [2018-05-21 16:09:00] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO]  [2018-05-21 16:09:00] [org.apache.spark.streaming.util.FileBasedWriteAheadLog_ReceivedBlockTracker]Attempting to clear 0 old log files in hdfs://hadoop1:9000/lib/sparkstreaming/check_point/receivedBlockMetadata older than 1526890080000: 
[INFO]  [2018-05-21 16:09:00] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1526890065000 ms
[INFO]  [2018-05-21 16:09:15] [org.apache.spark.streaming.dstream.FileInputDStream]Finding new files took 33 ms
[INFO]  [2018-05-21 16:09:15] [org.apache.spark.streaming.dstream.FileInputDStream]New files at time 1526890155000 ms:

[INFO]  [2018-05-21 16:09:15] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526890155000 ms
[INFO]  [2018-05-21 16:09:15] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526890155000 ms
[INFO]  [2018-05-21 16:09:15] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526890155000 ms
[INFO]  [2018-05-21 16:09:15] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526890155000 ms
[INFO]  [2018-05-21 16:09:15] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526890155000 ms to writer queue
[INFO]  [2018-05-21 16:09:15] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526890155000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890155000'
[INFO]  [2018-05-21 16:09:15] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526890155000 ms.0 from job set of time 1526890155000 ms
[INFO]  [2018-05-21 16:09:15] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:09:15] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 33 (mapToPair at HdfsSparkStreamT.java:56)
[INFO]  [2018-05-21 16:09:15] [org.apache.spark.scheduler.DAGScheduler]Got job 12 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:09:15] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 25 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:09:15] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 24)
[INFO]  [2018-05-21 16:09:15] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:09:15] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 25 (ShuffledRDD[34] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:09:15] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_12 stored as values in memory (estimated size 3.8 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:09:15] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_12_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:09:15] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_12_piece0 in memory on 172.18.42.247:50738 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:09:15] [org.apache.spark.SparkContext]Created broadcast 12 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:09:15] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 25 (ShuffledRDD[34] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:09:15] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 25.0 with 1 tasks
[INFO]  [2018-05-21 16:09:15] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 25.0 (TID 12, localhost, partition 0, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:09:15] [org.apache.spark.executor.Executor]Running task 0.0 in stage 25.0 (TID 12)
[INFO]  [2018-05-21 16:09:15] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:09:15] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 16:09:15] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 25.0 (TID 12). 1549 bytes result sent to driver
[INFO]  [2018-05-21 16:09:15] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 25.0 (TID 12) in 5 ms on localhost (1/1)
[INFO]  [2018-05-21 16:09:15] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 25.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:09:15] [org.apache.spark.scheduler.DAGScheduler]ResultStage 25 (print at HdfsSparkStreamT.java:59) finished in 0.006 s
[INFO]  [2018-05-21 16:09:15] [org.apache.spark.scheduler.DAGScheduler]Job 12 finished: print at HdfsSparkStreamT.java:59, took 0.017567 s
[INFO]  [2018-05-21 16:09:15] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:09:15] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 6 is 83 bytes
[INFO]  [2018-05-21 16:09:15] [org.apache.spark.scheduler.DAGScheduler]Got job 13 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:09:15] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 27 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:09:15] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 26)
[INFO]  [2018-05-21 16:09:15] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:09:15] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 27 (ShuffledRDD[34] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:09:15] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_13 stored as values in memory (estimated size 3.8 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:09:15] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_13_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:09:15] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_13_piece0 in memory on 172.18.42.247:50738 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:09:15] [org.apache.spark.SparkContext]Created broadcast 13 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:09:15] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 27 (ShuffledRDD[34] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:09:15] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 27.0 with 1 tasks
[INFO]  [2018-05-21 16:09:15] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 27.0 (TID 13, localhost, partition 1, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:09:15] [org.apache.spark.executor.Executor]Running task 0.0 in stage 27.0 (TID 13)
[INFO]  [2018-05-21 16:09:15] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:09:15] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO]  [2018-05-21 16:09:15] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 27.0 (TID 13). 1549 bytes result sent to driver
[INFO]  [2018-05-21 16:09:15] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 27.0 (TID 13) in 7 ms on localhost (1/1)
[INFO]  [2018-05-21 16:09:15] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 27.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:09:15] [org.apache.spark.scheduler.DAGScheduler]ResultStage 27 (print at HdfsSparkStreamT.java:59) finished in 0.009 s
[INFO]  [2018-05-21 16:09:15] [org.apache.spark.scheduler.DAGScheduler]Job 13 finished: print at HdfsSparkStreamT.java:59, took 0.027636 s
[INFO]  [2018-05-21 16:09:15] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526890155000 ms.0 from job set of time 1526890155000 ms
[INFO]  [2018-05-21 16:09:15] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.108 s for time 1526890155000 ms (execution: 0.059 s)
[INFO]  [2018-05-21 16:09:15] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 29 from persistence list
[INFO]  [2018-05-21 16:09:15] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 28 from persistence list
[INFO]  [2018-05-21 16:09:15] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 27 from persistence list
[INFO]  [2018-05-21 16:09:15] [org.apache.spark.storage.BlockManager]Removing RDD 28
[INFO]  [2018-05-21 16:09:15] [org.apache.spark.storage.BlockManager]Removing RDD 29
[INFO]  [2018-05-21 16:09:15] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 26 from persistence list
[INFO]  [2018-05-21 16:09:15] [org.apache.spark.storage.BlockManager]Removing RDD 26
[INFO]  [2018-05-21 16:09:15] [org.apache.spark.streaming.dstream.FileInputDStream]Cleared 1 old files that were older than 1526890095000 ms: 1526890080000 ms
[INFO]  [2018-05-21 16:09:15] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526890155000 ms
[INFO]  [2018-05-21 16:09:15] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526890155000 ms
[INFO]  [2018-05-21 16:09:15] [org.apache.spark.storage.BlockManager]Removing RDD 27
[INFO]  [2018-05-21 16:09:15] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526890155000 ms
[INFO]  [2018-05-21 16:09:15] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526890155000 ms to writer queue
[INFO]  [2018-05-21 16:09:15] [org.apache.spark.streaming.CheckpointWriter]Deleting hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890080000.bk
[INFO]  [2018-05-21 16:09:15] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526890155000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890155000', took 4434 bytes and 138 ms
[INFO]  [2018-05-21 16:09:15] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526890155000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890155000'
[INFO]  [2018-05-21 16:09:15] [org.apache.spark.streaming.CheckpointWriter]Deleting hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890080000
[INFO]  [2018-05-21 16:09:15] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526890155000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890155000', took 4433 bytes and 88 ms
[INFO]  [2018-05-21 16:09:15] [org.apache.spark.streaming.DStreamGraph]Clearing checkpoint data for time 1526890155000 ms
[INFO]  [2018-05-21 16:09:15] [org.apache.spark.streaming.DStreamGraph]Cleared checkpoint data for time 1526890155000 ms
[INFO]  [2018-05-21 16:09:15] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO]  [2018-05-21 16:09:15] [org.apache.spark.streaming.util.FileBasedWriteAheadLog_ReceivedBlockTracker]Attempting to clear 0 old log files in hdfs://hadoop1:9000/lib/sparkstreaming/check_point/receivedBlockMetadata older than 1526890095000: 
[INFO]  [2018-05-21 16:09:15] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1526890080000 ms
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.streaming.dstream.FileInputDStream]Finding new files took 41 ms
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.streaming.dstream.FileInputDStream]New files at time 1526890170000 ms:

[INFO]  [2018-05-21 16:09:30] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526890170000 ms
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526890170000 ms
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526890170000 ms
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526890170000 ms
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526890170000 ms.0 from job set of time 1526890170000 ms
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526890170000 ms to writer queue
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 38 (mapToPair at HdfsSparkStreamT.java:56)
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.scheduler.DAGScheduler]Got job 14 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 29 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 28)
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 29 (ShuffledRDD[39] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_14 stored as values in memory (estimated size 3.8 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526890170000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890170000'
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_14_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_14_piece0 in memory on 172.18.42.247:50738 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.SparkContext]Created broadcast 14 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 29 (ShuffledRDD[39] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 29.0 with 1 tasks
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 29.0 (TID 14, localhost, partition 0, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.executor.Executor]Running task 0.0 in stage 29.0 (TID 14)
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_13_piece0 on 172.18.42.247:50738 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_10_piece0 on 172.18.42.247:50738 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 29.0 (TID 14). 1549 bytes result sent to driver
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_11_piece0 on 172.18.42.247:50738 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 29.0 (TID 14) in 30 ms on localhost (1/1)
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 29.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.scheduler.DAGScheduler]ResultStage 29 (print at HdfsSparkStreamT.java:59) finished in 0.035 s
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.scheduler.DAGScheduler]Job 14 finished: print at HdfsSparkStreamT.java:59, took 0.054508 s
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 7 is 83 bytes
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.scheduler.DAGScheduler]Got job 15 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 31 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 30)
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 31 (ShuffledRDD[39] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_15 stored as values in memory (estimated size 3.8 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_15_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_12_piece0 on 172.18.42.247:50738 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_15_piece0 in memory on 172.18.42.247:50738 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.SparkContext]Created broadcast 15 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 31 (ShuffledRDD[39] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 31.0 with 1 tasks
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 31.0 (TID 15, localhost, partition 1, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.ContextCleaner]Cleaned shuffle 0
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.executor.Executor]Running task 0.0 in stage 31.0 (TID 15)
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_1_piece0 on 172.18.42.247:50738 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 2 ms
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 31.0 (TID 15). 1549 bytes result sent to driver
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 31.0 (TID 15) in 83 ms on localhost (1/1)
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.scheduler.DAGScheduler]ResultStage 31 (print at HdfsSparkStreamT.java:59) finished in 0.087 s
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.scheduler.DAGScheduler]Job 15 finished: print at HdfsSparkStreamT.java:59, took 0.106216 s
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 31.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526890170000 ms.0 from job set of time 1526890170000 ms
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.230 s for time 1526890170000 ms (execution: 0.175 s)
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 34 from persistence list
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.ContextCleaner]Cleaned shuffle 1
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 33 from persistence list
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.storage.BlockManager]Removing RDD 34
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 32 from persistence list
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.storage.BlockManager]Removing RDD 33
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_2_piece0 on 172.18.42.247:50738 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 31 from persistence list
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.streaming.dstream.FileInputDStream]Cleared 1 old files that were older than 1526890110000 ms: 1526890095000 ms
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526890170000 ms
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526890170000 ms
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526890170000 ms
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.storage.BlockManager]Removing RDD 32
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526890170000 ms to writer queue
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.storage.BlockManager]Removing RDD 31
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_3_piece0 on 172.18.42.247:50738 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.ContextCleaner]Cleaned shuffle 2
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_4_piece0 on 172.18.42.247:50738 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_5_piece0 on 172.18.42.247:50738 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.ContextCleaner]Cleaned shuffle 3
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.streaming.CheckpointWriter]Deleting hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890095000.bk
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_6_piece0 on 172.18.42.247:50738 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526890170000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890170000', took 4434 bytes and 197 ms
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526890170000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890170000'
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_7_piece0 on 172.18.42.247:50738 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.ContextCleaner]Cleaned shuffle 4
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_8_piece0 on 172.18.42.247:50738 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_9_piece0 on 172.18.42.247:50738 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.ContextCleaner]Cleaned shuffle 5
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.streaming.CheckpointWriter]Deleting hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890095000
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526890170000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890170000', took 4431 bytes and 150 ms
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.streaming.DStreamGraph]Clearing checkpoint data for time 1526890170000 ms
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.streaming.DStreamGraph]Cleared checkpoint data for time 1526890170000 ms
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.streaming.util.FileBasedWriteAheadLog_ReceivedBlockTracker]Attempting to clear 0 old log files in hdfs://hadoop1:9000/lib/sparkstreaming/check_point/receivedBlockMetadata older than 1526890110000: 
[INFO]  [2018-05-21 16:09:30] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1526890095000 ms
[INFO]  [2018-05-21 16:09:45] [org.apache.spark.streaming.dstream.FileInputDStream]Finding new files took 21 ms
[INFO]  [2018-05-21 16:09:45] [org.apache.spark.streaming.dstream.FileInputDStream]New files at time 1526890185000 ms:

[INFO]  [2018-05-21 16:09:45] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526890185000 ms
[INFO]  [2018-05-21 16:09:45] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526890185000 ms
[INFO]  [2018-05-21 16:09:45] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526890185000 ms
[INFO]  [2018-05-21 16:09:45] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526890185000 ms
[INFO]  [2018-05-21 16:09:45] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526890185000 ms to writer queue
[INFO]  [2018-05-21 16:09:45] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526890185000 ms.0 from job set of time 1526890185000 ms
[INFO]  [2018-05-21 16:09:45] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526890185000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890185000'
[INFO]  [2018-05-21 16:09:45] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:09:45] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 43 (mapToPair at HdfsSparkStreamT.java:56)
[INFO]  [2018-05-21 16:09:45] [org.apache.spark.scheduler.DAGScheduler]Got job 16 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:09:45] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 33 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:09:45] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 32)
[INFO]  [2018-05-21 16:09:45] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:09:45] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 33 (ShuffledRDD[44] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:09:45] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_16 stored as values in memory (estimated size 3.8 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:09:45] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_16_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:09:45] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_16_piece0 in memory on 172.18.42.247:50738 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:09:45] [org.apache.spark.SparkContext]Created broadcast 16 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:09:45] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 33 (ShuffledRDD[44] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:09:45] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 33.0 with 1 tasks
[INFO]  [2018-05-21 16:09:45] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 33.0 (TID 16, localhost, partition 0, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:09:45] [org.apache.spark.executor.Executor]Running task 0.0 in stage 33.0 (TID 16)
[INFO]  [2018-05-21 16:09:45] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:09:45] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 16:09:45] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 33.0 (TID 16). 1470 bytes result sent to driver
[INFO]  [2018-05-21 16:09:45] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 33.0 (TID 16) in 10 ms on localhost (1/1)
[INFO]  [2018-05-21 16:09:45] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 33.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:09:45] [org.apache.spark.scheduler.DAGScheduler]ResultStage 33 (print at HdfsSparkStreamT.java:59) finished in 0.010 s
[INFO]  [2018-05-21 16:09:45] [org.apache.spark.scheduler.DAGScheduler]Job 16 finished: print at HdfsSparkStreamT.java:59, took 0.022528 s
[INFO]  [2018-05-21 16:09:45] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:09:45] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 8 is 83 bytes
[INFO]  [2018-05-21 16:09:45] [org.apache.spark.scheduler.DAGScheduler]Got job 17 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:09:45] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 35 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:09:45] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 34)
[INFO]  [2018-05-21 16:09:45] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:09:45] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 35 (ShuffledRDD[44] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:09:45] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_17 stored as values in memory (estimated size 3.8 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:09:45] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_17_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:09:45] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_17_piece0 in memory on 172.18.42.247:50738 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:09:45] [org.apache.spark.SparkContext]Created broadcast 17 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:09:45] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 35 (ShuffledRDD[44] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:09:45] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 35.0 with 1 tasks
[INFO]  [2018-05-21 16:09:45] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 35.0 (TID 17, localhost, partition 1, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:09:45] [org.apache.spark.executor.Executor]Running task 0.0 in stage 35.0 (TID 17)
[INFO]  [2018-05-21 16:09:45] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:09:45] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 16:09:45] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 35.0 (TID 17). 1549 bytes result sent to driver
[INFO]  [2018-05-21 16:09:45] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 35.0 (TID 17) in 8 ms on localhost (1/1)
[INFO]  [2018-05-21 16:09:45] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 35.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:09:45] [org.apache.spark.scheduler.DAGScheduler]ResultStage 35 (print at HdfsSparkStreamT.java:59) finished in 0.010 s
[INFO]  [2018-05-21 16:09:45] [org.apache.spark.scheduler.DAGScheduler]Job 17 finished: print at HdfsSparkStreamT.java:59, took 0.020666 s
[INFO]  [2018-05-21 16:09:45] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526890185000 ms.0 from job set of time 1526890185000 ms
[INFO]  [2018-05-21 16:09:45] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.094 s for time 1526890185000 ms (execution: 0.057 s)
[INFO]  [2018-05-21 16:09:45] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 39 from persistence list
[INFO]  [2018-05-21 16:09:45] [org.apache.spark.storage.BlockManager]Removing RDD 39
[INFO]  [2018-05-21 16:09:45] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 38 from persistence list
[INFO]  [2018-05-21 16:09:45] [org.apache.spark.storage.BlockManager]Removing RDD 38
[INFO]  [2018-05-21 16:09:45] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 37 from persistence list
[INFO]  [2018-05-21 16:09:45] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 36 from persistence list
[INFO]  [2018-05-21 16:09:45] [org.apache.spark.storage.BlockManager]Removing RDD 37
[INFO]  [2018-05-21 16:09:45] [org.apache.spark.storage.BlockManager]Removing RDD 36
[INFO]  [2018-05-21 16:09:45] [org.apache.spark.streaming.dstream.FileInputDStream]Cleared 1 old files that were older than 1526890125000 ms: 1526890110000 ms
[INFO]  [2018-05-21 16:09:45] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526890185000 ms
[INFO]  [2018-05-21 16:09:45] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526890185000 ms
[INFO]  [2018-05-21 16:09:45] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526890185000 ms
[INFO]  [2018-05-21 16:09:45] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526890185000 ms to writer queue
[INFO]  [2018-05-21 16:09:45] [org.apache.spark.streaming.CheckpointWriter]Deleting hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890110000.bk
[INFO]  [2018-05-21 16:09:45] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526890185000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890185000', took 4435 bytes and 88 ms
[INFO]  [2018-05-21 16:09:45] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526890185000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890185000'
[INFO]  [2018-05-21 16:09:45] [org.apache.spark.streaming.CheckpointWriter]Deleting hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890110000
[INFO]  [2018-05-21 16:09:45] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526890185000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890185000', took 4432 bytes and 90 ms
[INFO]  [2018-05-21 16:09:45] [org.apache.spark.streaming.DStreamGraph]Clearing checkpoint data for time 1526890185000 ms
[INFO]  [2018-05-21 16:09:45] [org.apache.spark.streaming.DStreamGraph]Cleared checkpoint data for time 1526890185000 ms
[INFO]  [2018-05-21 16:09:45] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO]  [2018-05-21 16:09:45] [org.apache.spark.streaming.util.FileBasedWriteAheadLog_ReceivedBlockTracker]Attempting to clear 0 old log files in hdfs://hadoop1:9000/lib/sparkstreaming/check_point/receivedBlockMetadata older than 1526890125000: 
[INFO]  [2018-05-21 16:09:45] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1526890110000 ms
[INFO]  [2018-05-21 16:10:00] [org.apache.spark.streaming.dstream.FileInputDStream]Finding new files took 26 ms
[INFO]  [2018-05-21 16:10:00] [org.apache.spark.streaming.dstream.FileInputDStream]New files at time 1526890200000 ms:

[INFO]  [2018-05-21 16:10:00] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526890200000 ms
[INFO]  [2018-05-21 16:10:00] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526890200000 ms
[INFO]  [2018-05-21 16:10:00] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526890200000 ms
[INFO]  [2018-05-21 16:10:00] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526890200000 ms
[INFO]  [2018-05-21 16:10:00] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526890200000 ms to writer queue
[INFO]  [2018-05-21 16:10:00] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526890200000 ms.0 from job set of time 1526890200000 ms
[INFO]  [2018-05-21 16:10:00] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526890200000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890200000'
[INFO]  [2018-05-21 16:10:00] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:10:00] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 48 (mapToPair at HdfsSparkStreamT.java:56)
[INFO]  [2018-05-21 16:10:00] [org.apache.spark.scheduler.DAGScheduler]Got job 18 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:10:00] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 37 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:10:00] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 36)
[INFO]  [2018-05-21 16:10:00] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:10:00] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 37 (ShuffledRDD[49] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:10:00] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_18 stored as values in memory (estimated size 3.8 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:10:00] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_18_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:10:00] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_18_piece0 in memory on 172.18.42.247:50738 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:10:00] [org.apache.spark.SparkContext]Created broadcast 18 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:10:00] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 37 (ShuffledRDD[49] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:10:00] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 37.0 with 1 tasks
[INFO]  [2018-05-21 16:10:00] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 37.0 (TID 18, localhost, partition 0, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:10:00] [org.apache.spark.executor.Executor]Running task 0.0 in stage 37.0 (TID 18)
[INFO]  [2018-05-21 16:10:00] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:10:00] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO]  [2018-05-21 16:10:00] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 37.0 (TID 18). 1549 bytes result sent to driver
[INFO]  [2018-05-21 16:10:00] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 37.0 (TID 18) in 11 ms on localhost (1/1)
[INFO]  [2018-05-21 16:10:00] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 37.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:10:00] [org.apache.spark.scheduler.DAGScheduler]ResultStage 37 (print at HdfsSparkStreamT.java:59) finished in 0.024 s
[INFO]  [2018-05-21 16:10:00] [org.apache.spark.scheduler.DAGScheduler]Job 18 finished: print at HdfsSparkStreamT.java:59, took 0.041931 s
[INFO]  [2018-05-21 16:10:00] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:10:00] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 9 is 83 bytes
[INFO]  [2018-05-21 16:10:00] [org.apache.spark.scheduler.DAGScheduler]Got job 19 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:10:00] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 39 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:10:00] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 38)
[INFO]  [2018-05-21 16:10:00] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:10:00] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 39 (ShuffledRDD[49] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:10:00] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_19 stored as values in memory (estimated size 3.8 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:10:00] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_19_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:10:00] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_19_piece0 in memory on 172.18.42.247:50738 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:10:00] [org.apache.spark.SparkContext]Created broadcast 19 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:10:00] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 39 (ShuffledRDD[49] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:10:00] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 39.0 with 1 tasks
[INFO]  [2018-05-21 16:10:00] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 39.0 (TID 19, localhost, partition 1, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:10:00] [org.apache.spark.executor.Executor]Running task 0.0 in stage 39.0 (TID 19)
[INFO]  [2018-05-21 16:10:00] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:10:00] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 16:10:00] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 39.0 (TID 19). 1549 bytes result sent to driver
[INFO]  [2018-05-21 16:10:00] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 39.0 (TID 19) in 8 ms on localhost (1/1)
[INFO]  [2018-05-21 16:10:00] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 39.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:10:00] [org.apache.spark.scheduler.DAGScheduler]ResultStage 39 (print at HdfsSparkStreamT.java:59) finished in 0.020 s
[INFO]  [2018-05-21 16:10:00] [org.apache.spark.scheduler.DAGScheduler]Job 19 finished: print at HdfsSparkStreamT.java:59, took 0.031572 s
[INFO]  [2018-05-21 16:10:00] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526890200000 ms.0 from job set of time 1526890200000 ms
[INFO]  [2018-05-21 16:10:00] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.131 s for time 1526890200000 ms (execution: 0.087 s)
[INFO]  [2018-05-21 16:10:00] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 44 from persistence list
[INFO]  [2018-05-21 16:10:00] [org.apache.spark.storage.BlockManager]Removing RDD 44
[INFO]  [2018-05-21 16:10:00] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 43 from persistence list
[INFO]  [2018-05-21 16:10:00] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 42 from persistence list
[INFO]  [2018-05-21 16:10:00] [org.apache.spark.storage.BlockManager]Removing RDD 43
[INFO]  [2018-05-21 16:10:00] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 41 from persistence list
[INFO]  [2018-05-21 16:10:00] [org.apache.spark.storage.BlockManager]Removing RDD 42
[INFO]  [2018-05-21 16:10:00] [org.apache.spark.streaming.dstream.FileInputDStream]Cleared 1 old files that were older than 1526890140000 ms: 1526890125000 ms
[INFO]  [2018-05-21 16:10:00] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526890200000 ms
[INFO]  [2018-05-21 16:10:00] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526890200000 ms
[INFO]  [2018-05-21 16:10:00] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526890200000 ms
[INFO]  [2018-05-21 16:10:00] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526890200000 ms to writer queue
[INFO]  [2018-05-21 16:10:00] [org.apache.spark.storage.BlockManager]Removing RDD 41
[INFO]  [2018-05-21 16:10:00] [org.apache.spark.streaming.CheckpointWriter]Deleting hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890125000.bk
[INFO]  [2018-05-21 16:10:00] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526890200000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890200000', took 4435 bytes and 187 ms
[INFO]  [2018-05-21 16:10:00] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526890200000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890200000'
[INFO]  [2018-05-21 16:10:00] [org.apache.spark.streaming.CheckpointWriter]Deleting hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890125000
[INFO]  [2018-05-21 16:10:00] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526890200000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890200000', took 4434 bytes and 437 ms
[INFO]  [2018-05-21 16:10:00] [org.apache.spark.streaming.DStreamGraph]Clearing checkpoint data for time 1526890200000 ms
[INFO]  [2018-05-21 16:10:00] [org.apache.spark.streaming.DStreamGraph]Cleared checkpoint data for time 1526890200000 ms
[INFO]  [2018-05-21 16:10:00] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO]  [2018-05-21 16:10:00] [org.apache.spark.streaming.util.FileBasedWriteAheadLog_ReceivedBlockTracker]Attempting to clear 1 old log files in hdfs://hadoop1:9000/lib/sparkstreaming/check_point/receivedBlockMetadata older than 1526890140000: hdfs://hadoop1:9000/lib/sparkstreaming/check_point/receivedBlockMetadata/log-1526890066160-1526890126160
[INFO]  [2018-05-21 16:10:00] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1526890125000 ms
[INFO]  [2018-05-21 16:10:00] [org.apache.spark.streaming.util.FileBasedWriteAheadLog_ReceivedBlockTracker]Cleared log files in hdfs://hadoop1:9000/lib/sparkstreaming/check_point/receivedBlockMetadata older than 1526890140000
[INFO]  [2018-05-21 16:10:15] [org.apache.spark.streaming.dstream.FileInputDStream]Finding new files took 68 ms
[INFO]  [2018-05-21 16:10:15] [org.apache.spark.streaming.dstream.FileInputDStream]New files at time 1526890215000 ms:

[INFO]  [2018-05-21 16:10:15] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526890215000 ms
[INFO]  [2018-05-21 16:10:15] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526890215000 ms
[INFO]  [2018-05-21 16:10:15] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526890215000 ms
[INFO]  [2018-05-21 16:10:15] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526890215000 ms
[INFO]  [2018-05-21 16:10:15] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526890215000 ms.0 from job set of time 1526890215000 ms
[INFO]  [2018-05-21 16:10:15] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526890215000 ms to writer queue
[INFO]  [2018-05-21 16:10:15] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:10:15] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526890215000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890215000'
[INFO]  [2018-05-21 16:10:15] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 53 (mapToPair at HdfsSparkStreamT.java:56)
[INFO]  [2018-05-21 16:10:15] [org.apache.spark.scheduler.DAGScheduler]Got job 20 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:10:15] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 41 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:10:15] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 40)
[INFO]  [2018-05-21 16:10:15] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:10:15] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 41 (ShuffledRDD[54] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:10:15] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_20 stored as values in memory (estimated size 3.8 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:10:15] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_20_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:10:15] [org.apache.spark.streaming.CheckpointWriter]Deleting hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890140000.bk
[INFO]  [2018-05-21 16:10:15] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526890215000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890215000', took 4429 bytes and 60 ms
[INFO]  [2018-05-21 16:10:15] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_20_piece0 in memory on 172.18.42.247:50738 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:10:15] [org.apache.spark.SparkContext]Created broadcast 20 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:10:15] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 41 (ShuffledRDD[54] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:10:15] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 41.0 with 1 tasks
[INFO]  [2018-05-21 16:10:15] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 41.0 (TID 20, localhost, partition 0, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:10:15] [org.apache.spark.executor.Executor]Running task 0.0 in stage 41.0 (TID 20)
[INFO]  [2018-05-21 16:10:15] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:10:15] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 3 ms
[INFO]  [2018-05-21 16:10:15] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 41.0 (TID 20). 1462 bytes result sent to driver
[INFO]  [2018-05-21 16:10:15] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 41.0 (TID 20) in 5 ms on localhost (1/1)
[INFO]  [2018-05-21 16:10:15] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 41.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:10:15] [org.apache.spark.scheduler.DAGScheduler]ResultStage 41 (print at HdfsSparkStreamT.java:59) finished in 0.012 s
[INFO]  [2018-05-21 16:10:15] [org.apache.spark.scheduler.DAGScheduler]Job 20 finished: print at HdfsSparkStreamT.java:59, took 0.079783 s
[INFO]  [2018-05-21 16:10:15] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:10:15] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 10 is 83 bytes
[INFO]  [2018-05-21 16:10:15] [org.apache.spark.scheduler.DAGScheduler]Got job 21 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:10:15] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 43 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:10:15] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 42)
[INFO]  [2018-05-21 16:10:15] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:10:15] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 43 (ShuffledRDD[54] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:10:15] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_21 stored as values in memory (estimated size 3.8 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:10:15] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_21_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:10:15] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_21_piece0 in memory on 172.18.42.247:50738 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:10:15] [org.apache.spark.SparkContext]Created broadcast 21 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:10:15] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 43 (ShuffledRDD[54] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:10:15] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 43.0 with 1 tasks
[INFO]  [2018-05-21 16:10:15] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 43.0 (TID 21, localhost, partition 1, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:10:15] [org.apache.spark.executor.Executor]Running task 0.0 in stage 43.0 (TID 21)
[INFO]  [2018-05-21 16:10:15] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:10:15] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 3 ms
[INFO]  [2018-05-21 16:10:15] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 43.0 (TID 21). 1549 bytes result sent to driver
[INFO]  [2018-05-21 16:10:15] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 43.0 (TID 21) in 10 ms on localhost (1/1)
[INFO]  [2018-05-21 16:10:15] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 43.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:10:15] [org.apache.spark.scheduler.DAGScheduler]ResultStage 43 (print at HdfsSparkStreamT.java:59) finished in 0.010 s
[INFO]  [2018-05-21 16:10:15] [org.apache.spark.scheduler.DAGScheduler]Job 21 finished: print at HdfsSparkStreamT.java:59, took 0.018794 s
[INFO]  [2018-05-21 16:10:15] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526890215000 ms.0 from job set of time 1526890215000 ms
[INFO]  [2018-05-21 16:10:15] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.187 s for time 1526890215000 ms (execution: 0.110 s)
[INFO]  [2018-05-21 16:10:15] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 49 from persistence list
[INFO]  [2018-05-21 16:10:15] [org.apache.spark.storage.BlockManager]Removing RDD 49
[INFO]  [2018-05-21 16:10:15] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 48 from persistence list
[INFO]  [2018-05-21 16:10:15] [org.apache.spark.storage.BlockManager]Removing RDD 48
[INFO]  [2018-05-21 16:10:15] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 47 from persistence list
[INFO]  [2018-05-21 16:10:15] [org.apache.spark.storage.BlockManager]Removing RDD 47
[INFO]  [2018-05-21 16:10:15] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 46 from persistence list
[INFO]  [2018-05-21 16:10:15] [org.apache.spark.storage.BlockManager]Removing RDD 46
[INFO]  [2018-05-21 16:10:15] [org.apache.spark.streaming.dstream.FileInputDStream]Cleared 1 old files that were older than 1526890155000 ms: 1526890140000 ms
[INFO]  [2018-05-21 16:10:15] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526890215000 ms
[INFO]  [2018-05-21 16:10:15] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526890215000 ms
[INFO]  [2018-05-21 16:10:15] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526890215000 ms
[INFO]  [2018-05-21 16:10:15] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526890215000 ms to writer queue
[INFO]  [2018-05-21 16:10:15] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526890215000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890215000'
[INFO]  [2018-05-21 16:10:15] [org.apache.spark.streaming.CheckpointWriter]Deleting hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890140000
[INFO]  [2018-05-21 16:10:15] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526890215000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890215000', took 4428 bytes and 123 ms
[INFO]  [2018-05-21 16:10:15] [org.apache.spark.streaming.DStreamGraph]Clearing checkpoint data for time 1526890215000 ms
[INFO]  [2018-05-21 16:10:15] [org.apache.spark.streaming.DStreamGraph]Cleared checkpoint data for time 1526890215000 ms
[INFO]  [2018-05-21 16:10:15] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO]  [2018-05-21 16:10:15] [org.apache.spark.streaming.util.FileBasedWriteAheadLog_ReceivedBlockTracker]Attempting to clear 0 old log files in hdfs://hadoop1:9000/lib/sparkstreaming/check_point/receivedBlockMetadata older than 1526890155000: 
[INFO]  [2018-05-21 16:10:15] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1526890140000 ms
[INFO]  [2018-05-21 16:10:30] [org.apache.spark.streaming.dstream.FileInputDStream]Finding new files took 15 ms
[INFO]  [2018-05-21 16:10:30] [org.apache.spark.streaming.dstream.FileInputDStream]New files at time 1526890230000 ms:

[INFO]  [2018-05-21 16:10:30] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526890230000 ms
[INFO]  [2018-05-21 16:10:30] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526890230000 ms
[INFO]  [2018-05-21 16:10:30] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526890230000 ms
[INFO]  [2018-05-21 16:10:30] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526890230000 ms
[INFO]  [2018-05-21 16:10:30] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526890230000 ms to writer queue
[INFO]  [2018-05-21 16:10:30] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526890230000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890230000'
[INFO]  [2018-05-21 16:10:30] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526890230000 ms.0 from job set of time 1526890230000 ms
[INFO]  [2018-05-21 16:10:30] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:10:30] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 58 (mapToPair at HdfsSparkStreamT.java:56)
[INFO]  [2018-05-21 16:10:30] [org.apache.spark.scheduler.DAGScheduler]Got job 22 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:10:30] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 45 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:10:30] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 44)
[INFO]  [2018-05-21 16:10:30] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:10:30] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 45 (ShuffledRDD[59] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:10:30] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_22 stored as values in memory (estimated size 3.8 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:10:30] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_22_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:10:30] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_22_piece0 in memory on 172.18.42.247:50738 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:10:30] [org.apache.spark.SparkContext]Created broadcast 22 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:10:30] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 45 (ShuffledRDD[59] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:10:30] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 45.0 with 1 tasks
[INFO]  [2018-05-21 16:10:30] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 45.0 (TID 22, localhost, partition 0, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:10:30] [org.apache.spark.executor.Executor]Running task 0.0 in stage 45.0 (TID 22)
[INFO]  [2018-05-21 16:10:30] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:10:30] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 16:10:30] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 45.0 (TID 22). 1470 bytes result sent to driver
[INFO]  [2018-05-21 16:10:30] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 45.0 (TID 22) in 18 ms on localhost (1/1)
[INFO]  [2018-05-21 16:10:30] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 45.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:10:30] [org.apache.spark.scheduler.DAGScheduler]ResultStage 45 (print at HdfsSparkStreamT.java:59) finished in 0.020 s
[INFO]  [2018-05-21 16:10:30] [org.apache.spark.scheduler.DAGScheduler]Job 22 finished: print at HdfsSparkStreamT.java:59, took 0.031513 s
[INFO]  [2018-05-21 16:10:30] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:10:30] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 11 is 83 bytes
[INFO]  [2018-05-21 16:10:30] [org.apache.spark.scheduler.DAGScheduler]Got job 23 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:10:30] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 47 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:10:30] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 46)
[INFO]  [2018-05-21 16:10:30] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:10:30] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 47 (ShuffledRDD[59] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:10:30] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_23 stored as values in memory (estimated size 3.8 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:10:30] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_23_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:10:30] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_23_piece0 in memory on 172.18.42.247:50738 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:10:30] [org.apache.spark.SparkContext]Created broadcast 23 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:10:30] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 47 (ShuffledRDD[59] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:10:30] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 47.0 with 1 tasks
[INFO]  [2018-05-21 16:10:30] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 47.0 (TID 23, localhost, partition 1, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:10:30] [org.apache.spark.executor.Executor]Running task 0.0 in stage 47.0 (TID 23)
[INFO]  [2018-05-21 16:10:30] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:10:30] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 16:10:30] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 47.0 (TID 23). 1549 bytes result sent to driver
[INFO]  [2018-05-21 16:10:30] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 47.0 (TID 23) in 5 ms on localhost (1/1)
[INFO]  [2018-05-21 16:10:30] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 47.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:10:30] [org.apache.spark.scheduler.DAGScheduler]ResultStage 47 (print at HdfsSparkStreamT.java:59) finished in 0.005 s
[INFO]  [2018-05-21 16:10:30] [org.apache.spark.scheduler.DAGScheduler]Job 23 finished: print at HdfsSparkStreamT.java:59, took 0.014191 s
[INFO]  [2018-05-21 16:10:30] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526890230000 ms.0 from job set of time 1526890230000 ms
[INFO]  [2018-05-21 16:10:30] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.097 s for time 1526890230000 ms (execution: 0.063 s)
[INFO]  [2018-05-21 16:10:30] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 54 from persistence list
[INFO]  [2018-05-21 16:10:30] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 53 from persistence list
[INFO]  [2018-05-21 16:10:30] [org.apache.spark.storage.BlockManager]Removing RDD 54
[INFO]  [2018-05-21 16:10:30] [org.apache.spark.storage.BlockManager]Removing RDD 53
[INFO]  [2018-05-21 16:10:30] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 52 from persistence list
[INFO]  [2018-05-21 16:10:30] [org.apache.spark.storage.BlockManager]Removing RDD 52
[INFO]  [2018-05-21 16:10:30] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 51 from persistence list
[INFO]  [2018-05-21 16:10:30] [org.apache.spark.storage.BlockManager]Removing RDD 51
[INFO]  [2018-05-21 16:10:30] [org.apache.spark.streaming.dstream.FileInputDStream]Cleared 1 old files that were older than 1526890170000 ms: 1526890155000 ms
[INFO]  [2018-05-21 16:10:30] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526890230000 ms
[INFO]  [2018-05-21 16:10:30] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526890230000 ms
[INFO]  [2018-05-21 16:10:30] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526890230000 ms
[INFO]  [2018-05-21 16:10:30] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526890230000 ms to writer queue
[INFO]  [2018-05-21 16:10:30] [org.apache.spark.streaming.CheckpointWriter]Deleting hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890155000.bk
[INFO]  [2018-05-21 16:10:30] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526890230000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890230000', took 4434 bytes and 93 ms
[INFO]  [2018-05-21 16:10:30] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526890230000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890230000'
[INFO]  [2018-05-21 16:10:30] [org.apache.spark.streaming.CheckpointWriter]Deleting hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890155000
[INFO]  [2018-05-21 16:10:30] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526890230000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890230000', took 4432 bytes and 60 ms
[INFO]  [2018-05-21 16:10:30] [org.apache.spark.streaming.DStreamGraph]Clearing checkpoint data for time 1526890230000 ms
[INFO]  [2018-05-21 16:10:30] [org.apache.spark.streaming.DStreamGraph]Cleared checkpoint data for time 1526890230000 ms
[INFO]  [2018-05-21 16:10:30] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO]  [2018-05-21 16:10:30] [org.apache.spark.streaming.util.FileBasedWriteAheadLog_ReceivedBlockTracker]Attempting to clear 0 old log files in hdfs://hadoop1:9000/lib/sparkstreaming/check_point/receivedBlockMetadata older than 1526890170000: 
[INFO]  [2018-05-21 16:10:30] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1526890155000 ms
[INFO]  [2018-05-21 16:10:45] [org.apache.spark.streaming.dstream.FileInputDStream]Finding new files took 22 ms
[INFO]  [2018-05-21 16:10:45] [org.apache.spark.streaming.dstream.FileInputDStream]New files at time 1526890245000 ms:

[INFO]  [2018-05-21 16:10:45] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526890245000 ms
[INFO]  [2018-05-21 16:10:45] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526890245000 ms
[INFO]  [2018-05-21 16:10:45] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526890245000 ms
[INFO]  [2018-05-21 16:10:45] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526890245000 ms
[INFO]  [2018-05-21 16:10:45] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526890245000 ms to writer queue
[INFO]  [2018-05-21 16:10:45] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:10:45] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526890245000 ms.0 from job set of time 1526890245000 ms
[INFO]  [2018-05-21 16:10:45] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526890245000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890245000'
[INFO]  [2018-05-21 16:10:45] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 63 (mapToPair at HdfsSparkStreamT.java:56)
[INFO]  [2018-05-21 16:10:45] [org.apache.spark.scheduler.DAGScheduler]Got job 24 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:10:45] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 49 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:10:45] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 48)
[INFO]  [2018-05-21 16:10:45] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:10:45] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 49 (ShuffledRDD[64] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:10:45] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_24 stored as values in memory (estimated size 3.8 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:10:45] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_24_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:10:45] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_24_piece0 in memory on 172.18.42.247:50738 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:10:45] [org.apache.spark.SparkContext]Created broadcast 24 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:10:45] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 49 (ShuffledRDD[64] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:10:45] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 49.0 with 1 tasks
[INFO]  [2018-05-21 16:10:45] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 49.0 (TID 24, localhost, partition 0, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:10:45] [org.apache.spark.executor.Executor]Running task 0.0 in stage 49.0 (TID 24)
[INFO]  [2018-05-21 16:10:45] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:10:45] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 16:10:45] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 49.0 (TID 24). 1383 bytes result sent to driver
[INFO]  [2018-05-21 16:10:45] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 49.0 (TID 24) in 5 ms on localhost (1/1)
[INFO]  [2018-05-21 16:10:45] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 49.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:10:45] [org.apache.spark.scheduler.DAGScheduler]ResultStage 49 (print at HdfsSparkStreamT.java:59) finished in 0.008 s
[INFO]  [2018-05-21 16:10:45] [org.apache.spark.scheduler.DAGScheduler]Job 24 finished: print at HdfsSparkStreamT.java:59, took 0.013977 s
[INFO]  [2018-05-21 16:10:45] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:10:45] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 12 is 83 bytes
[INFO]  [2018-05-21 16:10:45] [org.apache.spark.scheduler.DAGScheduler]Got job 25 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:10:45] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 51 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:10:45] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 50)
[INFO]  [2018-05-21 16:10:45] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:10:45] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 51 (ShuffledRDD[64] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:10:45] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_25 stored as values in memory (estimated size 3.8 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:10:45] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_25_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:10:45] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_25_piece0 in memory on 172.18.42.247:50738 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:10:45] [org.apache.spark.SparkContext]Created broadcast 25 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:10:45] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 51 (ShuffledRDD[64] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:10:45] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 51.0 with 1 tasks
[INFO]  [2018-05-21 16:10:45] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 51.0 (TID 25, localhost, partition 1, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:10:45] [org.apache.spark.executor.Executor]Running task 0.0 in stage 51.0 (TID 25)
[INFO]  [2018-05-21 16:10:45] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:10:45] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 16:10:45] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 51.0 (TID 25). 1470 bytes result sent to driver
[INFO]  [2018-05-21 16:10:45] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 51.0 (TID 25) in 7 ms on localhost (1/1)
[INFO]  [2018-05-21 16:10:45] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 51.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:10:45] [org.apache.spark.scheduler.DAGScheduler]ResultStage 51 (print at HdfsSparkStreamT.java:59) finished in 0.007 s
[INFO]  [2018-05-21 16:10:45] [org.apache.spark.scheduler.DAGScheduler]Job 25 finished: print at HdfsSparkStreamT.java:59, took 0.015676 s
[INFO]  [2018-05-21 16:10:45] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526890245000 ms.0 from job set of time 1526890245000 ms
[INFO]  [2018-05-21 16:10:45] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.107 s for time 1526890245000 ms (execution: 0.035 s)
[INFO]  [2018-05-21 16:10:45] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 59 from persistence list
[INFO]  [2018-05-21 16:10:45] [org.apache.spark.storage.BlockManager]Removing RDD 59
[INFO]  [2018-05-21 16:10:45] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 58 from persistence list
[INFO]  [2018-05-21 16:10:45] [org.apache.spark.storage.BlockManager]Removing RDD 58
[INFO]  [2018-05-21 16:10:45] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 57 from persistence list
[INFO]  [2018-05-21 16:10:45] [org.apache.spark.storage.BlockManager]Removing RDD 57
[INFO]  [2018-05-21 16:10:45] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 56 from persistence list
[INFO]  [2018-05-21 16:10:45] [org.apache.spark.storage.BlockManager]Removing RDD 56
[INFO]  [2018-05-21 16:10:45] [org.apache.spark.streaming.dstream.FileInputDStream]Cleared 1 old files that were older than 1526890185000 ms: 1526890170000 ms
[INFO]  [2018-05-21 16:10:45] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526890245000 ms
[INFO]  [2018-05-21 16:10:45] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526890245000 ms
[INFO]  [2018-05-21 16:10:45] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526890245000 ms
[INFO]  [2018-05-21 16:10:45] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526890245000 ms to writer queue
[INFO]  [2018-05-21 16:10:45] [org.apache.spark.streaming.CheckpointWriter]Deleting hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890170000.bk
[INFO]  [2018-05-21 16:10:45] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526890245000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890245000', took 4431 bytes and 227 ms
[INFO]  [2018-05-21 16:10:45] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526890245000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890245000'
[INFO]  [2018-05-21 16:10:45] [org.apache.spark.streaming.CheckpointWriter]Deleting hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890170000
[INFO]  [2018-05-21 16:10:45] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526890245000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890245000', took 4430 bytes and 95 ms
[INFO]  [2018-05-21 16:10:45] [org.apache.spark.streaming.DStreamGraph]Clearing checkpoint data for time 1526890245000 ms
[INFO]  [2018-05-21 16:10:45] [org.apache.spark.streaming.DStreamGraph]Cleared checkpoint data for time 1526890245000 ms
[INFO]  [2018-05-21 16:10:45] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO]  [2018-05-21 16:10:45] [org.apache.spark.streaming.util.FileBasedWriteAheadLog_ReceivedBlockTracker]Attempting to clear 0 old log files in hdfs://hadoop1:9000/lib/sparkstreaming/check_point/receivedBlockMetadata older than 1526890185000: 
[INFO]  [2018-05-21 16:10:45] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1526890170000 ms
[INFO]  [2018-05-21 16:11:00] [org.apache.spark.streaming.dstream.FileInputDStream]Finding new files took 9 ms
[INFO]  [2018-05-21 16:11:00] [org.apache.spark.streaming.dstream.FileInputDStream]New files at time 1526890260000 ms:

[INFO]  [2018-05-21 16:11:00] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526890260000 ms
[INFO]  [2018-05-21 16:11:00] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526890260000 ms
[INFO]  [2018-05-21 16:11:00] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526890260000 ms
[INFO]  [2018-05-21 16:11:00] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526890260000 ms
[INFO]  [2018-05-21 16:11:00] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526890260000 ms.0 from job set of time 1526890260000 ms
[INFO]  [2018-05-21 16:11:00] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526890260000 ms to writer queue
[INFO]  [2018-05-21 16:11:00] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526890260000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890260000'
[INFO]  [2018-05-21 16:11:00] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:11:00] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 68 (mapToPair at HdfsSparkStreamT.java:56)
[INFO]  [2018-05-21 16:11:00] [org.apache.spark.scheduler.DAGScheduler]Got job 26 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:11:00] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 53 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:11:00] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 52)
[INFO]  [2018-05-21 16:11:00] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:11:00] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 53 (ShuffledRDD[69] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:11:00] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_26 stored as values in memory (estimated size 3.8 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:11:00] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_26_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:11:00] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_26_piece0 in memory on 172.18.42.247:50738 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:11:00] [org.apache.spark.SparkContext]Created broadcast 26 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:11:00] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 53 (ShuffledRDD[69] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:11:00] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 53.0 with 1 tasks
[INFO]  [2018-05-21 16:11:00] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 53.0 (TID 26, localhost, partition 0, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:11:00] [org.apache.spark.executor.Executor]Running task 0.0 in stage 53.0 (TID 26)
[INFO]  [2018-05-21 16:11:00] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:11:00] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 16:11:00] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 53.0 (TID 26). 1636 bytes result sent to driver
[INFO]  [2018-05-21 16:11:00] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 53.0 (TID 26) in 5 ms on localhost (1/1)
[INFO]  [2018-05-21 16:11:00] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 53.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:11:00] [org.apache.spark.scheduler.DAGScheduler]ResultStage 53 (print at HdfsSparkStreamT.java:59) finished in 0.007 s
[INFO]  [2018-05-21 16:11:00] [org.apache.spark.scheduler.DAGScheduler]Job 26 finished: print at HdfsSparkStreamT.java:59, took 0.012761 s
[INFO]  [2018-05-21 16:11:00] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:11:00] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 13 is 83 bytes
[INFO]  [2018-05-21 16:11:00] [org.apache.spark.scheduler.DAGScheduler]Got job 27 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:11:00] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 55 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:11:00] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 54)
[INFO]  [2018-05-21 16:11:00] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:11:00] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 55 (ShuffledRDD[69] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:11:00] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_27 stored as values in memory (estimated size 3.8 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:11:00] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_27_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:11:00] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_27_piece0 in memory on 172.18.42.247:50738 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:11:00] [org.apache.spark.SparkContext]Created broadcast 27 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:11:00] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 55 (ShuffledRDD[69] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:11:00] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 55.0 with 1 tasks
[INFO]  [2018-05-21 16:11:00] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 55.0 (TID 27, localhost, partition 1, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:11:00] [org.apache.spark.executor.Executor]Running task 0.0 in stage 55.0 (TID 27)
[INFO]  [2018-05-21 16:11:00] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:11:00] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 16:11:00] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 55.0 (TID 27). 1470 bytes result sent to driver
[INFO]  [2018-05-21 16:11:00] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 55.0 (TID 27) in 5 ms on localhost (1/1)
[INFO]  [2018-05-21 16:11:00] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 55.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:11:00] [org.apache.spark.scheduler.DAGScheduler]ResultStage 55 (print at HdfsSparkStreamT.java:59) finished in 0.006 s
[INFO]  [2018-05-21 16:11:00] [org.apache.spark.scheduler.DAGScheduler]Job 27 finished: print at HdfsSparkStreamT.java:59, took 0.012181 s
[INFO]  [2018-05-21 16:11:00] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526890260000 ms.0 from job set of time 1526890260000 ms
[INFO]  [2018-05-21 16:11:00] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.089 s for time 1526890260000 ms (execution: 0.035 s)
[INFO]  [2018-05-21 16:11:00] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 64 from persistence list
[INFO]  [2018-05-21 16:11:00] [org.apache.spark.storage.BlockManager]Removing RDD 64
[INFO]  [2018-05-21 16:11:00] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 63 from persistence list
[INFO]  [2018-05-21 16:11:00] [org.apache.spark.storage.BlockManager]Removing RDD 63
[INFO]  [2018-05-21 16:11:00] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 62 from persistence list
[INFO]  [2018-05-21 16:11:00] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 61 from persistence list
[INFO]  [2018-05-21 16:11:00] [org.apache.spark.storage.BlockManager]Removing RDD 62
[INFO]  [2018-05-21 16:11:00] [org.apache.spark.storage.BlockManager]Removing RDD 61
[INFO]  [2018-05-21 16:11:00] [org.apache.spark.streaming.dstream.FileInputDStream]Cleared 1 old files that were older than 1526890200000 ms: 1526890185000 ms
[INFO]  [2018-05-21 16:11:00] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526890260000 ms
[INFO]  [2018-05-21 16:11:00] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526890260000 ms
[INFO]  [2018-05-21 16:11:00] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526890260000 ms
[INFO]  [2018-05-21 16:11:00] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526890260000 ms to writer queue
[INFO]  [2018-05-21 16:11:00] [org.apache.spark.streaming.CheckpointWriter]Deleting hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890185000.bk
[INFO]  [2018-05-21 16:11:00] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526890260000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890260000', took 4434 bytes and 105 ms
[INFO]  [2018-05-21 16:11:00] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526890260000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890260000'
[INFO]  [2018-05-21 16:11:00] [org.apache.spark.streaming.CheckpointWriter]Deleting hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890185000
[INFO]  [2018-05-21 16:11:00] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526890260000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890260000', took 4433 bytes and 46 ms
[INFO]  [2018-05-21 16:11:00] [org.apache.spark.streaming.DStreamGraph]Clearing checkpoint data for time 1526890260000 ms
[INFO]  [2018-05-21 16:11:00] [org.apache.spark.streaming.DStreamGraph]Cleared checkpoint data for time 1526890260000 ms
[INFO]  [2018-05-21 16:11:00] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO]  [2018-05-21 16:11:00] [org.apache.spark.streaming.util.FileBasedWriteAheadLog_ReceivedBlockTracker]Attempting to clear 0 old log files in hdfs://hadoop1:9000/lib/sparkstreaming/check_point/receivedBlockMetadata older than 1526890200000: 
[INFO]  [2018-05-21 16:11:00] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1526890185000 ms
[INFO]  [2018-05-21 16:11:15] [org.apache.spark.streaming.dstream.FileInputDStream]Finding new files took 18 ms
[INFO]  [2018-05-21 16:11:15] [org.apache.spark.streaming.dstream.FileInputDStream]New files at time 1526890275000 ms:

[INFO]  [2018-05-21 16:11:15] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526890275000 ms
[INFO]  [2018-05-21 16:11:15] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526890275000 ms
[INFO]  [2018-05-21 16:11:15] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526890275000 ms
[INFO]  [2018-05-21 16:11:15] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526890275000 ms
[INFO]  [2018-05-21 16:11:15] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526890275000 ms to writer queue
[INFO]  [2018-05-21 16:11:15] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526890275000 ms.0 from job set of time 1526890275000 ms
[INFO]  [2018-05-21 16:11:15] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526890275000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890275000'
[INFO]  [2018-05-21 16:11:15] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:11:15] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 73 (mapToPair at HdfsSparkStreamT.java:56)
[INFO]  [2018-05-21 16:11:15] [org.apache.spark.scheduler.DAGScheduler]Got job 28 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:11:15] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 57 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:11:15] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 56)
[INFO]  [2018-05-21 16:11:15] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:11:15] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 57 (ShuffledRDD[74] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:11:15] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_28 stored as values in memory (estimated size 3.8 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:11:15] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_28_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:11:15] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_28_piece0 in memory on 172.18.42.247:50738 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:11:15] [org.apache.spark.SparkContext]Created broadcast 28 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:11:15] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 57 (ShuffledRDD[74] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:11:15] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 57.0 with 1 tasks
[INFO]  [2018-05-21 16:11:15] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 57.0 (TID 28, localhost, partition 0, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:11:15] [org.apache.spark.executor.Executor]Running task 0.0 in stage 57.0 (TID 28)
[INFO]  [2018-05-21 16:11:15] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:11:15] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 16:11:15] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 57.0 (TID 28). 1470 bytes result sent to driver
[INFO]  [2018-05-21 16:11:15] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 57.0 (TID 28) in 8 ms on localhost (1/1)
[INFO]  [2018-05-21 16:11:15] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 57.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:11:15] [org.apache.spark.scheduler.DAGScheduler]ResultStage 57 (print at HdfsSparkStreamT.java:59) finished in 0.010 s
[INFO]  [2018-05-21 16:11:15] [org.apache.spark.scheduler.DAGScheduler]Job 28 finished: print at HdfsSparkStreamT.java:59, took 0.017688 s
[INFO]  [2018-05-21 16:11:15] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:11:15] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 14 is 83 bytes
[INFO]  [2018-05-21 16:11:15] [org.apache.spark.scheduler.DAGScheduler]Got job 29 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:11:15] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 59 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:11:15] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 58)
[INFO]  [2018-05-21 16:11:15] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:11:15] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 59 (ShuffledRDD[74] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:11:15] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_29 stored as values in memory (estimated size 3.8 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:11:15] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_29_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:11:15] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_29_piece0 in memory on 172.18.42.247:50738 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:11:15] [org.apache.spark.SparkContext]Created broadcast 29 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:11:15] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 59 (ShuffledRDD[74] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:11:15] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 59.0 with 1 tasks
[INFO]  [2018-05-21 16:11:15] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 59.0 (TID 29, localhost, partition 1, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:11:15] [org.apache.spark.executor.Executor]Running task 0.0 in stage 59.0 (TID 29)
[INFO]  [2018-05-21 16:11:15] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:11:15] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 16:11:15] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 59.0 (TID 29). 1470 bytes result sent to driver
[INFO]  [2018-05-21 16:11:15] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 59.0 (TID 29) in 7 ms on localhost (1/1)
[INFO]  [2018-05-21 16:11:15] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 59.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:11:15] [org.apache.spark.scheduler.DAGScheduler]ResultStage 59 (print at HdfsSparkStreamT.java:59) finished in 0.010 s
[INFO]  [2018-05-21 16:11:15] [org.apache.spark.scheduler.DAGScheduler]Job 29 finished: print at HdfsSparkStreamT.java:59, took 0.017398 s
[INFO]  [2018-05-21 16:11:15] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526890275000 ms.0 from job set of time 1526890275000 ms
[INFO]  [2018-05-21 16:11:15] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.087 s for time 1526890275000 ms (execution: 0.050 s)
[INFO]  [2018-05-21 16:11:15] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 69 from persistence list
[INFO]  [2018-05-21 16:11:15] [org.apache.spark.storage.BlockManager]Removing RDD 69
[INFO]  [2018-05-21 16:11:15] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 68 from persistence list
[INFO]  [2018-05-21 16:11:15] [org.apache.spark.storage.BlockManager]Removing RDD 68
[INFO]  [2018-05-21 16:11:15] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 67 from persistence list
[INFO]  [2018-05-21 16:11:15] [org.apache.spark.storage.BlockManager]Removing RDD 67
[INFO]  [2018-05-21 16:11:15] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 66 from persistence list
[INFO]  [2018-05-21 16:11:15] [org.apache.spark.storage.BlockManager]Removing RDD 66
[INFO]  [2018-05-21 16:11:15] [org.apache.spark.streaming.dstream.FileInputDStream]Cleared 1 old files that were older than 1526890215000 ms: 1526890200000 ms
[INFO]  [2018-05-21 16:11:15] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526890275000 ms
[INFO]  [2018-05-21 16:11:15] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526890275000 ms
[INFO]  [2018-05-21 16:11:15] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526890275000 ms
[INFO]  [2018-05-21 16:11:15] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526890275000 ms to writer queue
[INFO]  [2018-05-21 16:11:15] [org.apache.spark.streaming.CheckpointWriter]Deleting hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890200000.bk
[INFO]  [2018-05-21 16:11:15] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526890275000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890275000', took 4435 bytes and 118 ms
[INFO]  [2018-05-21 16:11:15] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526890275000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890275000'
[INFO]  [2018-05-21 16:11:15] [org.apache.spark.streaming.CheckpointWriter]Deleting hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890200000
[INFO]  [2018-05-21 16:11:15] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526890275000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890275000', took 4432 bytes and 50 ms
[INFO]  [2018-05-21 16:11:15] [org.apache.spark.streaming.DStreamGraph]Clearing checkpoint data for time 1526890275000 ms
[INFO]  [2018-05-21 16:11:15] [org.apache.spark.streaming.DStreamGraph]Cleared checkpoint data for time 1526890275000 ms
[INFO]  [2018-05-21 16:11:15] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO]  [2018-05-21 16:11:15] [org.apache.spark.streaming.util.FileBasedWriteAheadLog_ReceivedBlockTracker]Attempting to clear 1 old log files in hdfs://hadoop1:9000/lib/sparkstreaming/check_point/receivedBlockMetadata older than 1526890215000: hdfs://hadoop1:9000/lib/sparkstreaming/check_point/receivedBlockMetadata/log-1526890140264-1526890200264
[INFO]  [2018-05-21 16:11:15] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1526890200000 ms
[INFO]  [2018-05-21 16:11:15] [org.apache.spark.streaming.util.FileBasedWriteAheadLog_ReceivedBlockTracker]Cleared log files in hdfs://hadoop1:9000/lib/sparkstreaming/check_point/receivedBlockMetadata older than 1526890215000
[INFO]  [2018-05-21 16:11:30] [org.apache.spark.streaming.dstream.FileInputDStream]Finding new files took 13 ms
[INFO]  [2018-05-21 16:11:30] [org.apache.spark.streaming.dstream.FileInputDStream]New files at time 1526890290000 ms:

[INFO]  [2018-05-21 16:11:30] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526890290000 ms
[INFO]  [2018-05-21 16:11:30] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526890290000 ms
[INFO]  [2018-05-21 16:11:30] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526890290000 ms
[INFO]  [2018-05-21 16:11:30] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526890290000 ms
[INFO]  [2018-05-21 16:11:30] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:11:30] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526890290000 ms to writer queue
[INFO]  [2018-05-21 16:11:30] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526890290000 ms.0 from job set of time 1526890290000 ms
[INFO]  [2018-05-21 16:11:30] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 78 (mapToPair at HdfsSparkStreamT.java:56)
[INFO]  [2018-05-21 16:11:30] [org.apache.spark.scheduler.DAGScheduler]Got job 30 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:11:30] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 61 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:11:30] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 60)
[INFO]  [2018-05-21 16:11:30] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:11:30] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 61 (ShuffledRDD[79] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:11:30] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_30 stored as values in memory (estimated size 3.8 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:11:30] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_30_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:11:30] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_30_piece0 in memory on 172.18.42.247:50738 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:11:30] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526890290000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890290000'
[INFO]  [2018-05-21 16:11:30] [org.apache.spark.SparkContext]Created broadcast 30 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:11:30] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 61 (ShuffledRDD[79] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:11:30] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 61.0 with 1 tasks
[INFO]  [2018-05-21 16:11:30] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 61.0 (TID 30, localhost, partition 0, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:11:30] [org.apache.spark.executor.Executor]Running task 0.0 in stage 61.0 (TID 30)
[INFO]  [2018-05-21 16:11:30] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:11:30] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 16:11:30] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 61.0 (TID 30). 1557 bytes result sent to driver
[INFO]  [2018-05-21 16:11:30] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 61.0 (TID 30) in 6 ms on localhost (1/1)
[INFO]  [2018-05-21 16:11:30] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 61.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:11:30] [org.apache.spark.scheduler.DAGScheduler]ResultStage 61 (print at HdfsSparkStreamT.java:59) finished in 0.006 s
[INFO]  [2018-05-21 16:11:30] [org.apache.spark.scheduler.DAGScheduler]Job 30 finished: print at HdfsSparkStreamT.java:59, took 0.020808 s
[INFO]  [2018-05-21 16:11:30] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:11:30] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 15 is 83 bytes
[INFO]  [2018-05-21 16:11:30] [org.apache.spark.scheduler.DAGScheduler]Got job 31 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:11:30] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 63 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:11:30] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 62)
[INFO]  [2018-05-21 16:11:30] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:11:30] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 63 (ShuffledRDD[79] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:11:30] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_31 stored as values in memory (estimated size 3.8 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:11:30] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_31_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:11:30] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_31_piece0 in memory on 172.18.42.247:50738 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:11:30] [org.apache.spark.SparkContext]Created broadcast 31 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:11:30] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 63 (ShuffledRDD[79] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:11:30] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 63.0 with 1 tasks
[INFO]  [2018-05-21 16:11:30] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 63.0 (TID 31, localhost, partition 1, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:11:30] [org.apache.spark.executor.Executor]Running task 0.0 in stage 63.0 (TID 31)
[INFO]  [2018-05-21 16:11:30] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:11:30] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO]  [2018-05-21 16:11:30] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 63.0 (TID 31). 1549 bytes result sent to driver
[INFO]  [2018-05-21 16:11:30] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 63.0 (TID 31) in 6 ms on localhost (1/1)
[INFO]  [2018-05-21 16:11:30] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 63.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:11:30] [org.apache.spark.scheduler.DAGScheduler]ResultStage 63 (print at HdfsSparkStreamT.java:59) finished in 0.009 s
[INFO]  [2018-05-21 16:11:30] [org.apache.spark.scheduler.DAGScheduler]Job 31 finished: print at HdfsSparkStreamT.java:59, took 0.070883 s
[INFO]  [2018-05-21 16:11:30] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526890290000 ms.0 from job set of time 1526890290000 ms
[INFO]  [2018-05-21 16:11:30] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.138 s for time 1526890290000 ms (execution: 0.097 s)
[INFO]  [2018-05-21 16:11:30] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 74 from persistence list
[INFO]  [2018-05-21 16:11:30] [org.apache.spark.storage.BlockManager]Removing RDD 74
[INFO]  [2018-05-21 16:11:30] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 73 from persistence list
[INFO]  [2018-05-21 16:11:30] [org.apache.spark.storage.BlockManager]Removing RDD 73
[INFO]  [2018-05-21 16:11:30] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 72 from persistence list
[INFO]  [2018-05-21 16:11:30] [org.apache.spark.storage.BlockManager]Removing RDD 72
[INFO]  [2018-05-21 16:11:30] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 71 from persistence list
[INFO]  [2018-05-21 16:11:30] [org.apache.spark.storage.BlockManager]Removing RDD 71
[INFO]  [2018-05-21 16:11:30] [org.apache.spark.streaming.dstream.FileInputDStream]Cleared 1 old files that were older than 1526890230000 ms: 1526890215000 ms
[INFO]  [2018-05-21 16:11:30] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526890290000 ms
[INFO]  [2018-05-21 16:11:30] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526890290000 ms
[INFO]  [2018-05-21 16:11:30] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526890290000 ms
[INFO]  [2018-05-21 16:11:30] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526890290000 ms to writer queue
[INFO]  [2018-05-21 16:11:30] [org.apache.spark.streaming.CheckpointWriter]Deleting hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890215000.bk
[INFO]  [2018-05-21 16:11:30] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526890290000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890290000', took 4436 bytes and 113 ms
[INFO]  [2018-05-21 16:11:30] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526890290000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890290000'
[INFO]  [2018-05-21 16:11:30] [org.apache.spark.streaming.CheckpointWriter]Deleting hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890215000
[INFO]  [2018-05-21 16:11:30] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526890290000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890290000', took 4435 bytes and 93 ms
[INFO]  [2018-05-21 16:11:30] [org.apache.spark.streaming.DStreamGraph]Clearing checkpoint data for time 1526890290000 ms
[INFO]  [2018-05-21 16:11:30] [org.apache.spark.streaming.DStreamGraph]Cleared checkpoint data for time 1526890290000 ms
[INFO]  [2018-05-21 16:11:30] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO]  [2018-05-21 16:11:30] [org.apache.spark.streaming.util.FileBasedWriteAheadLog_ReceivedBlockTracker]Attempting to clear 0 old log files in hdfs://hadoop1:9000/lib/sparkstreaming/check_point/receivedBlockMetadata older than 1526890230000: 
[INFO]  [2018-05-21 16:11:30] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1526890215000 ms
[INFO]  [2018-05-21 16:11:45] [org.apache.spark.streaming.dstream.FileInputDStream]Finding new files took 19 ms
[INFO]  [2018-05-21 16:11:45] [org.apache.spark.streaming.dstream.FileInputDStream]New files at time 1526890305000 ms:

[INFO]  [2018-05-21 16:11:45] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526890305000 ms
[INFO]  [2018-05-21 16:11:45] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526890305000 ms
[INFO]  [2018-05-21 16:11:45] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526890305000 ms
[INFO]  [2018-05-21 16:11:45] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526890305000 ms
[INFO]  [2018-05-21 16:11:45] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526890305000 ms to writer queue
[INFO]  [2018-05-21 16:11:45] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526890305000 ms.0 from job set of time 1526890305000 ms
[INFO]  [2018-05-21 16:11:45] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526890305000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890305000'
[INFO]  [2018-05-21 16:11:45] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:11:45] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 83 (mapToPair at HdfsSparkStreamT.java:56)
[INFO]  [2018-05-21 16:11:45] [org.apache.spark.scheduler.DAGScheduler]Got job 32 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:11:45] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 65 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:11:45] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 64)
[INFO]  [2018-05-21 16:11:45] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:11:45] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 65 (ShuffledRDD[84] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:11:45] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_32 stored as values in memory (estimated size 3.8 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:11:45] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_32_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:11:45] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_32_piece0 in memory on 172.18.42.247:50738 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:11:45] [org.apache.spark.SparkContext]Created broadcast 32 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:11:45] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 65 (ShuffledRDD[84] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:11:45] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 65.0 with 1 tasks
[INFO]  [2018-05-21 16:11:45] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 65.0 (TID 32, localhost, partition 0, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:11:45] [org.apache.spark.executor.Executor]Running task 0.0 in stage 65.0 (TID 32)
[INFO]  [2018-05-21 16:11:45] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:11:45] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 2 ms
[INFO]  [2018-05-21 16:11:45] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 65.0 (TID 32). 1549 bytes result sent to driver
[INFO]  [2018-05-21 16:11:45] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 65.0 (TID 32) in 5 ms on localhost (1/1)
[INFO]  [2018-05-21 16:11:45] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 65.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:11:45] [org.apache.spark.scheduler.DAGScheduler]ResultStage 65 (print at HdfsSparkStreamT.java:59) finished in 0.010 s
[INFO]  [2018-05-21 16:11:45] [org.apache.spark.scheduler.DAGScheduler]Job 32 finished: print at HdfsSparkStreamT.java:59, took 0.024700 s
[INFO]  [2018-05-21 16:11:45] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:11:45] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 16 is 83 bytes
[INFO]  [2018-05-21 16:11:45] [org.apache.spark.scheduler.DAGScheduler]Got job 33 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:11:45] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 67 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:11:45] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 66)
[INFO]  [2018-05-21 16:11:45] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:11:45] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 67 (ShuffledRDD[84] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:11:45] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_33 stored as values in memory (estimated size 3.8 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:11:45] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_33_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:11:45] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_33_piece0 in memory on 172.18.42.247:50738 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:11:45] [org.apache.spark.SparkContext]Created broadcast 33 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:11:45] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 67 (ShuffledRDD[84] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:11:45] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 67.0 with 1 tasks
[INFO]  [2018-05-21 16:11:45] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 67.0 (TID 33, localhost, partition 1, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:11:45] [org.apache.spark.executor.Executor]Running task 0.0 in stage 67.0 (TID 33)
[INFO]  [2018-05-21 16:11:45] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:11:45] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 16:11:45] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 67.0 (TID 33). 1470 bytes result sent to driver
[INFO]  [2018-05-21 16:11:45] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 67.0 (TID 33) in 7 ms on localhost (1/1)
[INFO]  [2018-05-21 16:11:45] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 67.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:11:45] [org.apache.spark.scheduler.DAGScheduler]ResultStage 67 (print at HdfsSparkStreamT.java:59) finished in 0.007 s
[INFO]  [2018-05-21 16:11:45] [org.apache.spark.scheduler.DAGScheduler]Job 33 finished: print at HdfsSparkStreamT.java:59, took 0.019312 s
[INFO]  [2018-05-21 16:11:45] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526890305000 ms.0 from job set of time 1526890305000 ms
[INFO]  [2018-05-21 16:11:45] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.100 s for time 1526890305000 ms (execution: 0.060 s)
[INFO]  [2018-05-21 16:11:45] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 79 from persistence list
[INFO]  [2018-05-21 16:11:45] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 78 from persistence list
[INFO]  [2018-05-21 16:11:45] [org.apache.spark.storage.BlockManager]Removing RDD 78
[INFO]  [2018-05-21 16:11:45] [org.apache.spark.storage.BlockManager]Removing RDD 79
[INFO]  [2018-05-21 16:11:45] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 77 from persistence list
[INFO]  [2018-05-21 16:11:45] [org.apache.spark.storage.BlockManager]Removing RDD 77
[INFO]  [2018-05-21 16:11:45] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 76 from persistence list
[INFO]  [2018-05-21 16:11:45] [org.apache.spark.streaming.dstream.FileInputDStream]Cleared 1 old files that were older than 1526890245000 ms: 1526890230000 ms
[INFO]  [2018-05-21 16:11:45] [org.apache.spark.storage.BlockManager]Removing RDD 76
[INFO]  [2018-05-21 16:11:45] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526890305000 ms
[INFO]  [2018-05-21 16:11:45] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526890305000 ms
[INFO]  [2018-05-21 16:11:45] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526890305000 ms
[INFO]  [2018-05-21 16:11:45] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526890305000 ms to writer queue
[INFO]  [2018-05-21 16:11:45] [org.apache.spark.streaming.CheckpointWriter]Deleting hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890230000.bk
[INFO]  [2018-05-21 16:11:45] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526890305000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890305000', took 4434 bytes and 87 ms
[INFO]  [2018-05-21 16:11:45] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526890305000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890305000'
[INFO]  [2018-05-21 16:11:45] [org.apache.spark.streaming.CheckpointWriter]Deleting hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890230000
[INFO]  [2018-05-21 16:11:45] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526890305000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890305000', took 4431 bytes and 38 ms
[INFO]  [2018-05-21 16:11:45] [org.apache.spark.streaming.DStreamGraph]Clearing checkpoint data for time 1526890305000 ms
[INFO]  [2018-05-21 16:11:45] [org.apache.spark.streaming.DStreamGraph]Cleared checkpoint data for time 1526890305000 ms
[INFO]  [2018-05-21 16:11:45] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO]  [2018-05-21 16:11:45] [org.apache.spark.streaming.util.FileBasedWriteAheadLog_ReceivedBlockTracker]Attempting to clear 0 old log files in hdfs://hadoop1:9000/lib/sparkstreaming/check_point/receivedBlockMetadata older than 1526890245000: 
[INFO]  [2018-05-21 16:11:45] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1526890230000 ms
[INFO]  [2018-05-21 16:12:00] [org.apache.spark.streaming.dstream.FileInputDStream]Finding new files took 17 ms
[INFO]  [2018-05-21 16:12:00] [org.apache.spark.streaming.dstream.FileInputDStream]New files at time 1526890320000 ms:

[INFO]  [2018-05-21 16:12:00] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526890320000 ms
[INFO]  [2018-05-21 16:12:00] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526890320000 ms
[INFO]  [2018-05-21 16:12:00] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526890320000 ms
[INFO]  [2018-05-21 16:12:00] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526890320000 ms
[INFO]  [2018-05-21 16:12:00] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526890320000 ms to writer queue
[INFO]  [2018-05-21 16:12:00] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526890320000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890320000'
[INFO]  [2018-05-21 16:12:00] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526890320000 ms.0 from job set of time 1526890320000 ms
[INFO]  [2018-05-21 16:12:00] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:12:00] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 88 (mapToPair at HdfsSparkStreamT.java:56)
[INFO]  [2018-05-21 16:12:00] [org.apache.spark.scheduler.DAGScheduler]Got job 34 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:12:00] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 69 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:12:00] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 68)
[INFO]  [2018-05-21 16:12:00] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:12:00] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 69 (ShuffledRDD[89] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:12:00] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_34 stored as values in memory (estimated size 3.8 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:12:00] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_34_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:12:00] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_34_piece0 in memory on 172.18.42.247:50738 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:12:00] [org.apache.spark.SparkContext]Created broadcast 34 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:12:00] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 69 (ShuffledRDD[89] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:12:00] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 69.0 with 1 tasks
[INFO]  [2018-05-21 16:12:00] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 69.0 (TID 34, localhost, partition 0, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:12:00] [org.apache.spark.executor.Executor]Running task 0.0 in stage 69.0 (TID 34)
[INFO]  [2018-05-21 16:12:00] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:12:00] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO]  [2018-05-21 16:12:00] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 69.0 (TID 34). 1549 bytes result sent to driver
[INFO]  [2018-05-21 16:12:00] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 69.0 (TID 34) in 6 ms on localhost (1/1)
[INFO]  [2018-05-21 16:12:00] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 69.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:12:00] [org.apache.spark.scheduler.DAGScheduler]ResultStage 69 (print at HdfsSparkStreamT.java:59) finished in 0.010 s
[INFO]  [2018-05-21 16:12:00] [org.apache.spark.scheduler.DAGScheduler]Job 34 finished: print at HdfsSparkStreamT.java:59, took 0.020944 s
[INFO]  [2018-05-21 16:12:00] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:12:00] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 17 is 83 bytes
[INFO]  [2018-05-21 16:12:00] [org.apache.spark.scheduler.DAGScheduler]Got job 35 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:12:00] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 71 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:12:00] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 70)
[INFO]  [2018-05-21 16:12:00] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:12:00] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 71 (ShuffledRDD[89] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:12:00] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_35 stored as values in memory (estimated size 3.8 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:12:00] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_35_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:12:00] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_35_piece0 in memory on 172.18.42.247:50738 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:12:00] [org.apache.spark.SparkContext]Created broadcast 35 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:12:00] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 71 (ShuffledRDD[89] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:12:00] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 71.0 with 1 tasks
[INFO]  [2018-05-21 16:12:00] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 71.0 (TID 35, localhost, partition 1, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:12:00] [org.apache.spark.executor.Executor]Running task 0.0 in stage 71.0 (TID 35)
[INFO]  [2018-05-21 16:12:00] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:12:00] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 16:12:00] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 71.0 (TID 35). 1549 bytes result sent to driver
[INFO]  [2018-05-21 16:12:00] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 71.0 (TID 35) in 6 ms on localhost (1/1)
[INFO]  [2018-05-21 16:12:00] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 71.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:12:00] [org.apache.spark.scheduler.DAGScheduler]ResultStage 71 (print at HdfsSparkStreamT.java:59) finished in 0.006 s
[INFO]  [2018-05-21 16:12:00] [org.apache.spark.scheduler.DAGScheduler]Job 35 finished: print at HdfsSparkStreamT.java:59, took 0.015982 s
[INFO]  [2018-05-21 16:12:00] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526890320000 ms.0 from job set of time 1526890320000 ms
[INFO]  [2018-05-21 16:12:00] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.089 s for time 1526890320000 ms (execution: 0.050 s)
[INFO]  [2018-05-21 16:12:00] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 84 from persistence list
[INFO]  [2018-05-21 16:12:00] [org.apache.spark.storage.BlockManager]Removing RDD 84
[INFO]  [2018-05-21 16:12:00] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 83 from persistence list
[INFO]  [2018-05-21 16:12:00] [org.apache.spark.storage.BlockManager]Removing RDD 83
[INFO]  [2018-05-21 16:12:00] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 82 from persistence list
[INFO]  [2018-05-21 16:12:00] [org.apache.spark.storage.BlockManager]Removing RDD 82
[INFO]  [2018-05-21 16:12:00] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 81 from persistence list
[INFO]  [2018-05-21 16:12:00] [org.apache.spark.storage.BlockManager]Removing RDD 81
[INFO]  [2018-05-21 16:12:00] [org.apache.spark.streaming.dstream.FileInputDStream]Cleared 1 old files that were older than 1526890260000 ms: 1526890245000 ms
[INFO]  [2018-05-21 16:12:00] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526890320000 ms
[INFO]  [2018-05-21 16:12:00] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526890320000 ms
[INFO]  [2018-05-21 16:12:00] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526890320000 ms
[INFO]  [2018-05-21 16:12:00] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526890320000 ms to writer queue
[INFO]  [2018-05-21 16:12:00] [org.apache.spark.streaming.CheckpointWriter]Deleting hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890245000.bk
[INFO]  [2018-05-21 16:12:00] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526890320000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890320000', took 4436 bytes and 87 ms
[INFO]  [2018-05-21 16:12:00] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526890320000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890320000'
[INFO]  [2018-05-21 16:12:00] [org.apache.spark.streaming.CheckpointWriter]Deleting hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890245000
[INFO]  [2018-05-21 16:12:00] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526890320000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890320000', took 4435 bytes and 44 ms
[INFO]  [2018-05-21 16:12:00] [org.apache.spark.streaming.DStreamGraph]Clearing checkpoint data for time 1526890320000 ms
[INFO]  [2018-05-21 16:12:00] [org.apache.spark.streaming.DStreamGraph]Cleared checkpoint data for time 1526890320000 ms
[INFO]  [2018-05-21 16:12:00] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO]  [2018-05-21 16:12:00] [org.apache.spark.streaming.util.FileBasedWriteAheadLog_ReceivedBlockTracker]Attempting to clear 0 old log files in hdfs://hadoop1:9000/lib/sparkstreaming/check_point/receivedBlockMetadata older than 1526890260000: 
[INFO]  [2018-05-21 16:12:00] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1526890245000 ms
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.streaming.dstream.FileInputDStream]Finding new files took 85 ms
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.streaming.dstream.FileInputDStream]New files at time 1526890335000 ms:

[INFO]  [2018-05-21 16:12:15] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526890335000 ms
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526890335000 ms
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526890335000 ms
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526890335000 ms.0 from job set of time 1526890335000 ms
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 93 (mapToPair at HdfsSparkStreamT.java:56)
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.scheduler.DAGScheduler]Got job 36 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 73 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 72)
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 73 (ShuffledRDD[94] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526890335000 ms
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526890335000 ms to writer queue
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526890335000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890335000'
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_36 stored as values in memory (estimated size 3.8 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_36_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_36_piece0 in memory on 172.18.42.247:50738 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.SparkContext]Created broadcast 36 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 73 (ShuffledRDD[94] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 73.0 with 1 tasks
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_27_piece0 on 172.18.42.247:50738 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 73.0 (TID 36, localhost, partition 0, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.executor.Executor]Running task 0.0 in stage 73.0 (TID 36)
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 73.0 (TID 36). 1549 bytes result sent to driver
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.ContextCleaner]Cleaned shuffle 14
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 73.0 (TID 36) in 10 ms on localhost (1/1)
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 73.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.scheduler.DAGScheduler]ResultStage 73 (print at HdfsSparkStreamT.java:59) finished in 0.010 s
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_28_piece0 on 172.18.42.247:50738 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.scheduler.DAGScheduler]Job 36 finished: print at HdfsSparkStreamT.java:59, took 0.037173 s
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 18 is 83 bytes
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.scheduler.DAGScheduler]Got job 37 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 75 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 74)
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 75 (ShuffledRDD[94] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_37 stored as values in memory (estimated size 3.8 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_37_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_37_piece0 in memory on 172.18.42.247:50738 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.SparkContext]Created broadcast 37 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 75 (ShuffledRDD[94] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 75.0 with 1 tasks
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 75.0 (TID 37, localhost, partition 1, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.executor.Executor]Running task 0.0 in stage 75.0 (TID 37)
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_29_piece0 on 172.18.42.247:50738 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.ContextCleaner]Cleaned shuffle 15
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 75.0 (TID 37). 1470 bytes result sent to driver
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_30_piece0 on 172.18.42.247:50738 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 75.0 (TID 37) in 10 ms on localhost (1/1)
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 75.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.scheduler.DAGScheduler]ResultStage 75 (print at HdfsSparkStreamT.java:59) finished in 0.015 s
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.scheduler.DAGScheduler]Job 37 finished: print at HdfsSparkStreamT.java:59, took 0.030132 s
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526890335000 ms.0 from job set of time 1526890335000 ms
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.170 s for time 1526890335000 ms (execution: 0.073 s)
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 89 from persistence list
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_31_piece0 on 172.18.42.247:50738 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 88 from persistence list
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.storage.BlockManager]Removing RDD 89
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 87 from persistence list
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.storage.BlockManager]Removing RDD 88
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.storage.BlockManager]Removing RDD 87
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 86 from persistence list
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.storage.BlockManager]Removing RDD 86
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.streaming.dstream.FileInputDStream]Cleared 1 old files that were older than 1526890275000 ms: 1526890260000 ms
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526890335000 ms
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526890335000 ms
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526890335000 ms
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.ContextCleaner]Cleaned shuffle 16
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526890335000 ms to writer queue
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_32_piece0 on 172.18.42.247:50738 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_33_piece0 on 172.18.42.247:50738 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_34_piece0 on 172.18.42.247:50738 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_35_piece0 on 172.18.42.247:50738 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.ContextCleaner]Cleaned shuffle 6
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.ContextCleaner]Cleaned shuffle 7
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_14_piece0 on 172.18.42.247:50738 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_15_piece0 on 172.18.42.247:50738 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.ContextCleaner]Cleaned shuffle 8
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_16_piece0 on 172.18.42.247:50738 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_17_piece0 on 172.18.42.247:50738 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.ContextCleaner]Cleaned shuffle 9
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_18_piece0 on 172.18.42.247:50738 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_19_piece0 on 172.18.42.247:50738 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.ContextCleaner]Cleaned shuffle 10
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_20_piece0 on 172.18.42.247:50738 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_21_piece0 on 172.18.42.247:50738 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.ContextCleaner]Cleaned shuffle 11
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_22_piece0 on 172.18.42.247:50738 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_23_piece0 on 172.18.42.247:50738 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.ContextCleaner]Cleaned shuffle 12
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_24_piece0 on 172.18.42.247:50738 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_25_piece0 on 172.18.42.247:50738 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.ContextCleaner]Cleaned shuffle 13
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_26_piece0 on 172.18.42.247:50738 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.streaming.CheckpointWriter]Deleting hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890260000.bk
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526890335000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890335000', took 4438 bytes and 168 ms
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526890335000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890335000'
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.streaming.CheckpointWriter]Deleting hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890260000
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526890335000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890335000', took 4435 bytes and 107 ms
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.streaming.DStreamGraph]Clearing checkpoint data for time 1526890335000 ms
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.streaming.DStreamGraph]Cleared checkpoint data for time 1526890335000 ms
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.streaming.util.FileBasedWriteAheadLog_ReceivedBlockTracker]Attempting to clear 1 old log files in hdfs://hadoop1:9000/lib/sparkstreaming/check_point/receivedBlockMetadata older than 1526890275000: hdfs://hadoop1:9000/lib/sparkstreaming/check_point/receivedBlockMetadata/log-1526890200669-1526890260669
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1526890260000 ms
[INFO]  [2018-05-21 16:12:15] [org.apache.spark.streaming.util.FileBasedWriteAheadLog_ReceivedBlockTracker]Cleared log files in hdfs://hadoop1:9000/lib/sparkstreaming/check_point/receivedBlockMetadata older than 1526890275000
[INFO]  [2018-05-21 16:12:30] [org.apache.spark.streaming.dstream.FileInputDStream]Finding new files took 18 ms
[INFO]  [2018-05-21 16:12:30] [org.apache.spark.streaming.dstream.FileInputDStream]New files at time 1526890350000 ms:

[INFO]  [2018-05-21 16:12:30] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526890350000 ms
[INFO]  [2018-05-21 16:12:30] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526890350000 ms
[INFO]  [2018-05-21 16:12:30] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526890350000 ms
[INFO]  [2018-05-21 16:12:30] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526890350000 ms
[INFO]  [2018-05-21 16:12:30] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526890350000 ms to writer queue
[INFO]  [2018-05-21 16:12:30] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526890350000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890350000'
[INFO]  [2018-05-21 16:12:30] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526890350000 ms.0 from job set of time 1526890350000 ms
[INFO]  [2018-05-21 16:12:30] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:12:30] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 98 (mapToPair at HdfsSparkStreamT.java:56)
[INFO]  [2018-05-21 16:12:30] [org.apache.spark.scheduler.DAGScheduler]Got job 38 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:12:30] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 77 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:12:30] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 76)
[INFO]  [2018-05-21 16:12:30] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:12:30] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 77 (ShuffledRDD[99] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:12:30] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_38 stored as values in memory (estimated size 3.8 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:12:30] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_38_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:12:30] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_38_piece0 in memory on 172.18.42.247:50738 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:12:30] [org.apache.spark.SparkContext]Created broadcast 38 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:12:30] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 77 (ShuffledRDD[99] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:12:30] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 77.0 with 1 tasks
[INFO]  [2018-05-21 16:12:30] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 77.0 (TID 38, localhost, partition 0, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:12:30] [org.apache.spark.executor.Executor]Running task 0.0 in stage 77.0 (TID 38)
[INFO]  [2018-05-21 16:12:30] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:12:30] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 16:12:30] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 77.0 (TID 38). 1470 bytes result sent to driver
[INFO]  [2018-05-21 16:12:30] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 77.0 (TID 38) in 13 ms on localhost (1/1)
[INFO]  [2018-05-21 16:12:30] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 77.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:12:30] [org.apache.spark.scheduler.DAGScheduler]ResultStage 77 (print at HdfsSparkStreamT.java:59) finished in 0.015 s
[INFO]  [2018-05-21 16:12:30] [org.apache.spark.scheduler.DAGScheduler]Job 38 finished: print at HdfsSparkStreamT.java:59, took 0.028018 s
[INFO]  [2018-05-21 16:12:30] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:12:30] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 19 is 83 bytes
[INFO]  [2018-05-21 16:12:30] [org.apache.spark.scheduler.DAGScheduler]Got job 39 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:12:30] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 79 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:12:30] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 78)
[INFO]  [2018-05-21 16:12:30] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:12:30] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 79 (ShuffledRDD[99] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:12:30] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_39 stored as values in memory (estimated size 3.8 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:12:30] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_39_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:12:30] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_39_piece0 in memory on 172.18.42.247:50738 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:12:30] [org.apache.spark.SparkContext]Created broadcast 39 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:12:30] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 79 (ShuffledRDD[99] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:12:30] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 79.0 with 1 tasks
[INFO]  [2018-05-21 16:12:30] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 79.0 (TID 39, localhost, partition 1, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:12:30] [org.apache.spark.executor.Executor]Running task 0.0 in stage 79.0 (TID 39)
[INFO]  [2018-05-21 16:12:30] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:12:30] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 2 ms
[INFO]  [2018-05-21 16:12:30] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 79.0 (TID 39). 1549 bytes result sent to driver
[INFO]  [2018-05-21 16:12:30] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 79.0 (TID 39) in 13 ms on localhost (1/1)
[INFO]  [2018-05-21 16:12:30] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 79.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:12:30] [org.apache.spark.scheduler.DAGScheduler]ResultStage 79 (print at HdfsSparkStreamT.java:59) finished in 0.015 s
[INFO]  [2018-05-21 16:12:30] [org.apache.spark.scheduler.DAGScheduler]Job 39 finished: print at HdfsSparkStreamT.java:59, took 0.021555 s
[INFO]  [2018-05-21 16:12:30] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526890350000 ms.0 from job set of time 1526890350000 ms
[INFO]  [2018-05-21 16:12:30] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.098 s for time 1526890350000 ms (execution: 0.063 s)
[INFO]  [2018-05-21 16:12:30] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 94 from persistence list
[INFO]  [2018-05-21 16:12:30] [org.apache.spark.storage.BlockManager]Removing RDD 94
[INFO]  [2018-05-21 16:12:30] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 93 from persistence list
[INFO]  [2018-05-21 16:12:30] [org.apache.spark.storage.BlockManager]Removing RDD 93
[INFO]  [2018-05-21 16:12:30] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 92 from persistence list
[INFO]  [2018-05-21 16:12:30] [org.apache.spark.storage.BlockManager]Removing RDD 92
[INFO]  [2018-05-21 16:12:30] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 91 from persistence list
[INFO]  [2018-05-21 16:12:30] [org.apache.spark.storage.BlockManager]Removing RDD 91
[INFO]  [2018-05-21 16:12:30] [org.apache.spark.streaming.dstream.FileInputDStream]Cleared 1 old files that were older than 1526890290000 ms: 1526890275000 ms
[INFO]  [2018-05-21 16:12:30] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526890350000 ms
[INFO]  [2018-05-21 16:12:30] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526890350000 ms
[INFO]  [2018-05-21 16:12:30] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526890350000 ms
[INFO]  [2018-05-21 16:12:30] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526890350000 ms to writer queue
[INFO]  [2018-05-21 16:12:30] [org.apache.spark.streaming.CheckpointWriter]Deleting hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890275000.bk
[INFO]  [2018-05-21 16:12:30] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526890350000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890350000', took 4433 bytes and 123 ms
[INFO]  [2018-05-21 16:12:30] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526890350000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890350000'
[INFO]  [2018-05-21 16:12:30] [org.apache.spark.streaming.CheckpointWriter]Deleting hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890275000
[INFO]  [2018-05-21 16:12:30] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526890350000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890350000', took 4431 bytes and 80 ms
[INFO]  [2018-05-21 16:12:30] [org.apache.spark.streaming.DStreamGraph]Clearing checkpoint data for time 1526890350000 ms
[INFO]  [2018-05-21 16:12:30] [org.apache.spark.streaming.DStreamGraph]Cleared checkpoint data for time 1526890350000 ms
[INFO]  [2018-05-21 16:12:30] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO]  [2018-05-21 16:12:30] [org.apache.spark.streaming.util.FileBasedWriteAheadLog_ReceivedBlockTracker]Attempting to clear 0 old log files in hdfs://hadoop1:9000/lib/sparkstreaming/check_point/receivedBlockMetadata older than 1526890290000: 
[INFO]  [2018-05-21 16:12:30] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1526890275000 ms
[INFO]  [2018-05-21 16:12:45] [org.apache.spark.streaming.dstream.FileInputDStream]Finding new files took 18 ms
[INFO]  [2018-05-21 16:12:45] [org.apache.spark.streaming.dstream.FileInputDStream]New files at time 1526890365000 ms:

[INFO]  [2018-05-21 16:12:45] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526890365000 ms
[INFO]  [2018-05-21 16:12:45] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526890365000 ms
[INFO]  [2018-05-21 16:12:45] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526890365000 ms
[INFO]  [2018-05-21 16:12:45] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526890365000 ms
[INFO]  [2018-05-21 16:12:45] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526890365000 ms to writer queue
[INFO]  [2018-05-21 16:12:45] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526890365000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890365000'
[INFO]  [2018-05-21 16:12:45] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:12:45] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526890365000 ms.0 from job set of time 1526890365000 ms
[INFO]  [2018-05-21 16:12:45] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 103 (mapToPair at HdfsSparkStreamT.java:56)
[INFO]  [2018-05-21 16:12:45] [org.apache.spark.scheduler.DAGScheduler]Got job 40 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:12:45] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 81 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:12:45] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 80)
[INFO]  [2018-05-21 16:12:45] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:12:45] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 81 (ShuffledRDD[104] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:12:45] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_40 stored as values in memory (estimated size 3.8 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:12:45] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_40_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:12:45] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_40_piece0 in memory on 172.18.42.247:50738 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:12:45] [org.apache.spark.SparkContext]Created broadcast 40 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:12:45] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 81 (ShuffledRDD[104] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:12:45] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 81.0 with 1 tasks
[INFO]  [2018-05-21 16:12:45] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 81.0 (TID 40, localhost, partition 0, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:12:45] [org.apache.spark.executor.Executor]Running task 0.0 in stage 81.0 (TID 40)
[INFO]  [2018-05-21 16:12:45] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:12:45] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 16:12:45] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 81.0 (TID 40). 1549 bytes result sent to driver
[INFO]  [2018-05-21 16:12:45] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 81.0 (TID 40) in 6 ms on localhost (1/1)
[INFO]  [2018-05-21 16:12:45] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 81.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:12:45] [org.apache.spark.scheduler.DAGScheduler]ResultStage 81 (print at HdfsSparkStreamT.java:59) finished in 0.008 s
[INFO]  [2018-05-21 16:12:45] [org.apache.spark.scheduler.DAGScheduler]Job 40 finished: print at HdfsSparkStreamT.java:59, took 0.021192 s
[INFO]  [2018-05-21 16:12:45] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:12:45] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 20 is 83 bytes
[INFO]  [2018-05-21 16:12:45] [org.apache.spark.scheduler.DAGScheduler]Got job 41 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:12:45] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 83 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:12:45] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 82)
[INFO]  [2018-05-21 16:12:45] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:12:45] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 83 (ShuffledRDD[104] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:12:45] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_41 stored as values in memory (estimated size 3.8 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:12:45] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_41_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:12:45] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_41_piece0 in memory on 172.18.42.247:50738 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:12:45] [org.apache.spark.SparkContext]Created broadcast 41 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:12:45] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 83 (ShuffledRDD[104] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:12:45] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 83.0 with 1 tasks
[INFO]  [2018-05-21 16:12:45] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 83.0 (TID 41, localhost, partition 1, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:12:45] [org.apache.spark.executor.Executor]Running task 0.0 in stage 83.0 (TID 41)
[INFO]  [2018-05-21 16:12:45] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:12:45] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 16:12:45] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 83.0 (TID 41). 1549 bytes result sent to driver
[INFO]  [2018-05-21 16:12:45] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 83.0 (TID 41) in 6 ms on localhost (1/1)
[INFO]  [2018-05-21 16:12:45] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 83.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:12:45] [org.apache.spark.scheduler.DAGScheduler]ResultStage 83 (print at HdfsSparkStreamT.java:59) finished in 0.007 s
[INFO]  [2018-05-21 16:12:45] [org.apache.spark.scheduler.DAGScheduler]Job 41 finished: print at HdfsSparkStreamT.java:59, took 0.014309 s
[INFO]  [2018-05-21 16:12:45] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526890365000 ms.0 from job set of time 1526890365000 ms
[INFO]  [2018-05-21 16:12:45] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.088 s for time 1526890365000 ms (execution: 0.046 s)
[INFO]  [2018-05-21 16:12:45] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 99 from persistence list
[INFO]  [2018-05-21 16:12:45] [org.apache.spark.storage.BlockManager]Removing RDD 99
[INFO]  [2018-05-21 16:12:45] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 98 from persistence list
[INFO]  [2018-05-21 16:12:45] [org.apache.spark.storage.BlockManager]Removing RDD 98
[INFO]  [2018-05-21 16:12:45] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 97 from persistence list
[INFO]  [2018-05-21 16:12:45] [org.apache.spark.storage.BlockManager]Removing RDD 97
[INFO]  [2018-05-21 16:12:45] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 96 from persistence list
[INFO]  [2018-05-21 16:12:45] [org.apache.spark.storage.BlockManager]Removing RDD 96
[INFO]  [2018-05-21 16:12:45] [org.apache.spark.streaming.dstream.FileInputDStream]Cleared 1 old files that were older than 1526890305000 ms: 1526890290000 ms
[INFO]  [2018-05-21 16:12:45] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526890365000 ms
[INFO]  [2018-05-21 16:12:45] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526890365000 ms
[INFO]  [2018-05-21 16:12:45] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526890365000 ms
[INFO]  [2018-05-21 16:12:45] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526890365000 ms to writer queue
[INFO]  [2018-05-21 16:12:45] [org.apache.spark.streaming.CheckpointWriter]Deleting hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890290000.bk
[INFO]  [2018-05-21 16:12:45] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526890365000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890365000', took 4427 bytes and 83 ms
[INFO]  [2018-05-21 16:12:45] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526890365000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890365000'
[INFO]  [2018-05-21 16:12:45] [org.apache.spark.streaming.CheckpointWriter]Deleting hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890290000
[INFO]  [2018-05-21 16:12:45] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526890365000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890365000', took 4426 bytes and 66 ms
[INFO]  [2018-05-21 16:12:45] [org.apache.spark.streaming.DStreamGraph]Clearing checkpoint data for time 1526890365000 ms
[INFO]  [2018-05-21 16:12:45] [org.apache.spark.streaming.DStreamGraph]Cleared checkpoint data for time 1526890365000 ms
[INFO]  [2018-05-21 16:12:45] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO]  [2018-05-21 16:12:45] [org.apache.spark.streaming.util.FileBasedWriteAheadLog_ReceivedBlockTracker]Attempting to clear 0 old log files in hdfs://hadoop1:9000/lib/sparkstreaming/check_point/receivedBlockMetadata older than 1526890305000: 
[INFO]  [2018-05-21 16:12:45] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1526890290000 ms
[INFO]  [2018-05-21 16:13:00] [org.apache.spark.streaming.dstream.FileInputDStream]Finding new files took 20 ms
[INFO]  [2018-05-21 16:13:00] [org.apache.spark.streaming.dstream.FileInputDStream]New files at time 1526890380000 ms:

[INFO]  [2018-05-21 16:13:00] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526890380000 ms
[INFO]  [2018-05-21 16:13:00] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526890380000 ms
[INFO]  [2018-05-21 16:13:00] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526890380000 ms
[INFO]  [2018-05-21 16:13:00] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526890380000 ms
[INFO]  [2018-05-21 16:13:00] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526890380000 ms to writer queue
[INFO]  [2018-05-21 16:13:00] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526890380000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890380000'
[INFO]  [2018-05-21 16:13:00] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526890380000 ms.0 from job set of time 1526890380000 ms
[INFO]  [2018-05-21 16:13:00] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:13:00] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 108 (mapToPair at HdfsSparkStreamT.java:56)
[INFO]  [2018-05-21 16:13:00] [org.apache.spark.scheduler.DAGScheduler]Got job 42 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:13:00] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 85 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:13:00] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 84)
[INFO]  [2018-05-21 16:13:00] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:13:00] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 85 (ShuffledRDD[109] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:13:00] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_42 stored as values in memory (estimated size 3.8 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:13:00] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_42_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:13:00] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_42_piece0 in memory on 172.18.42.247:50738 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:13:00] [org.apache.spark.SparkContext]Created broadcast 42 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:13:00] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 85 (ShuffledRDD[109] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:13:00] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 85.0 with 1 tasks
[INFO]  [2018-05-21 16:13:00] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 85.0 (TID 42, localhost, partition 0, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:13:00] [org.apache.spark.executor.Executor]Running task 0.0 in stage 85.0 (TID 42)
[INFO]  [2018-05-21 16:13:00] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:13:00] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 16:13:00] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 85.0 (TID 42). 1462 bytes result sent to driver
[INFO]  [2018-05-21 16:13:00] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 85.0 (TID 42) in 7 ms on localhost (1/1)
[INFO]  [2018-05-21 16:13:00] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 85.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:13:00] [org.apache.spark.scheduler.DAGScheduler]ResultStage 85 (print at HdfsSparkStreamT.java:59) finished in 0.007 s
[INFO]  [2018-05-21 16:13:00] [org.apache.spark.scheduler.DAGScheduler]Job 42 finished: print at HdfsSparkStreamT.java:59, took 0.026872 s
[INFO]  [2018-05-21 16:13:00] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:13:00] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 21 is 83 bytes
[INFO]  [2018-05-21 16:13:00] [org.apache.spark.scheduler.DAGScheduler]Got job 43 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:13:00] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 87 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:13:00] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 86)
[INFO]  [2018-05-21 16:13:00] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:13:00] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 87 (ShuffledRDD[109] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:13:00] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_43 stored as values in memory (estimated size 3.8 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:13:00] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_43_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:13:00] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_43_piece0 in memory on 172.18.42.247:50738 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:13:00] [org.apache.spark.SparkContext]Created broadcast 43 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:13:00] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 87 (ShuffledRDD[109] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:13:00] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 87.0 with 1 tasks
[INFO]  [2018-05-21 16:13:00] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 87.0 (TID 43, localhost, partition 1, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:13:00] [org.apache.spark.executor.Executor]Running task 0.0 in stage 87.0 (TID 43)
[INFO]  [2018-05-21 16:13:00] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:13:00] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 16:13:00] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 87.0 (TID 43). 1470 bytes result sent to driver
[INFO]  [2018-05-21 16:13:00] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 87.0 (TID 43) in 2 ms on localhost (1/1)
[INFO]  [2018-05-21 16:13:00] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 87.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:13:00] [org.apache.spark.scheduler.DAGScheduler]ResultStage 87 (print at HdfsSparkStreamT.java:59) finished in 0.005 s
[INFO]  [2018-05-21 16:13:00] [org.apache.spark.scheduler.DAGScheduler]Job 43 finished: print at HdfsSparkStreamT.java:59, took 0.009511 s
[INFO]  [2018-05-21 16:13:00] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526890380000 ms.0 from job set of time 1526890380000 ms
[INFO]  [2018-05-21 16:13:00] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.088 s for time 1526890380000 ms (execution: 0.047 s)
[INFO]  [2018-05-21 16:13:00] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 104 from persistence list
[INFO]  [2018-05-21 16:13:00] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 103 from persistence list
[INFO]  [2018-05-21 16:13:00] [org.apache.spark.storage.BlockManager]Removing RDD 104
[INFO]  [2018-05-21 16:13:00] [org.apache.spark.storage.BlockManager]Removing RDD 103
[INFO]  [2018-05-21 16:13:00] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 102 from persistence list
[INFO]  [2018-05-21 16:13:00] [org.apache.spark.storage.BlockManager]Removing RDD 102
[INFO]  [2018-05-21 16:13:00] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 101 from persistence list
[INFO]  [2018-05-21 16:13:00] [org.apache.spark.streaming.dstream.FileInputDStream]Cleared 1 old files that were older than 1526890320000 ms: 1526890305000 ms
[INFO]  [2018-05-21 16:13:00] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526890380000 ms
[INFO]  [2018-05-21 16:13:00] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526890380000 ms
[INFO]  [2018-05-21 16:13:00] [org.apache.spark.storage.BlockManager]Removing RDD 101
[INFO]  [2018-05-21 16:13:00] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526890380000 ms
[INFO]  [2018-05-21 16:13:00] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526890380000 ms to writer queue
[INFO]  [2018-05-21 16:13:00] [org.apache.spark.streaming.CheckpointWriter]Deleting hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890305000.bk
[INFO]  [2018-05-21 16:13:00] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526890380000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890380000', took 4432 bytes and 88 ms
[INFO]  [2018-05-21 16:13:00] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526890380000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890380000'
[INFO]  [2018-05-21 16:13:00] [org.apache.spark.streaming.CheckpointWriter]Deleting hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890305000
[INFO]  [2018-05-21 16:13:00] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526890380000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890380000', took 4431 bytes and 146 ms
[INFO]  [2018-05-21 16:13:00] [org.apache.spark.streaming.DStreamGraph]Clearing checkpoint data for time 1526890380000 ms
[INFO]  [2018-05-21 16:13:00] [org.apache.spark.streaming.DStreamGraph]Cleared checkpoint data for time 1526890380000 ms
[INFO]  [2018-05-21 16:13:00] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO]  [2018-05-21 16:13:00] [org.apache.spark.streaming.util.FileBasedWriteAheadLog_ReceivedBlockTracker]Attempting to clear 0 old log files in hdfs://hadoop1:9000/lib/sparkstreaming/check_point/receivedBlockMetadata older than 1526890320000: 
[INFO]  [2018-05-21 16:13:00] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1526890305000 ms
[INFO]  [2018-05-21 16:13:15] [org.apache.spark.streaming.dstream.FileInputDStream]Finding new files took 27 ms
[INFO]  [2018-05-21 16:13:15] [org.apache.spark.streaming.dstream.FileInputDStream]New files at time 1526890395000 ms:

[INFO]  [2018-05-21 16:13:15] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526890395000 ms
[INFO]  [2018-05-21 16:13:15] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526890395000 ms
[INFO]  [2018-05-21 16:13:15] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526890395000 ms
[INFO]  [2018-05-21 16:13:15] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526890395000 ms
[INFO]  [2018-05-21 16:13:15] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526890395000 ms to writer queue
[INFO]  [2018-05-21 16:13:15] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526890395000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890395000'
[INFO]  [2018-05-21 16:13:15] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526890395000 ms.0 from job set of time 1526890395000 ms
[INFO]  [2018-05-21 16:13:15] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:13:15] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 113 (mapToPair at HdfsSparkStreamT.java:56)
[INFO]  [2018-05-21 16:13:15] [org.apache.spark.scheduler.DAGScheduler]Got job 44 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:13:15] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 89 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:13:15] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 88)
[INFO]  [2018-05-21 16:13:15] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:13:15] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 89 (ShuffledRDD[114] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:13:15] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_44 stored as values in memory (estimated size 3.8 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:13:15] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_44_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:13:15] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_44_piece0 in memory on 172.18.42.247:50738 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:13:15] [org.apache.spark.SparkContext]Created broadcast 44 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:13:15] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 89 (ShuffledRDD[114] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:13:15] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 89.0 with 1 tasks
[INFO]  [2018-05-21 16:13:15] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 89.0 (TID 44, localhost, partition 0, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:13:15] [org.apache.spark.executor.Executor]Running task 0.0 in stage 89.0 (TID 44)
[INFO]  [2018-05-21 16:13:15] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:13:15] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 16:13:15] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 89.0 (TID 44). 1549 bytes result sent to driver
[INFO]  [2018-05-21 16:13:15] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 89.0 (TID 44) in 7 ms on localhost (1/1)
[INFO]  [2018-05-21 16:13:15] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 89.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:13:15] [org.apache.spark.scheduler.DAGScheduler]ResultStage 89 (print at HdfsSparkStreamT.java:59) finished in 0.009 s
[INFO]  [2018-05-21 16:13:15] [org.apache.spark.scheduler.DAGScheduler]Job 44 finished: print at HdfsSparkStreamT.java:59, took 0.017982 s
[INFO]  [2018-05-21 16:13:15] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:13:15] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 22 is 83 bytes
[INFO]  [2018-05-21 16:13:15] [org.apache.spark.scheduler.DAGScheduler]Got job 45 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:13:15] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 91 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:13:15] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 90)
[INFO]  [2018-05-21 16:13:15] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:13:15] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 91 (ShuffledRDD[114] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:13:15] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_45 stored as values in memory (estimated size 3.8 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:13:15] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_45_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:13:15] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_45_piece0 in memory on 172.18.42.247:50738 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:13:15] [org.apache.spark.SparkContext]Created broadcast 45 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:13:15] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 91 (ShuffledRDD[114] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:13:15] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 91.0 with 1 tasks
[INFO]  [2018-05-21 16:13:15] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 91.0 (TID 45, localhost, partition 1, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:13:15] [org.apache.spark.executor.Executor]Running task 0.0 in stage 91.0 (TID 45)
[INFO]  [2018-05-21 16:13:15] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:13:15] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 16:13:15] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 91.0 (TID 45). 1549 bytes result sent to driver
[INFO]  [2018-05-21 16:13:15] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 91.0 (TID 45) in 4 ms on localhost (1/1)
[INFO]  [2018-05-21 16:13:15] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 91.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:13:15] [org.apache.spark.scheduler.DAGScheduler]ResultStage 91 (print at HdfsSparkStreamT.java:59) finished in 0.007 s
[INFO]  [2018-05-21 16:13:15] [org.apache.spark.scheduler.DAGScheduler]Job 45 finished: print at HdfsSparkStreamT.java:59, took 0.012651 s
[INFO]  [2018-05-21 16:13:15] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526890395000 ms.0 from job set of time 1526890395000 ms
[INFO]  [2018-05-21 16:13:15] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 109 from persistence list
[INFO]  [2018-05-21 16:13:15] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.080 s for time 1526890395000 ms (execution: 0.037 s)
[INFO]  [2018-05-21 16:13:15] [org.apache.spark.storage.BlockManager]Removing RDD 109
[INFO]  [2018-05-21 16:13:15] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 108 from persistence list
[INFO]  [2018-05-21 16:13:15] [org.apache.spark.storage.BlockManager]Removing RDD 108
[INFO]  [2018-05-21 16:13:15] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 107 from persistence list
[INFO]  [2018-05-21 16:13:15] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 106 from persistence list
[INFO]  [2018-05-21 16:13:15] [org.apache.spark.streaming.dstream.FileInputDStream]Cleared 1 old files that were older than 1526890335000 ms: 1526890320000 ms
[INFO]  [2018-05-21 16:13:15] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526890395000 ms
[INFO]  [2018-05-21 16:13:15] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526890395000 ms
[INFO]  [2018-05-21 16:13:15] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526890395000 ms
[INFO]  [2018-05-21 16:13:15] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526890395000 ms to writer queue
[INFO]  [2018-05-21 16:13:15] [org.apache.spark.storage.BlockManager]Removing RDD 107
[INFO]  [2018-05-21 16:13:15] [org.apache.spark.storage.BlockManager]Removing RDD 106
[INFO]  [2018-05-21 16:13:15] [org.apache.spark.streaming.CheckpointWriter]Deleting hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890320000.bk
[INFO]  [2018-05-21 16:13:15] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526890395000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890395000', took 4434 bytes and 158 ms
[INFO]  [2018-05-21 16:13:15] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526890395000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890395000'
[INFO]  [2018-05-21 16:13:15] [org.apache.spark.streaming.CheckpointWriter]Deleting hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890320000
[INFO]  [2018-05-21 16:13:15] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526890395000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890395000', took 4433 bytes and 60 ms
[INFO]  [2018-05-21 16:13:15] [org.apache.spark.streaming.DStreamGraph]Clearing checkpoint data for time 1526890395000 ms
[INFO]  [2018-05-21 16:13:15] [org.apache.spark.streaming.DStreamGraph]Cleared checkpoint data for time 1526890395000 ms
[INFO]  [2018-05-21 16:13:15] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO]  [2018-05-21 16:13:15] [org.apache.spark.streaming.util.FileBasedWriteAheadLog_ReceivedBlockTracker]Attempting to clear 0 old log files in hdfs://hadoop1:9000/lib/sparkstreaming/check_point/receivedBlockMetadata older than 1526890335000: 
[INFO]  [2018-05-21 16:13:15] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1526890320000 ms
[INFO]  [2018-05-21 16:13:30] [org.apache.spark.streaming.dstream.FileInputDStream]Finding new files took 12 ms
[INFO]  [2018-05-21 16:13:30] [org.apache.spark.streaming.dstream.FileInputDStream]New files at time 1526890410000 ms:

[INFO]  [2018-05-21 16:13:30] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526890410000 ms
[INFO]  [2018-05-21 16:13:30] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526890410000 ms
[INFO]  [2018-05-21 16:13:30] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526890410000 ms
[INFO]  [2018-05-21 16:13:30] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526890410000 ms
[INFO]  [2018-05-21 16:13:30] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526890410000 ms to writer queue
[INFO]  [2018-05-21 16:13:30] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:13:30] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526890410000 ms.0 from job set of time 1526890410000 ms
[INFO]  [2018-05-21 16:13:30] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526890410000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890410000'
[INFO]  [2018-05-21 16:13:30] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 118 (mapToPair at HdfsSparkStreamT.java:56)
[INFO]  [2018-05-21 16:13:30] [org.apache.spark.scheduler.DAGScheduler]Got job 46 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:13:30] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 93 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:13:30] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 92)
[INFO]  [2018-05-21 16:13:30] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:13:30] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 93 (ShuffledRDD[119] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:13:30] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_46 stored as values in memory (estimated size 3.8 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:13:30] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_46_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:13:30] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_46_piece0 in memory on 172.18.42.247:50738 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:13:30] [org.apache.spark.SparkContext]Created broadcast 46 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:13:30] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 93 (ShuffledRDD[119] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:13:30] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 93.0 with 1 tasks
[INFO]  [2018-05-21 16:13:30] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 93.0 (TID 46, localhost, partition 0, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:13:30] [org.apache.spark.executor.Executor]Running task 0.0 in stage 93.0 (TID 46)
[INFO]  [2018-05-21 16:13:30] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:13:30] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO]  [2018-05-21 16:13:30] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 93.0 (TID 46). 1549 bytes result sent to driver
[INFO]  [2018-05-21 16:13:30] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 93.0 (TID 46) in 5 ms on localhost (1/1)
[INFO]  [2018-05-21 16:13:30] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 93.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:13:30] [org.apache.spark.scheduler.DAGScheduler]ResultStage 93 (print at HdfsSparkStreamT.java:59) finished in 0.006 s
[INFO]  [2018-05-21 16:13:30] [org.apache.spark.scheduler.DAGScheduler]Job 46 finished: print at HdfsSparkStreamT.java:59, took 0.046555 s
[INFO]  [2018-05-21 16:13:30] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:13:30] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 23 is 83 bytes
[INFO]  [2018-05-21 16:13:30] [org.apache.spark.scheduler.DAGScheduler]Got job 47 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:13:30] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 95 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:13:30] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 94)
[INFO]  [2018-05-21 16:13:30] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:13:30] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 95 (ShuffledRDD[119] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:13:30] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_47 stored as values in memory (estimated size 3.8 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:13:30] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_47_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:13:30] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_47_piece0 in memory on 172.18.42.247:50738 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:13:30] [org.apache.spark.SparkContext]Created broadcast 47 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:13:30] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 95 (ShuffledRDD[119] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:13:30] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 95.0 with 1 tasks
[INFO]  [2018-05-21 16:13:30] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 95.0 (TID 47, localhost, partition 1, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:13:30] [org.apache.spark.executor.Executor]Running task 0.0 in stage 95.0 (TID 47)
[INFO]  [2018-05-21 16:13:30] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:13:30] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 16:13:30] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 95.0 (TID 47). 1549 bytes result sent to driver
[INFO]  [2018-05-21 16:13:30] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 95.0 (TID 47) in 8 ms on localhost (1/1)
[INFO]  [2018-05-21 16:13:30] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 95.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:13:30] [org.apache.spark.scheduler.DAGScheduler]ResultStage 95 (print at HdfsSparkStreamT.java:59) finished in 0.011 s
[INFO]  [2018-05-21 16:13:30] [org.apache.spark.scheduler.DAGScheduler]Job 47 finished: print at HdfsSparkStreamT.java:59, took 0.019924 s
[INFO]  [2018-05-21 16:13:30] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526890410000 ms.0 from job set of time 1526890410000 ms
[INFO]  [2018-05-21 16:13:30] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.105 s for time 1526890410000 ms (execution: 0.072 s)
[INFO]  [2018-05-21 16:13:30] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 114 from persistence list
[INFO]  [2018-05-21 16:13:30] [org.apache.spark.storage.BlockManager]Removing RDD 114
[INFO]  [2018-05-21 16:13:30] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 113 from persistence list
[INFO]  [2018-05-21 16:13:30] [org.apache.spark.storage.BlockManager]Removing RDD 113
[INFO]  [2018-05-21 16:13:30] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 112 from persistence list
[INFO]  [2018-05-21 16:13:30] [org.apache.spark.storage.BlockManager]Removing RDD 112
[INFO]  [2018-05-21 16:13:30] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 111 from persistence list
[INFO]  [2018-05-21 16:13:30] [org.apache.spark.storage.BlockManager]Removing RDD 111
[INFO]  [2018-05-21 16:13:30] [org.apache.spark.streaming.dstream.FileInputDStream]Cleared 1 old files that were older than 1526890350000 ms: 1526890335000 ms
[INFO]  [2018-05-21 16:13:30] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526890410000 ms
[INFO]  [2018-05-21 16:13:30] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526890410000 ms
[INFO]  [2018-05-21 16:13:30] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526890410000 ms
[INFO]  [2018-05-21 16:13:30] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526890410000 ms to writer queue
[INFO]  [2018-05-21 16:13:30] [org.apache.spark.streaming.CheckpointWriter]Deleting hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890335000.bk
[INFO]  [2018-05-21 16:13:30] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526890410000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890410000', took 4436 bytes and 65 ms
[INFO]  [2018-05-21 16:13:30] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526890410000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890410000'
[INFO]  [2018-05-21 16:13:30] [org.apache.spark.streaming.CheckpointWriter]Deleting hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890335000
[INFO]  [2018-05-21 16:13:30] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526890410000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890410000', took 4435 bytes and 68 ms
[INFO]  [2018-05-21 16:13:30] [org.apache.spark.streaming.DStreamGraph]Clearing checkpoint data for time 1526890410000 ms
[INFO]  [2018-05-21 16:13:30] [org.apache.spark.streaming.DStreamGraph]Cleared checkpoint data for time 1526890410000 ms
[INFO]  [2018-05-21 16:13:30] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO]  [2018-05-21 16:13:30] [org.apache.spark.streaming.util.FileBasedWriteAheadLog_ReceivedBlockTracker]Attempting to clear 1 old log files in hdfs://hadoop1:9000/lib/sparkstreaming/check_point/receivedBlockMetadata older than 1526890350000: hdfs://hadoop1:9000/lib/sparkstreaming/check_point/receivedBlockMetadata/log-1526890275209-1526890335209
[INFO]  [2018-05-21 16:13:30] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1526890335000 ms
[INFO]  [2018-05-21 16:13:30] [org.apache.spark.streaming.util.FileBasedWriteAheadLog_ReceivedBlockTracker]Cleared log files in hdfs://hadoop1:9000/lib/sparkstreaming/check_point/receivedBlockMetadata older than 1526890350000
[INFO]  [2018-05-21 16:13:45] [org.apache.spark.streaming.dstream.FileInputDStream]Finding new files took 114 ms
[INFO]  [2018-05-21 16:13:45] [org.apache.spark.streaming.dstream.FileInputDStream]New files at time 1526890425000 ms:

[INFO]  [2018-05-21 16:13:45] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526890425000 ms
[INFO]  [2018-05-21 16:13:45] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526890425000 ms
[INFO]  [2018-05-21 16:13:45] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526890425000 ms
[INFO]  [2018-05-21 16:13:45] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526890425000 ms
[INFO]  [2018-05-21 16:13:45] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526890425000 ms.0 from job set of time 1526890425000 ms
[INFO]  [2018-05-21 16:13:45] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526890425000 ms to writer queue
[INFO]  [2018-05-21 16:13:45] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526890425000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890425000'
[INFO]  [2018-05-21 16:13:45] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:13:45] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 123 (mapToPair at HdfsSparkStreamT.java:56)
[INFO]  [2018-05-21 16:13:45] [org.apache.spark.scheduler.DAGScheduler]Got job 48 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:13:45] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 97 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:13:45] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 96)
[INFO]  [2018-05-21 16:13:45] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:13:45] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 97 (ShuffledRDD[124] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:13:45] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_48 stored as values in memory (estimated size 3.8 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:13:45] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_48_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:13:45] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_48_piece0 in memory on 172.18.42.247:50738 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:13:45] [org.apache.spark.SparkContext]Created broadcast 48 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:13:45] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 97 (ShuffledRDD[124] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:13:45] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 97.0 with 1 tasks
[INFO]  [2018-05-21 16:13:45] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 97.0 (TID 48, localhost, partition 0, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:13:45] [org.apache.spark.executor.Executor]Running task 0.0 in stage 97.0 (TID 48)
[INFO]  [2018-05-21 16:13:45] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:13:45] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 16:13:45] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 97.0 (TID 48). 1470 bytes result sent to driver
[INFO]  [2018-05-21 16:13:45] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 97.0 (TID 48) in 3 ms on localhost (1/1)
[INFO]  [2018-05-21 16:13:45] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 97.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:13:45] [org.apache.spark.scheduler.DAGScheduler]ResultStage 97 (print at HdfsSparkStreamT.java:59) finished in 0.003 s
[INFO]  [2018-05-21 16:13:45] [org.apache.spark.scheduler.DAGScheduler]Job 48 finished: print at HdfsSparkStreamT.java:59, took 0.008461 s
[INFO]  [2018-05-21 16:13:45] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:13:45] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 24 is 83 bytes
[INFO]  [2018-05-21 16:13:45] [org.apache.spark.scheduler.DAGScheduler]Got job 49 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:13:45] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 99 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:13:45] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 98)
[INFO]  [2018-05-21 16:13:45] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:13:45] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 99 (ShuffledRDD[124] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:13:45] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_49 stored as values in memory (estimated size 3.8 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:13:45] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_49_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:13:45] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_49_piece0 in memory on 172.18.42.247:50738 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:13:45] [org.apache.spark.SparkContext]Created broadcast 49 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:13:45] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 99 (ShuffledRDD[124] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:13:45] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 99.0 with 1 tasks
[INFO]  [2018-05-21 16:13:45] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 99.0 (TID 49, localhost, partition 1, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:13:45] [org.apache.spark.executor.Executor]Running task 0.0 in stage 99.0 (TID 49)
[INFO]  [2018-05-21 16:13:45] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:13:45] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 16:13:45] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 99.0 (TID 49). 1470 bytes result sent to driver
[INFO]  [2018-05-21 16:13:45] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 99.0 (TID 49) in 4 ms on localhost (1/1)
[INFO]  [2018-05-21 16:13:45] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 99.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:13:45] [org.apache.spark.scheduler.DAGScheduler]ResultStage 99 (print at HdfsSparkStreamT.java:59) finished in 0.004 s
[INFO]  [2018-05-21 16:13:45] [org.apache.spark.scheduler.DAGScheduler]Job 49 finished: print at HdfsSparkStreamT.java:59, took 0.009319 s
[INFO]  [2018-05-21 16:13:45] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526890425000 ms.0 from job set of time 1526890425000 ms
[INFO]  [2018-05-21 16:13:45] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.151 s for time 1526890425000 ms (execution: 0.028 s)
[INFO]  [2018-05-21 16:13:45] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 119 from persistence list
[INFO]  [2018-05-21 16:13:45] [org.apache.spark.storage.BlockManager]Removing RDD 119
[INFO]  [2018-05-21 16:13:45] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 118 from persistence list
[INFO]  [2018-05-21 16:13:45] [org.apache.spark.storage.BlockManager]Removing RDD 118
[INFO]  [2018-05-21 16:13:45] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 117 from persistence list
[INFO]  [2018-05-21 16:13:45] [org.apache.spark.storage.BlockManager]Removing RDD 117
[INFO]  [2018-05-21 16:13:45] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 116 from persistence list
[INFO]  [2018-05-21 16:13:45] [org.apache.spark.storage.BlockManager]Removing RDD 116
[INFO]  [2018-05-21 16:13:45] [org.apache.spark.streaming.dstream.FileInputDStream]Cleared 1 old files that were older than 1526890365000 ms: 1526890350000 ms
[INFO]  [2018-05-21 16:13:45] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526890425000 ms
[INFO]  [2018-05-21 16:13:45] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526890425000 ms
[INFO]  [2018-05-21 16:13:45] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526890425000 ms
[INFO]  [2018-05-21 16:13:45] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526890425000 ms to writer queue
[INFO]  [2018-05-21 16:13:45] [org.apache.spark.streaming.CheckpointWriter]Deleting hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890350000.bk
[INFO]  [2018-05-21 16:13:45] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526890425000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890425000', took 4436 bytes and 152 ms
[INFO]  [2018-05-21 16:13:45] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526890425000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890425000'
[INFO]  [2018-05-21 16:13:45] [org.apache.spark.streaming.CheckpointWriter]Deleting hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890350000
[INFO]  [2018-05-21 16:13:45] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526890425000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890425000', took 4433 bytes and 78 ms
[INFO]  [2018-05-21 16:13:45] [org.apache.spark.streaming.DStreamGraph]Clearing checkpoint data for time 1526890425000 ms
[INFO]  [2018-05-21 16:13:45] [org.apache.spark.streaming.DStreamGraph]Cleared checkpoint data for time 1526890425000 ms
[INFO]  [2018-05-21 16:13:45] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO]  [2018-05-21 16:13:45] [org.apache.spark.streaming.util.FileBasedWriteAheadLog_ReceivedBlockTracker]Attempting to clear 0 old log files in hdfs://hadoop1:9000/lib/sparkstreaming/check_point/receivedBlockMetadata older than 1526890365000: 
[INFO]  [2018-05-21 16:13:45] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1526890350000 ms
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.streaming.dstream.FileInputDStream]Finding new files took 16 ms
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.streaming.dstream.FileInputDStream]New files at time 1526890440000 ms:

[INFO]  [2018-05-21 16:14:00] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526890440000 ms
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526890440000 ms
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526890440000 ms
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526890440000 ms
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526890440000 ms to writer queue
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526890440000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890440000'
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 128 (mapToPair at HdfsSparkStreamT.java:56)
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.scheduler.DAGScheduler]Got job 50 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 101 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 100)
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 101 (ShuffledRDD[129] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_50 stored as values in memory (estimated size 3.8 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_50_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.ContextCleaner]Cleaned shuffle 17
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_50_piece0 in memory on 172.18.42.247:50738 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526890440000 ms.0 from job set of time 1526890440000 ms
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.ContextCleaner]Cleaned shuffle 18
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.SparkContext]Created broadcast 50 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 101 (ShuffledRDD[129] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 101.0 with 1 tasks
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_36_piece0 on 172.18.42.247:50738 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 101.0 (TID 50, localhost, partition 0, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.executor.Executor]Running task 0.0 in stage 101.0 (TID 50)
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 101.0 (TID 50). 1549 bytes result sent to driver
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_37_piece0 on 172.18.42.247:50738 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 101.0 (TID 50) in 51 ms on localhost (1/1)
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 101.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.scheduler.DAGScheduler]ResultStage 101 (print at HdfsSparkStreamT.java:59) finished in 0.051 s
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.ContextCleaner]Cleaned shuffle 19
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.scheduler.DAGScheduler]Job 50 finished: print at HdfsSparkStreamT.java:59, took 0.066720 s
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_38_piece0 on 172.18.42.247:50738 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_39_piece0 on 172.18.42.247:50738 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.ContextCleaner]Cleaned shuffle 20
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_40_piece0 on 172.18.42.247:50738 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_41_piece0 on 172.18.42.247:50738 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 25 is 83 bytes
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.scheduler.DAGScheduler]Got job 51 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 103 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 102)
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 103 (ShuffledRDD[129] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.ContextCleaner]Cleaned shuffle 21
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_51 stored as values in memory (estimated size 3.8 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_51_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_51_piece0 in memory on 172.18.42.247:50738 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.SparkContext]Created broadcast 51 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 103 (ShuffledRDD[129] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 103.0 with 1 tasks
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_42_piece0 on 172.18.42.247:50738 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 103.0 (TID 51, localhost, partition 1, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.executor.Executor]Running task 0.0 in stage 103.0 (TID 51)
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_43_piece0 on 172.18.42.247:50738 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.ContextCleaner]Cleaned shuffle 22
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_44_piece0 on 172.18.42.247:50738 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 103.0 (TID 51). 1549 bytes result sent to driver
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 103.0 (TID 51) in 6 ms on localhost (1/1)
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_45_piece0 on 172.18.42.247:50738 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 103.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.scheduler.DAGScheduler]ResultStage 103 (print at HdfsSparkStreamT.java:59) finished in 0.006 s
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.scheduler.DAGScheduler]Job 51 finished: print at HdfsSparkStreamT.java:59, took 0.017811 s
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526890440000 ms.0 from job set of time 1526890440000 ms
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.143 s for time 1526890440000 ms (execution: 0.107 s)
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.ContextCleaner]Cleaned shuffle 23
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 124 from persistence list
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.storage.BlockManager]Removing RDD 124
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 123 from persistence list
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_46_piece0 on 172.18.42.247:50738 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.storage.BlockManager]Removing RDD 123
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 122 from persistence list
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.storage.BlockManager]Removing RDD 122
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 121 from persistence list
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.storage.BlockManager]Removing RDD 121
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.streaming.dstream.FileInputDStream]Cleared 1 old files that were older than 1526890380000 ms: 1526890365000 ms
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526890440000 ms
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526890440000 ms
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526890440000 ms
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_47_piece0 on 172.18.42.247:50738 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526890440000 ms to writer queue
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_48_piece0 on 172.18.42.247:50738 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_49_piece0 on 172.18.42.247:50738 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.streaming.CheckpointWriter]Deleting hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890365000.bk
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526890440000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890440000', took 4433 bytes and 164 ms
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526890440000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890440000'
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.streaming.CheckpointWriter]Deleting hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890365000
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526890440000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890440000', took 4433 bytes and 265 ms
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.streaming.DStreamGraph]Clearing checkpoint data for time 1526890440000 ms
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.streaming.DStreamGraph]Cleared checkpoint data for time 1526890440000 ms
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.streaming.util.FileBasedWriteAheadLog_ReceivedBlockTracker]Attempting to clear 0 old log files in hdfs://hadoop1:9000/lib/sparkstreaming/check_point/receivedBlockMetadata older than 1526890380000: 
[INFO]  [2018-05-21 16:14:00] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1526890365000 ms
[INFO]  [2018-05-21 16:14:15] [org.apache.spark.streaming.dstream.FileInputDStream]Finding new files took 7 ms
[INFO]  [2018-05-21 16:14:15] [org.apache.spark.streaming.dstream.FileInputDStream]New files at time 1526890455000 ms:

[INFO]  [2018-05-21 16:14:15] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526890455000 ms
[INFO]  [2018-05-21 16:14:15] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526890455000 ms
[INFO]  [2018-05-21 16:14:15] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526890455000 ms
[INFO]  [2018-05-21 16:14:15] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526890455000 ms
[INFO]  [2018-05-21 16:14:15] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526890455000 ms to writer queue
[INFO]  [2018-05-21 16:14:15] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526890455000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890455000'
[INFO]  [2018-05-21 16:14:15] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526890455000 ms.0 from job set of time 1526890455000 ms
[INFO]  [2018-05-21 16:14:15] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:14:15] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 133 (mapToPair at HdfsSparkStreamT.java:56)
[INFO]  [2018-05-21 16:14:15] [org.apache.spark.scheduler.DAGScheduler]Got job 52 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:14:15] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 105 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:14:15] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 104)
[INFO]  [2018-05-21 16:14:15] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:14:15] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 105 (ShuffledRDD[134] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:14:15] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_52 stored as values in memory (estimated size 3.8 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:14:15] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_52_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:14:15] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_52_piece0 in memory on 172.18.42.247:50738 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:14:15] [org.apache.spark.SparkContext]Created broadcast 52 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:14:15] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 105 (ShuffledRDD[134] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:14:15] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 105.0 with 1 tasks
[INFO]  [2018-05-21 16:14:15] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 105.0 (TID 52, localhost, partition 0, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:14:15] [org.apache.spark.executor.Executor]Running task 0.0 in stage 105.0 (TID 52)
[INFO]  [2018-05-21 16:14:15] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:14:15] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 16:14:15] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 105.0 (TID 52). 1549 bytes result sent to driver
[INFO]  [2018-05-21 16:14:15] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 105.0 (TID 52) in 9 ms on localhost (1/1)
[INFO]  [2018-05-21 16:14:15] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 105.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:14:15] [org.apache.spark.scheduler.DAGScheduler]ResultStage 105 (print at HdfsSparkStreamT.java:59) finished in 0.011 s
[INFO]  [2018-05-21 16:14:15] [org.apache.spark.scheduler.DAGScheduler]Job 52 finished: print at HdfsSparkStreamT.java:59, took 0.018206 s
[INFO]  [2018-05-21 16:14:15] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:14:15] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 26 is 83 bytes
[INFO]  [2018-05-21 16:14:15] [org.apache.spark.scheduler.DAGScheduler]Got job 53 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:14:15] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 107 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:14:15] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 106)
[INFO]  [2018-05-21 16:14:15] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:14:15] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 107 (ShuffledRDD[134] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:14:15] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_53 stored as values in memory (estimated size 3.8 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:14:15] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_53_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:14:15] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_53_piece0 in memory on 172.18.42.247:50738 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:14:15] [org.apache.spark.SparkContext]Created broadcast 53 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:14:15] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 107 (ShuffledRDD[134] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:14:15] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 107.0 with 1 tasks
[INFO]  [2018-05-21 16:14:15] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 107.0 (TID 53, localhost, partition 1, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:14:15] [org.apache.spark.executor.Executor]Running task 0.0 in stage 107.0 (TID 53)
[INFO]  [2018-05-21 16:14:15] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:14:15] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 1 ms
[INFO]  [2018-05-21 16:14:15] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 107.0 (TID 53). 1549 bytes result sent to driver
[INFO]  [2018-05-21 16:14:15] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 107.0 (TID 53) in 6 ms on localhost (1/1)
[INFO]  [2018-05-21 16:14:15] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 107.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:14:15] [org.apache.spark.scheduler.DAGScheduler]ResultStage 107 (print at HdfsSparkStreamT.java:59) finished in 0.013 s
[INFO]  [2018-05-21 16:14:15] [org.apache.spark.scheduler.DAGScheduler]Job 53 finished: print at HdfsSparkStreamT.java:59, took 0.021186 s
[INFO]  [2018-05-21 16:14:15] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526890455000 ms.0 from job set of time 1526890455000 ms
[INFO]  [2018-05-21 16:14:15] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.066 s for time 1526890455000 ms (execution: 0.049 s)
[INFO]  [2018-05-21 16:14:15] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 129 from persistence list
[INFO]  [2018-05-21 16:14:15] [org.apache.spark.storage.BlockManager]Removing RDD 129
[INFO]  [2018-05-21 16:14:15] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 128 from persistence list
[INFO]  [2018-05-21 16:14:15] [org.apache.spark.storage.BlockManager]Removing RDD 128
[INFO]  [2018-05-21 16:14:15] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 127 from persistence list
[INFO]  [2018-05-21 16:14:15] [org.apache.spark.storage.BlockManager]Removing RDD 127
[INFO]  [2018-05-21 16:14:15] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 126 from persistence list
[INFO]  [2018-05-21 16:14:15] [org.apache.spark.storage.BlockManager]Removing RDD 126
[INFO]  [2018-05-21 16:14:15] [org.apache.spark.streaming.dstream.FileInputDStream]Cleared 1 old files that were older than 1526890395000 ms: 1526890380000 ms
[INFO]  [2018-05-21 16:14:15] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526890455000 ms
[INFO]  [2018-05-21 16:14:15] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526890455000 ms
[INFO]  [2018-05-21 16:14:15] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526890455000 ms
[INFO]  [2018-05-21 16:14:15] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526890455000 ms to writer queue
[INFO]  [2018-05-21 16:14:15] [org.apache.spark.streaming.CheckpointWriter]Deleting hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890380000.bk
[INFO]  [2018-05-21 16:14:15] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526890455000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890455000', took 4434 bytes and 144 ms
[INFO]  [2018-05-21 16:14:15] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526890455000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890455000'
[INFO]  [2018-05-21 16:14:15] [org.apache.spark.streaming.CheckpointWriter]Deleting hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890380000
[INFO]  [2018-05-21 16:14:15] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526890455000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890455000', took 4433 bytes and 109 ms
[INFO]  [2018-05-21 16:14:15] [org.apache.spark.streaming.DStreamGraph]Clearing checkpoint data for time 1526890455000 ms
[INFO]  [2018-05-21 16:14:15] [org.apache.spark.streaming.DStreamGraph]Cleared checkpoint data for time 1526890455000 ms
[INFO]  [2018-05-21 16:14:15] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO]  [2018-05-21 16:14:15] [org.apache.spark.streaming.util.FileBasedWriteAheadLog_ReceivedBlockTracker]Attempting to clear 0 old log files in hdfs://hadoop1:9000/lib/sparkstreaming/check_point/receivedBlockMetadata older than 1526890395000: 
[INFO]  [2018-05-21 16:14:15] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1526890380000 ms
[INFO]  [2018-05-21 16:14:30] [org.apache.spark.streaming.dstream.FileInputDStream]Finding new files took 48 ms
[INFO]  [2018-05-21 16:14:30] [org.apache.spark.streaming.dstream.FileInputDStream]New files at time 1526890470000 ms:

[INFO]  [2018-05-21 16:14:30] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526890470000 ms
[INFO]  [2018-05-21 16:14:30] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526890470000 ms
[INFO]  [2018-05-21 16:14:30] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526890470000 ms
[INFO]  [2018-05-21 16:14:30] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526890470000 ms
[INFO]  [2018-05-21 16:14:30] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526890470000 ms to writer queue
[INFO]  [2018-05-21 16:14:30] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526890470000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890470000'
[INFO]  [2018-05-21 16:14:30] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526890470000 ms.0 from job set of time 1526890470000 ms
[INFO]  [2018-05-21 16:14:30] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:14:30] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 138 (mapToPair at HdfsSparkStreamT.java:56)
[INFO]  [2018-05-21 16:14:30] [org.apache.spark.scheduler.DAGScheduler]Got job 54 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:14:30] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 109 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:14:30] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 108)
[INFO]  [2018-05-21 16:14:30] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:14:30] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 109 (ShuffledRDD[139] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:14:30] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_54 stored as values in memory (estimated size 3.8 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:14:30] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_54_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:14:30] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_54_piece0 in memory on 172.18.42.247:50738 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:14:30] [org.apache.spark.SparkContext]Created broadcast 54 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:14:30] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 109 (ShuffledRDD[139] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:14:30] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 109.0 with 1 tasks
[INFO]  [2018-05-21 16:14:30] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 109.0 (TID 54, localhost, partition 0, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:14:30] [org.apache.spark.executor.Executor]Running task 0.0 in stage 109.0 (TID 54)
[INFO]  [2018-05-21 16:14:30] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:14:30] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 16:14:30] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 109.0 (TID 54). 1549 bytes result sent to driver
[INFO]  [2018-05-21 16:14:30] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 109.0 (TID 54) in 10 ms on localhost (1/1)
[INFO]  [2018-05-21 16:14:30] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 109.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:14:30] [org.apache.spark.scheduler.DAGScheduler]ResultStage 109 (print at HdfsSparkStreamT.java:59) finished in 0.013 s
[INFO]  [2018-05-21 16:14:30] [org.apache.spark.scheduler.DAGScheduler]Job 54 finished: print at HdfsSparkStreamT.java:59, took 0.018912 s
[INFO]  [2018-05-21 16:14:30] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:14:30] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 27 is 83 bytes
[INFO]  [2018-05-21 16:14:30] [org.apache.spark.scheduler.DAGScheduler]Got job 55 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:14:30] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 111 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:14:30] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 110)
[INFO]  [2018-05-21 16:14:30] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:14:30] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 111 (ShuffledRDD[139] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:14:30] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_55 stored as values in memory (estimated size 3.8 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:14:30] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_55_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:14:30] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_55_piece0 in memory on 172.18.42.247:50738 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:14:30] [org.apache.spark.SparkContext]Created broadcast 55 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:14:30] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 111 (ShuffledRDD[139] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:14:30] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 111.0 with 1 tasks
[INFO]  [2018-05-21 16:14:30] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 111.0 (TID 55, localhost, partition 1, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:14:30] [org.apache.spark.executor.Executor]Running task 0.0 in stage 111.0 (TID 55)
[INFO]  [2018-05-21 16:14:30] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:14:30] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 16:14:30] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 111.0 (TID 55). 1549 bytes result sent to driver
[INFO]  [2018-05-21 16:14:30] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 111.0 (TID 55) in 10 ms on localhost (1/1)
[INFO]  [2018-05-21 16:14:30] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 111.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:14:30] [org.apache.spark.scheduler.DAGScheduler]ResultStage 111 (print at HdfsSparkStreamT.java:59) finished in 0.013 s
[INFO]  [2018-05-21 16:14:30] [org.apache.spark.scheduler.DAGScheduler]Job 55 finished: print at HdfsSparkStreamT.java:59, took 0.021188 s
[INFO]  [2018-05-21 16:14:30] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526890470000 ms.0 from job set of time 1526890470000 ms
[INFO]  [2018-05-21 16:14:30] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.116 s for time 1526890470000 ms (execution: 0.054 s)
[INFO]  [2018-05-21 16:14:30] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 134 from persistence list
[INFO]  [2018-05-21 16:14:30] [org.apache.spark.storage.BlockManager]Removing RDD 134
[INFO]  [2018-05-21 16:14:30] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 133 from persistence list
[INFO]  [2018-05-21 16:14:30] [org.apache.spark.storage.BlockManager]Removing RDD 133
[INFO]  [2018-05-21 16:14:30] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 132 from persistence list
[INFO]  [2018-05-21 16:14:30] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 131 from persistence list
[INFO]  [2018-05-21 16:14:30] [org.apache.spark.storage.BlockManager]Removing RDD 132
[INFO]  [2018-05-21 16:14:30] [org.apache.spark.streaming.dstream.FileInputDStream]Cleared 1 old files that were older than 1526890410000 ms: 1526890395000 ms
[INFO]  [2018-05-21 16:14:30] [org.apache.spark.storage.BlockManager]Removing RDD 131
[INFO]  [2018-05-21 16:14:30] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526890470000 ms
[INFO]  [2018-05-21 16:14:30] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526890470000 ms
[INFO]  [2018-05-21 16:14:30] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526890470000 ms
[INFO]  [2018-05-21 16:14:30] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526890470000 ms to writer queue
[INFO]  [2018-05-21 16:14:30] [org.apache.spark.streaming.CheckpointWriter]Deleting hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890395000.bk
[INFO]  [2018-05-21 16:14:30] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526890470000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890470000', took 4436 bytes and 170 ms
[INFO]  [2018-05-21 16:14:30] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526890470000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890470000'
[INFO]  [2018-05-21 16:14:30] [org.apache.spark.streaming.CheckpointWriter]Deleting hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890395000
[INFO]  [2018-05-21 16:14:30] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526890470000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890470000', took 4435 bytes and 106 ms
[INFO]  [2018-05-21 16:14:30] [org.apache.spark.streaming.DStreamGraph]Clearing checkpoint data for time 1526890470000 ms
[INFO]  [2018-05-21 16:14:30] [org.apache.spark.streaming.DStreamGraph]Cleared checkpoint data for time 1526890470000 ms
[INFO]  [2018-05-21 16:14:30] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO]  [2018-05-21 16:14:30] [org.apache.spark.streaming.util.FileBasedWriteAheadLog_ReceivedBlockTracker]Attempting to clear 1 old log files in hdfs://hadoop1:9000/lib/sparkstreaming/check_point/receivedBlockMetadata older than 1526890410000: hdfs://hadoop1:9000/lib/sparkstreaming/check_point/receivedBlockMetadata/log-1526890335377-1526890395377
[INFO]  [2018-05-21 16:14:30] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1526890395000 ms
[INFO]  [2018-05-21 16:14:30] [org.apache.spark.streaming.util.FileBasedWriteAheadLog_ReceivedBlockTracker]Cleared log files in hdfs://hadoop1:9000/lib/sparkstreaming/check_point/receivedBlockMetadata older than 1526890410000
[INFO]  [2018-05-21 16:14:45] [org.apache.spark.streaming.dstream.FileInputDStream]Finding new files took 19 ms
[INFO]  [2018-05-21 16:14:45] [org.apache.spark.streaming.dstream.FileInputDStream]New files at time 1526890485000 ms:

[INFO]  [2018-05-21 16:14:45] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526890485000 ms
[INFO]  [2018-05-21 16:14:45] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526890485000 ms
[INFO]  [2018-05-21 16:14:45] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526890485000 ms
[INFO]  [2018-05-21 16:14:45] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526890485000 ms
[INFO]  [2018-05-21 16:14:45] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526890485000 ms to writer queue
[INFO]  [2018-05-21 16:14:45] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526890485000 ms.0 from job set of time 1526890485000 ms
[INFO]  [2018-05-21 16:14:45] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526890485000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890485000'
[INFO]  [2018-05-21 16:14:45] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:14:45] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 143 (mapToPair at HdfsSparkStreamT.java:56)
[INFO]  [2018-05-21 16:14:45] [org.apache.spark.scheduler.DAGScheduler]Got job 56 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:14:45] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 113 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:14:45] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 112)
[INFO]  [2018-05-21 16:14:45] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:14:45] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 113 (ShuffledRDD[144] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:14:45] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_56 stored as values in memory (estimated size 3.8 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:14:45] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_56_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:14:45] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_56_piece0 in memory on 172.18.42.247:50738 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:14:45] [org.apache.spark.SparkContext]Created broadcast 56 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:14:45] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 113 (ShuffledRDD[144] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:14:45] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 113.0 with 1 tasks
[INFO]  [2018-05-21 16:14:45] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 113.0 (TID 56, localhost, partition 0, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:14:45] [org.apache.spark.executor.Executor]Running task 0.0 in stage 113.0 (TID 56)
[INFO]  [2018-05-21 16:14:45] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:14:45] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 16:14:45] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 113.0 (TID 56). 1462 bytes result sent to driver
[INFO]  [2018-05-21 16:14:45] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 113.0 (TID 56) in 5 ms on localhost (1/1)
[INFO]  [2018-05-21 16:14:45] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 113.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:14:45] [org.apache.spark.scheduler.DAGScheduler]ResultStage 113 (print at HdfsSparkStreamT.java:59) finished in 0.005 s
[INFO]  [2018-05-21 16:14:45] [org.apache.spark.scheduler.DAGScheduler]Job 56 finished: print at HdfsSparkStreamT.java:59, took 0.018082 s
[INFO]  [2018-05-21 16:14:45] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:14:45] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 28 is 83 bytes
[INFO]  [2018-05-21 16:14:45] [org.apache.spark.scheduler.DAGScheduler]Got job 57 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:14:45] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 115 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:14:45] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 114)
[INFO]  [2018-05-21 16:14:45] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:14:45] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 115 (ShuffledRDD[144] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:14:45] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_57 stored as values in memory (estimated size 3.8 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:14:45] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_57_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:14:45] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_57_piece0 in memory on 172.18.42.247:50738 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:14:45] [org.apache.spark.SparkContext]Created broadcast 57 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:14:45] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 115 (ShuffledRDD[144] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:14:45] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 115.0 with 1 tasks
[INFO]  [2018-05-21 16:14:45] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 115.0 (TID 57, localhost, partition 1, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:14:45] [org.apache.spark.executor.Executor]Running task 0.0 in stage 115.0 (TID 57)
[INFO]  [2018-05-21 16:14:45] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:14:45] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 16:14:45] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 115.0 (TID 57). 1470 bytes result sent to driver
[INFO]  [2018-05-21 16:14:45] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 115.0 (TID 57) in 5 ms on localhost (1/1)
[INFO]  [2018-05-21 16:14:45] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 115.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:14:45] [org.apache.spark.scheduler.DAGScheduler]ResultStage 115 (print at HdfsSparkStreamT.java:59) finished in 0.005 s
[INFO]  [2018-05-21 16:14:45] [org.apache.spark.scheduler.DAGScheduler]Job 57 finished: print at HdfsSparkStreamT.java:59, took 0.012481 s
[INFO]  [2018-05-21 16:14:45] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526890485000 ms.0 from job set of time 1526890485000 ms
[INFO]  [2018-05-21 16:14:45] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.080 s for time 1526890485000 ms (execution: 0.040 s)
[INFO]  [2018-05-21 16:14:45] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 139 from persistence list
[INFO]  [2018-05-21 16:14:45] [org.apache.spark.storage.BlockManager]Removing RDD 139
[INFO]  [2018-05-21 16:14:45] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 138 from persistence list
[INFO]  [2018-05-21 16:14:45] [org.apache.spark.storage.BlockManager]Removing RDD 138
[INFO]  [2018-05-21 16:14:45] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 137 from persistence list
[INFO]  [2018-05-21 16:14:45] [org.apache.spark.storage.BlockManager]Removing RDD 137
[INFO]  [2018-05-21 16:14:45] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 136 from persistence list
[INFO]  [2018-05-21 16:14:45] [org.apache.spark.storage.BlockManager]Removing RDD 136
[INFO]  [2018-05-21 16:14:45] [org.apache.spark.streaming.dstream.FileInputDStream]Cleared 1 old files that were older than 1526890425000 ms: 1526890410000 ms
[INFO]  [2018-05-21 16:14:45] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526890485000 ms
[INFO]  [2018-05-21 16:14:45] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526890485000 ms
[INFO]  [2018-05-21 16:14:45] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526890485000 ms
[INFO]  [2018-05-21 16:14:45] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526890485000 ms to writer queue
[INFO]  [2018-05-21 16:14:45] [org.apache.spark.streaming.CheckpointWriter]Deleting hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890410000.bk
[INFO]  [2018-05-21 16:14:45] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526890485000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890485000', took 4434 bytes and 100 ms
[INFO]  [2018-05-21 16:14:45] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526890485000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890485000'
[INFO]  [2018-05-21 16:14:45] [org.apache.spark.streaming.CheckpointWriter]Deleting hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890410000
[INFO]  [2018-05-21 16:14:45] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526890485000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890485000', took 4432 bytes and 100 ms
[INFO]  [2018-05-21 16:14:45] [org.apache.spark.streaming.DStreamGraph]Clearing checkpoint data for time 1526890485000 ms
[INFO]  [2018-05-21 16:14:45] [org.apache.spark.streaming.DStreamGraph]Cleared checkpoint data for time 1526890485000 ms
[INFO]  [2018-05-21 16:14:45] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO]  [2018-05-21 16:14:45] [org.apache.spark.streaming.util.FileBasedWriteAheadLog_ReceivedBlockTracker]Attempting to clear 0 old log files in hdfs://hadoop1:9000/lib/sparkstreaming/check_point/receivedBlockMetadata older than 1526890425000: 
[INFO]  [2018-05-21 16:14:45] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1526890410000 ms
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.streaming.dstream.FileInputDStream]Finding new files took 50 ms
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.streaming.dstream.FileInputDStream]New files at time 1526890500000 ms:

[INFO]  [2018-05-21 16:15:00] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526890500000 ms
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526890500000 ms
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526890500000 ms
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526890500000 ms
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526890500000 ms to writer queue
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526890500000 ms.0 from job set of time 1526890500000 ms
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526890500000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890500000'
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 148 (mapToPair at HdfsSparkStreamT.java:56)
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.scheduler.DAGScheduler]Got job 58 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 117 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 116)
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 117 (ShuffledRDD[149] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_58 stored as values in memory (estimated size 3.8 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_58_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_58_piece0 in memory on 172.18.42.247:50738 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.ContextCleaner]Cleaned shuffle 24
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.SparkContext]Created broadcast 58 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 117 (ShuffledRDD[149] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 117.0 with 1 tasks
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.ContextCleaner]Cleaned shuffle 25
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 117.0 (TID 58, localhost, partition 0, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.executor.Executor]Running task 0.0 in stage 117.0 (TID 58)
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_50_piece0 on 172.18.42.247:50738 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 117.0 (TID 58). 1470 bytes result sent to driver
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 117.0 (TID 58) in 15 ms on localhost (1/1)
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 117.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.scheduler.DAGScheduler]ResultStage 117 (print at HdfsSparkStreamT.java:59) finished in 0.015 s
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.scheduler.DAGScheduler]Job 58 finished: print at HdfsSparkStreamT.java:59, took 0.036072 s
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 29 is 83 bytes
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.scheduler.DAGScheduler]Got job 59 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 119 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 118)
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 119 (ShuffledRDD[149] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_59 stored as values in memory (estimated size 3.8 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_59_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.7 MB)
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_59_piece0 in memory on 172.18.42.247:50738 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.SparkContext]Created broadcast 59 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 119 (ShuffledRDD[149] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 119.0 with 1 tasks
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_51_piece0 on 172.18.42.247:50738 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 119.0 (TID 59, localhost, partition 1, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.ContextCleaner]Cleaned shuffle 26
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.executor.Executor]Running task 0.0 in stage 119.0 (TID 59)
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_52_piece0 on 172.18.42.247:50738 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 119.0 (TID 59). 1470 bytes result sent to driver
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 119.0 (TID 59) in 5 ms on localhost (1/1)
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 119.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.scheduler.DAGScheduler]ResultStage 119 (print at HdfsSparkStreamT.java:59) finished in 0.010 s
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_53_piece0 on 172.18.42.247:50738 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.scheduler.DAGScheduler]Job 59 finished: print at HdfsSparkStreamT.java:59, took 0.057882 s
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526890500000 ms.0 from job set of time 1526890500000 ms
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.160 s for time 1526890500000 ms (execution: 0.098 s)
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 144 from persistence list
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.ContextCleaner]Cleaned shuffle 27
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.storage.BlockManager]Removing RDD 144
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_54_piece0 on 172.18.42.247:50738 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 143 from persistence list
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 142 from persistence list
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.storage.BlockManager]Removing RDD 143
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.storage.BlockManager]Removing RDD 142
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_55_piece0 on 172.18.42.247:50738 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 141 from persistence list
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.storage.BlockManager]Removing RDD 141
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.streaming.dstream.FileInputDStream]Cleared 1 old files that were older than 1526890440000 ms: 1526890425000 ms
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526890500000 ms
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526890500000 ms
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526890500000 ms
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_56_piece0 on 172.18.42.247:50738 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526890500000 ms to writer queue
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.storage.BlockManagerInfo]Removed broadcast_57_piece0 on 172.18.42.247:50738 in memory (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.streaming.CheckpointWriter]Deleting hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890425000.bk
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526890500000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890500000', took 4435 bytes and 160 ms
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526890500000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890500000'
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.streaming.CheckpointWriter]Deleting hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890425000
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526890500000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890500000', took 4432 bytes and 40 ms
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.streaming.DStreamGraph]Clearing checkpoint data for time 1526890500000 ms
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.streaming.DStreamGraph]Cleared checkpoint data for time 1526890500000 ms
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.streaming.util.FileBasedWriteAheadLog_ReceivedBlockTracker]Attempting to clear 0 old log files in hdfs://hadoop1:9000/lib/sparkstreaming/check_point/receivedBlockMetadata older than 1526890440000: 
[INFO]  [2018-05-21 16:15:00] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1526890425000 ms
[INFO]  [2018-05-21 16:15:15] [org.apache.spark.streaming.dstream.FileInputDStream]Finding new files took 11 ms
[INFO]  [2018-05-21 16:15:15] [org.apache.spark.streaming.dstream.FileInputDStream]New files at time 1526890515000 ms:

[INFO]  [2018-05-21 16:15:15] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526890515000 ms
[INFO]  [2018-05-21 16:15:15] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526890515000 ms
[INFO]  [2018-05-21 16:15:15] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526890515000 ms
[INFO]  [2018-05-21 16:15:15] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526890515000 ms
[INFO]  [2018-05-21 16:15:15] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526890515000 ms to writer queue
[INFO]  [2018-05-21 16:15:15] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526890515000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890515000'
[INFO]  [2018-05-21 16:15:15] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:15:15] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526890515000 ms.0 from job set of time 1526890515000 ms
[INFO]  [2018-05-21 16:15:15] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 153 (mapToPair at HdfsSparkStreamT.java:56)
[INFO]  [2018-05-21 16:15:15] [org.apache.spark.scheduler.DAGScheduler]Got job 60 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:15:15] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 121 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:15:15] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 120)
[INFO]  [2018-05-21 16:15:15] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:15:15] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 121 (ShuffledRDD[154] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:15:15] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_60 stored as values in memory (estimated size 3.8 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:15:15] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_60_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:15:15] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_60_piece0 in memory on 172.18.42.247:50738 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:15:15] [org.apache.spark.SparkContext]Created broadcast 60 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:15:15] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 121 (ShuffledRDD[154] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:15:15] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 121.0 with 1 tasks
[INFO]  [2018-05-21 16:15:15] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 121.0 (TID 60, localhost, partition 0, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:15:15] [org.apache.spark.executor.Executor]Running task 0.0 in stage 121.0 (TID 60)
[INFO]  [2018-05-21 16:15:15] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:15:15] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 16:15:15] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 121.0 (TID 60). 1470 bytes result sent to driver
[INFO]  [2018-05-21 16:15:15] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 121.0 (TID 60) in 7 ms on localhost (1/1)
[INFO]  [2018-05-21 16:15:15] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 121.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:15:15] [org.apache.spark.scheduler.DAGScheduler]ResultStage 121 (print at HdfsSparkStreamT.java:59) finished in 0.007 s
[INFO]  [2018-05-21 16:15:15] [org.apache.spark.scheduler.DAGScheduler]Job 60 finished: print at HdfsSparkStreamT.java:59, took 0.034333 s
[INFO]  [2018-05-21 16:15:15] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:15:15] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 30 is 83 bytes
[INFO]  [2018-05-21 16:15:15] [org.apache.spark.scheduler.DAGScheduler]Got job 61 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:15:15] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 123 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:15:15] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 122)
[INFO]  [2018-05-21 16:15:15] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:15:15] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 123 (ShuffledRDD[154] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:15:15] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_61 stored as values in memory (estimated size 3.8 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:15:15] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_61_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:15:15] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_61_piece0 in memory on 172.18.42.247:50738 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:15:15] [org.apache.spark.SparkContext]Created broadcast 61 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:15:15] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 123 (ShuffledRDD[154] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:15:15] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 123.0 with 1 tasks
[INFO]  [2018-05-21 16:15:15] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 123.0 (TID 61, localhost, partition 1, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:15:15] [org.apache.spark.executor.Executor]Running task 0.0 in stage 123.0 (TID 61)
[INFO]  [2018-05-21 16:15:15] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:15:15] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 16:15:15] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 123.0 (TID 61). 1462 bytes result sent to driver
[INFO]  [2018-05-21 16:15:15] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 123.0 (TID 61) in 5 ms on localhost (1/1)
[INFO]  [2018-05-21 16:15:15] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 123.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:15:15] [org.apache.spark.scheduler.DAGScheduler]ResultStage 123 (print at HdfsSparkStreamT.java:59) finished in 0.005 s
[INFO]  [2018-05-21 16:15:15] [org.apache.spark.scheduler.DAGScheduler]Job 61 finished: print at HdfsSparkStreamT.java:59, took 0.011041 s
[INFO]  [2018-05-21 16:15:15] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526890515000 ms.0 from job set of time 1526890515000 ms
[INFO]  [2018-05-21 16:15:15] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.087 s for time 1526890515000 ms (execution: 0.050 s)
[INFO]  [2018-05-21 16:15:15] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 149 from persistence list
[INFO]  [2018-05-21 16:15:15] [org.apache.spark.storage.BlockManager]Removing RDD 149
[INFO]  [2018-05-21 16:15:15] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 148 from persistence list
[INFO]  [2018-05-21 16:15:15] [org.apache.spark.storage.BlockManager]Removing RDD 148
[INFO]  [2018-05-21 16:15:15] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 147 from persistence list
[INFO]  [2018-05-21 16:15:15] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 146 from persistence list
[INFO]  [2018-05-21 16:15:15] [org.apache.spark.storage.BlockManager]Removing RDD 147
[INFO]  [2018-05-21 16:15:15] [org.apache.spark.storage.BlockManager]Removing RDD 146
[INFO]  [2018-05-21 16:15:15] [org.apache.spark.streaming.dstream.FileInputDStream]Cleared 1 old files that were older than 1526890455000 ms: 1526890440000 ms
[INFO]  [2018-05-21 16:15:15] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526890515000 ms
[INFO]  [2018-05-21 16:15:15] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526890515000 ms
[INFO]  [2018-05-21 16:15:15] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526890515000 ms
[INFO]  [2018-05-21 16:15:15] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526890515000 ms to writer queue
[INFO]  [2018-05-21 16:15:15] [org.apache.spark.streaming.CheckpointWriter]Deleting hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890440000.bk
[INFO]  [2018-05-21 16:15:15] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526890515000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890515000', took 4436 bytes and 90 ms
[INFO]  [2018-05-21 16:15:15] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526890515000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890515000'
[INFO]  [2018-05-21 16:15:15] [org.apache.spark.streaming.CheckpointWriter]Deleting hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890440000
[INFO]  [2018-05-21 16:15:15] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526890515000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890515000', took 4433 bytes and 77 ms
[INFO]  [2018-05-21 16:15:15] [org.apache.spark.streaming.DStreamGraph]Clearing checkpoint data for time 1526890515000 ms
[INFO]  [2018-05-21 16:15:15] [org.apache.spark.streaming.DStreamGraph]Cleared checkpoint data for time 1526890515000 ms
[INFO]  [2018-05-21 16:15:15] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO]  [2018-05-21 16:15:15] [org.apache.spark.streaming.util.FileBasedWriteAheadLog_ReceivedBlockTracker]Attempting to clear 0 old log files in hdfs://hadoop1:9000/lib/sparkstreaming/check_point/receivedBlockMetadata older than 1526890455000: 
[INFO]  [2018-05-21 16:15:15] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1526890440000 ms
[INFO]  [2018-05-21 16:15:30] [org.apache.spark.streaming.dstream.FileInputDStream]Finding new files took 26 ms
[INFO]  [2018-05-21 16:15:30] [org.apache.spark.streaming.dstream.FileInputDStream]New files at time 1526890530000 ms:

[INFO]  [2018-05-21 16:15:30] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526890530000 ms
[INFO]  [2018-05-21 16:15:30] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526890530000 ms
[INFO]  [2018-05-21 16:15:30] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526890530000 ms
[INFO]  [2018-05-21 16:15:30] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526890530000 ms
[INFO]  [2018-05-21 16:15:30] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526890530000 ms to writer queue
[INFO]  [2018-05-21 16:15:30] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526890530000 ms.0 from job set of time 1526890530000 ms
[INFO]  [2018-05-21 16:15:30] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526890530000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890530000'
[INFO]  [2018-05-21 16:15:30] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:15:30] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 158 (mapToPair at HdfsSparkStreamT.java:56)
[INFO]  [2018-05-21 16:15:30] [org.apache.spark.scheduler.DAGScheduler]Got job 62 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:15:30] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 125 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:15:30] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 124)
[INFO]  [2018-05-21 16:15:30] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:15:30] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 125 (ShuffledRDD[159] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:15:30] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_62 stored as values in memory (estimated size 3.8 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:15:30] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_62_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:15:30] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_62_piece0 in memory on 172.18.42.247:50738 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:15:30] [org.apache.spark.SparkContext]Created broadcast 62 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:15:30] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 125 (ShuffledRDD[159] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:15:30] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 125.0 with 1 tasks
[INFO]  [2018-05-21 16:15:30] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 125.0 (TID 62, localhost, partition 0, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:15:30] [org.apache.spark.executor.Executor]Running task 0.0 in stage 125.0 (TID 62)
[INFO]  [2018-05-21 16:15:30] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:15:30] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 16:15:30] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 125.0 (TID 62). 1549 bytes result sent to driver
[INFO]  [2018-05-21 16:15:30] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 125.0 (TID 62) in 5 ms on localhost (1/1)
[INFO]  [2018-05-21 16:15:30] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 125.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:15:30] [org.apache.spark.scheduler.DAGScheduler]ResultStage 125 (print at HdfsSparkStreamT.java:59) finished in 0.005 s
[INFO]  [2018-05-21 16:15:30] [org.apache.spark.scheduler.DAGScheduler]Job 62 finished: print at HdfsSparkStreamT.java:59, took 0.012228 s
[INFO]  [2018-05-21 16:15:30] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:15:30] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 31 is 83 bytes
[INFO]  [2018-05-21 16:15:30] [org.apache.spark.scheduler.DAGScheduler]Got job 63 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:15:30] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 127 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:15:30] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 126)
[INFO]  [2018-05-21 16:15:30] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:15:30] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 127 (ShuffledRDD[159] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:15:30] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_63 stored as values in memory (estimated size 3.8 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:15:30] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_63_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:15:30] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_63_piece0 in memory on 172.18.42.247:50738 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:15:30] [org.apache.spark.SparkContext]Created broadcast 63 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:15:30] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 127 (ShuffledRDD[159] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:15:30] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 127.0 with 1 tasks
[INFO]  [2018-05-21 16:15:30] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 127.0 (TID 63, localhost, partition 1, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:15:30] [org.apache.spark.executor.Executor]Running task 0.0 in stage 127.0 (TID 63)
[INFO]  [2018-05-21 16:15:30] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:15:30] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 16:15:30] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 127.0 (TID 63). 1549 bytes result sent to driver
[INFO]  [2018-05-21 16:15:30] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 127.0 (TID 63) in 4 ms on localhost (1/1)
[INFO]  [2018-05-21 16:15:30] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 127.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:15:30] [org.apache.spark.scheduler.DAGScheduler]ResultStage 127 (print at HdfsSparkStreamT.java:59) finished in 0.006 s
[INFO]  [2018-05-21 16:15:30] [org.apache.spark.scheduler.DAGScheduler]Job 63 finished: print at HdfsSparkStreamT.java:59, took 0.011596 s
[INFO]  [2018-05-21 16:15:30] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526890530000 ms.0 from job set of time 1526890530000 ms
[INFO]  [2018-05-21 16:15:30] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.076 s for time 1526890530000 ms (execution: 0.036 s)
[INFO]  [2018-05-21 16:15:30] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 154 from persistence list
[INFO]  [2018-05-21 16:15:30] [org.apache.spark.storage.BlockManager]Removing RDD 154
[INFO]  [2018-05-21 16:15:30] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 153 from persistence list
[INFO]  [2018-05-21 16:15:30] [org.apache.spark.storage.BlockManager]Removing RDD 153
[INFO]  [2018-05-21 16:15:30] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 152 from persistence list
[INFO]  [2018-05-21 16:15:30] [org.apache.spark.storage.BlockManager]Removing RDD 152
[INFO]  [2018-05-21 16:15:30] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 151 from persistence list
[INFO]  [2018-05-21 16:15:30] [org.apache.spark.storage.BlockManager]Removing RDD 151
[INFO]  [2018-05-21 16:15:30] [org.apache.spark.streaming.dstream.FileInputDStream]Cleared 1 old files that were older than 1526890470000 ms: 1526890455000 ms
[INFO]  [2018-05-21 16:15:30] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526890530000 ms
[INFO]  [2018-05-21 16:15:30] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526890530000 ms
[INFO]  [2018-05-21 16:15:30] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526890530000 ms
[INFO]  [2018-05-21 16:15:30] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526890530000 ms to writer queue
[INFO]  [2018-05-21 16:15:30] [org.apache.spark.streaming.CheckpointWriter]Deleting hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890455000.bk
[INFO]  [2018-05-21 16:15:30] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526890530000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890530000', took 4437 bytes and 87 ms
[INFO]  [2018-05-21 16:15:30] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526890530000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890530000'
[INFO]  [2018-05-21 16:15:30] [org.apache.spark.streaming.CheckpointWriter]Deleting hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890455000
[INFO]  [2018-05-21 16:15:30] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526890530000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890530000', took 4434 bytes and 54 ms
[INFO]  [2018-05-21 16:15:30] [org.apache.spark.streaming.DStreamGraph]Clearing checkpoint data for time 1526890530000 ms
[INFO]  [2018-05-21 16:15:30] [org.apache.spark.streaming.DStreamGraph]Cleared checkpoint data for time 1526890530000 ms
[INFO]  [2018-05-21 16:15:30] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO]  [2018-05-21 16:15:30] [org.apache.spark.streaming.util.FileBasedWriteAheadLog_ReceivedBlockTracker]Attempting to clear 0 old log files in hdfs://hadoop1:9000/lib/sparkstreaming/check_point/receivedBlockMetadata older than 1526890470000: 
[INFO]  [2018-05-21 16:15:30] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1526890455000 ms
[INFO]  [2018-05-21 16:15:45] [org.apache.spark.streaming.dstream.FileInputDStream]Finding new files took 16 ms
[INFO]  [2018-05-21 16:15:45] [org.apache.spark.streaming.dstream.FileInputDStream]New files at time 1526890545000 ms:

[INFO]  [2018-05-21 16:15:45] [org.apache.spark.streaming.scheduler.JobScheduler]Added jobs for time 1526890545000 ms
[INFO]  [2018-05-21 16:15:45] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526890545000 ms
[INFO]  [2018-05-21 16:15:45] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526890545000 ms
[INFO]  [2018-05-21 16:15:45] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526890545000 ms
[INFO]  [2018-05-21 16:15:45] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526890545000 ms to writer queue
[INFO]  [2018-05-21 16:15:45] [org.apache.spark.streaming.scheduler.JobScheduler]Starting job streaming job 1526890545000 ms.0 from job set of time 1526890545000 ms
[INFO]  [2018-05-21 16:15:45] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526890545000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890545000'
[INFO]  [2018-05-21 16:15:45] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:15:45] [org.apache.spark.scheduler.DAGScheduler]Registering RDD 163 (mapToPair at HdfsSparkStreamT.java:56)
[INFO]  [2018-05-21 16:15:45] [org.apache.spark.scheduler.DAGScheduler]Got job 64 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:15:45] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 129 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:15:45] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 128)
[INFO]  [2018-05-21 16:15:45] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:15:45] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 129 (ShuffledRDD[164] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:15:45] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_64 stored as values in memory (estimated size 3.8 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:15:45] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_64_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:15:45] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_64_piece0 in memory on 172.18.42.247:50738 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:15:45] [org.apache.spark.SparkContext]Created broadcast 64 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:15:45] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 129 (ShuffledRDD[164] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:15:45] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 129.0 with 1 tasks
[INFO]  [2018-05-21 16:15:45] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 129.0 (TID 64, localhost, partition 0, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:15:45] [org.apache.spark.executor.Executor]Running task 0.0 in stage 129.0 (TID 64)
[INFO]  [2018-05-21 16:15:45] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:15:45] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 16:15:45] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 129.0 (TID 64). 1470 bytes result sent to driver
[INFO]  [2018-05-21 16:15:45] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 129.0 (TID 64) in 8 ms on localhost (1/1)
[INFO]  [2018-05-21 16:15:45] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 129.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:15:45] [org.apache.spark.scheduler.DAGScheduler]ResultStage 129 (print at HdfsSparkStreamT.java:59) finished in 0.009 s
[INFO]  [2018-05-21 16:15:45] [org.apache.spark.scheduler.DAGScheduler]Job 64 finished: print at HdfsSparkStreamT.java:59, took 0.018832 s
[INFO]  [2018-05-21 16:15:45] [org.apache.spark.SparkContext]Starting job: print at HdfsSparkStreamT.java:59
[INFO]  [2018-05-21 16:15:45] [org.apache.spark.MapOutputTrackerMaster]Size of output statuses for shuffle 32 is 83 bytes
[INFO]  [2018-05-21 16:15:45] [org.apache.spark.scheduler.DAGScheduler]Got job 65 (print at HdfsSparkStreamT.java:59) with 1 output partitions
[INFO]  [2018-05-21 16:15:45] [org.apache.spark.scheduler.DAGScheduler]Final stage: ResultStage 131 (print at HdfsSparkStreamT.java:59)
[INFO]  [2018-05-21 16:15:45] [org.apache.spark.scheduler.DAGScheduler]Parents of final stage: List(ShuffleMapStage 130)
[INFO]  [2018-05-21 16:15:45] [org.apache.spark.scheduler.DAGScheduler]Missing parents: List()
[INFO]  [2018-05-21 16:15:45] [org.apache.spark.scheduler.DAGScheduler]Submitting ResultStage 131 (ShuffledRDD[164] at reduceByKey at HdfsSparkStreamT.java:57), which has no missing parents
[INFO]  [2018-05-21 16:15:45] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_65 stored as values in memory (estimated size 3.8 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:15:45] [org.apache.spark.storage.memory.MemoryStore]Block broadcast_65_piece0 stored as bytes in memory (estimated size 2.2 KB, free 901.8 MB)
[INFO]  [2018-05-21 16:15:45] [org.apache.spark.storage.BlockManagerInfo]Added broadcast_65_piece0 in memory on 172.18.42.247:50738 (size: 2.2 KB, free: 901.8 MB)
[INFO]  [2018-05-21 16:15:45] [org.apache.spark.SparkContext]Created broadcast 65 from broadcast at DAGScheduler.scala:1012
[INFO]  [2018-05-21 16:15:45] [org.apache.spark.scheduler.DAGScheduler]Submitting 1 missing tasks from ResultStage 131 (ShuffledRDD[164] at reduceByKey at HdfsSparkStreamT.java:57)
[INFO]  [2018-05-21 16:15:45] [org.apache.spark.scheduler.TaskSchedulerImpl]Adding task set 131.0 with 1 tasks
[INFO]  [2018-05-21 16:15:45] [org.apache.spark.scheduler.TaskSetManager]Starting task 0.0 in stage 131.0 (TID 65, localhost, partition 1, PROCESS_LOCAL, 5787 bytes)
[INFO]  [2018-05-21 16:15:45] [org.apache.spark.executor.Executor]Running task 0.0 in stage 131.0 (TID 65)
[INFO]  [2018-05-21 16:15:45] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Getting 0 non-empty blocks out of 0 blocks
[INFO]  [2018-05-21 16:15:45] [org.apache.spark.storage.ShuffleBlockFetcherIterator]Started 0 remote fetches in 0 ms
[INFO]  [2018-05-21 16:15:45] [org.apache.spark.executor.Executor]Finished task 0.0 in stage 131.0 (TID 65). 1549 bytes result sent to driver
[INFO]  [2018-05-21 16:15:45] [org.apache.spark.scheduler.TaskSetManager]Finished task 0.0 in stage 131.0 (TID 65) in 7 ms on localhost (1/1)
[INFO]  [2018-05-21 16:15:45] [org.apache.spark.scheduler.TaskSchedulerImpl]Removed TaskSet 131.0, whose tasks have all completed, from pool 
[INFO]  [2018-05-21 16:15:45] [org.apache.spark.scheduler.DAGScheduler]ResultStage 131 (print at HdfsSparkStreamT.java:59) finished in 0.011 s
[INFO]  [2018-05-21 16:15:45] [org.apache.spark.scheduler.DAGScheduler]Job 65 finished: print at HdfsSparkStreamT.java:59, took 0.017082 s
[INFO]  [2018-05-21 16:15:45] [org.apache.spark.streaming.scheduler.JobScheduler]Finished job streaming job 1526890545000 ms.0 from job set of time 1526890545000 ms
[INFO]  [2018-05-21 16:15:45] [org.apache.spark.streaming.scheduler.JobScheduler]Total delay: 0.084 s for time 1526890545000 ms (execution: 0.048 s)
[INFO]  [2018-05-21 16:15:45] [org.apache.spark.rdd.ShuffledRDD]Removing RDD 159 from persistence list
[INFO]  [2018-05-21 16:15:45] [org.apache.spark.storage.BlockManager]Removing RDD 159
[INFO]  [2018-05-21 16:15:45] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 158 from persistence list
[INFO]  [2018-05-21 16:15:45] [org.apache.spark.storage.BlockManager]Removing RDD 158
[INFO]  [2018-05-21 16:15:45] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 157 from persistence list
[INFO]  [2018-05-21 16:15:45] [org.apache.spark.storage.BlockManager]Removing RDD 157
[INFO]  [2018-05-21 16:15:45] [org.apache.spark.rdd.MapPartitionsRDD]Removing RDD 156 from persistence list
[INFO]  [2018-05-21 16:15:45] [org.apache.spark.streaming.dstream.FileInputDStream]Cleared 1 old files that were older than 1526890485000 ms: 1526890470000 ms
[INFO]  [2018-05-21 16:15:45] [org.apache.spark.streaming.scheduler.JobGenerator]Checkpointing graph for time 1526890545000 ms
[INFO]  [2018-05-21 16:15:45] [org.apache.spark.streaming.DStreamGraph]Updating checkpoint data for time 1526890545000 ms
[INFO]  [2018-05-21 16:15:45] [org.apache.spark.streaming.DStreamGraph]Updated checkpoint data for time 1526890545000 ms
[INFO]  [2018-05-21 16:15:45] [org.apache.spark.streaming.CheckpointWriter]Submitted checkpoint of time 1526890545000 ms to writer queue
[INFO]  [2018-05-21 16:15:45] [org.apache.spark.storage.BlockManager]Removing RDD 156
[INFO]  [2018-05-21 16:15:45] [org.apache.spark.streaming.CheckpointWriter]Deleting hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890470000.bk
[INFO]  [2018-05-21 16:15:45] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526890545000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890545000', took 4437 bytes and 151 ms
[INFO]  [2018-05-21 16:15:45] [org.apache.spark.streaming.CheckpointWriter]Saving checkpoint for time 1526890545000 ms to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890545000'
[INFO]  [2018-05-21 16:15:45] [org.apache.spark.streaming.CheckpointWriter]Deleting hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890470000
[INFO]  [2018-05-21 16:15:45] [org.apache.spark.streaming.CheckpointWriter]Checkpoint for time 1526890545000 ms saved to file 'hdfs://hadoop1:9000/lib/sparkstreaming/check_point/checkpoint-1526890545000', took 4434 bytes and 52 ms
[INFO]  [2018-05-21 16:15:45] [org.apache.spark.streaming.DStreamGraph]Clearing checkpoint data for time 1526890545000 ms
[INFO]  [2018-05-21 16:15:45] [org.apache.spark.streaming.DStreamGraph]Cleared checkpoint data for time 1526890545000 ms
[INFO]  [2018-05-21 16:15:45] [org.apache.spark.streaming.scheduler.ReceivedBlockTracker]Deleting batches: 
[INFO]  [2018-05-21 16:15:45] [org.apache.spark.streaming.util.FileBasedWriteAheadLog_ReceivedBlockTracker]Attempting to clear 1 old log files in hdfs://hadoop1:9000/lib/sparkstreaming/check_point/receivedBlockMetadata older than 1526890485000: hdfs://hadoop1:9000/lib/sparkstreaming/check_point/receivedBlockMetadata/log-1526890410202-1526890470202
[INFO]  [2018-05-21 16:15:45] [org.apache.spark.streaming.scheduler.InputInfoTracker]remove old batch metadata: 1526890470000 ms
[INFO]  [2018-05-21 16:15:45] [org.apache.spark.streaming.util.FileBasedWriteAheadLog_ReceivedBlockTracker]Cleared log files in hdfs://hadoop1:9000/lib/sparkstreaming/check_point/receivedBlockMetadata older than 1526890485000
[ERROR]  [2018-05-21 16:31:09] [org.apache.hadoop.util.Shell]Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:278)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:300)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:293)
	at org.apache.hadoop.hive.conf.HiveConf$ConfVars.findHadoopBinary(HiveConf.java:2327)
	at org.apache.hadoop.hive.conf.HiveConf$ConfVars.<clinit>(HiveConf.java:365)
	at org.apache.hadoop.hive.conf.HiveConf.<clinit>(HiveConf.java:105)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:225)
	at org.apache.spark.sql.SparkSession$.hiveClassesArePresent(SparkSession.scala:960)
	at org.apache.spark.sql.SparkSession$Builder.enableHiveSupport(SparkSession.scala:775)
	at com.jiang.sparkhive.SparkHive.<clinit>(SparkHive.java:20)
[INFO]  [2018-05-21 16:31:10] [org.apache.spark.SparkContext]Running Spark version 2.0.0
[WARN]  [2018-05-21 16:31:10] [org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO]  [2018-05-21 16:31:10] [org.apache.spark.SecurityManager]Changing view acls to: Bruin,hadoop
[INFO]  [2018-05-21 16:31:10] [org.apache.spark.SecurityManager]Changing modify acls to: Bruin,hadoop
[INFO]  [2018-05-21 16:31:10] [org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO]  [2018-05-21 16:31:10] [org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO]  [2018-05-21 16:31:10] [org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Bruin, hadoop); groups with view permissions: Set(); users  with modify permissions: Set(Bruin, hadoop); groups with modify permissions: Set()
[INFO]  [2018-05-21 16:31:11] [org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 51863.
[INFO]  [2018-05-21 16:31:11] [org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO]  [2018-05-21 16:31:11] [org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO]  [2018-05-21 16:31:11] [org.apache.spark.storage.DiskBlockManager]Created local directory at C:\Users\Bruin\AppData\Local\Temp\blockmgr-ae23baeb-bc32-46d8-93a9-f42b6492ee7b
[INFO]  [2018-05-21 16:31:11] [org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 901.8 MB
[INFO]  [2018-05-21 16:31:12] [org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO]  [2018-05-21 16:31:12] [org.spark_project.jetty.util.log]Logging initialized @3661ms
[INFO]  [2018-05-21 16:31:12] [org.spark_project.jetty.server.Server]jetty-9.2.z-SNAPSHOT
[INFO]  [2018-05-21 16:31:12] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3f07b12c{/jobs,null,AVAILABLE}
[INFO]  [2018-05-21 16:31:12] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4bd1f8dd{/jobs/json,null,AVAILABLE}
[INFO]  [2018-05-21 16:31:12] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7096b474{/jobs/job,null,AVAILABLE}
[INFO]  [2018-05-21 16:31:12] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3e14c16d{/jobs/job/json,null,AVAILABLE}
[INFO]  [2018-05-21 16:31:12] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3c989952{/stages,null,AVAILABLE}
[INFO]  [2018-05-21 16:31:12] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@784b990c{/stages/json,null,AVAILABLE}
[INFO]  [2018-05-21 16:31:12] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3d3ba765{/stages/stage,null,AVAILABLE}
[INFO]  [2018-05-21 16:31:12] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@25bc0606{/stages/stage/json,null,AVAILABLE}
[INFO]  [2018-05-21 16:31:12] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5d1659ea{/stages/pool,null,AVAILABLE}
[INFO]  [2018-05-21 16:31:12] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@793138bd{/stages/pool/json,null,AVAILABLE}
[INFO]  [2018-05-21 16:31:12] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1951b871{/storage,null,AVAILABLE}
[INFO]  [2018-05-21 16:31:12] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5c18016b{/storage/json,null,AVAILABLE}
[INFO]  [2018-05-21 16:31:12] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@33aeca0b{/storage/rdd,null,AVAILABLE}
[INFO]  [2018-05-21 16:31:12] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@43aaf813{/storage/rdd/json,null,AVAILABLE}
[INFO]  [2018-05-21 16:31:12] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@57ac5227{/environment,null,AVAILABLE}
[INFO]  [2018-05-21 16:31:12] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4ba302e0{/environment/json,null,AVAILABLE}
[INFO]  [2018-05-21 16:31:12] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@e98770d{/executors,null,AVAILABLE}
[INFO]  [2018-05-21 16:31:12] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1ae67cad{/executors/json,null,AVAILABLE}
[INFO]  [2018-05-21 16:31:12] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2f6e28bc{/executors/threadDump,null,AVAILABLE}
[INFO]  [2018-05-21 16:31:12] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7c098bb3{/executors/threadDump/json,null,AVAILABLE}
[INFO]  [2018-05-21 16:31:12] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@31e4bb20{/static,null,AVAILABLE}
[INFO]  [2018-05-21 16:31:12] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@18cebaa5{/,null,AVAILABLE}
[INFO]  [2018-05-21 16:31:12] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@463b4ac8{/api,null,AVAILABLE}
[INFO]  [2018-05-21 16:31:12] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@765f05af{/stages/stage/kill,null,AVAILABLE}
[INFO]  [2018-05-21 16:31:12] [org.spark_project.jetty.server.ServerConnector]Started ServerConnector@7516e4e5{HTTP/1.1}{0.0.0.0:4040}
[INFO]  [2018-05-21 16:31:12] [org.spark_project.jetty.server.Server]Started @3784ms
[INFO]  [2018-05-21 16:31:12] [org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO]  [2018-05-21 16:31:12] [org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://172.18.42.247:4040
[INFO]  [2018-05-21 16:31:12] [org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO]  [2018-05-21 16:31:12] [org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51876.
[INFO]  [2018-05-21 16:31:12] [org.apache.spark.network.netty.NettyBlockTransferService]Server created on 172.18.42.247:51876
[INFO]  [2018-05-21 16:31:12] [org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 172.18.42.247, 51876)
[INFO]  [2018-05-21 16:31:12] [org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 172.18.42.247:51876 with 901.8 MB RAM, BlockManagerId(driver, 172.18.42.247, 51876)
[INFO]  [2018-05-21 16:31:12] [org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 172.18.42.247, 51876)
[INFO]  [2018-05-21 16:31:12] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5f7f2382{/metrics/json,null,AVAILABLE}
[WARN]  [2018-05-21 16:31:12] [org.apache.spark.SparkContext]Use an existing SparkContext, some configuration may not take effect.
[INFO]  [2018-05-21 16:31:12] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1051817b{/SQL,null,AVAILABLE}
[INFO]  [2018-05-21 16:31:12] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@620aa4ea{/SQL/json,null,AVAILABLE}
[INFO]  [2018-05-21 16:31:12] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@41beb473{/SQL/execution,null,AVAILABLE}
[INFO]  [2018-05-21 16:31:12] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@13006998{/SQL/execution/json,null,AVAILABLE}
[INFO]  [2018-05-21 16:31:12] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@10027fc9{/static/sql,null,AVAILABLE}
[INFO]  [2018-05-21 16:31:12] [org.apache.spark.sql.hive.HiveSharedState]Warehouse path is 'file:E:\git\repository\BigData\sparkdemo/spark-warehouse'.
[INFO]  [2018-05-21 16:31:13] [org.apache.spark.sql.execution.SparkSqlParser]Parsing command: SELECT * FROM user_access_sogo
[INFO]  [2018-05-21 16:31:14] [org.apache.spark.sql.hive.HiveUtils]Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
[INFO]  [2018-05-21 16:31:15] [org.apache.hadoop.conf.Configuration.deprecation]mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
[INFO]  [2018-05-21 16:31:15] [org.apache.hadoop.conf.Configuration.deprecation]mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
[INFO]  [2018-05-21 16:31:15] [org.apache.hadoop.conf.Configuration.deprecation]mapred.committer.job.setup.cleanup.needed is deprecated. Instead, use mapreduce.job.committer.setup.cleanup.needed
[INFO]  [2018-05-21 16:31:15] [org.apache.hadoop.conf.Configuration.deprecation]mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
[INFO]  [2018-05-21 16:31:15] [org.apache.hadoop.conf.Configuration.deprecation]mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
[INFO]  [2018-05-21 16:31:15] [org.apache.hadoop.conf.Configuration.deprecation]mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
[INFO]  [2018-05-21 16:31:15] [org.apache.hadoop.conf.Configuration.deprecation]mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
[INFO]  [2018-05-21 16:31:15] [org.apache.hadoop.conf.Configuration.deprecation]mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
[INFO]  [2018-05-21 16:31:16] [org.apache.hadoop.hive.metastore.HiveMetaStore]0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
[INFO]  [2018-05-21 16:31:16] [org.apache.hadoop.hive.metastore.ObjectStore]ObjectStore, initialize called
[INFO]  [2018-05-21 16:31:17] [DataNucleus.Persistence]Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
[INFO]  [2018-05-21 16:31:17] [DataNucleus.Persistence]Property datanucleus.cache.level2 unknown - will be ignored
[INFO]  [2018-05-21 16:31:28] [org.apache.hadoop.hive.metastore.ObjectStore]Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
[INFO]  [2018-05-21 16:31:30] [DataNucleus.Datastore]The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
[INFO]  [2018-05-21 16:31:30] [DataNucleus.Datastore]The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
[INFO]  [2018-05-21 16:31:35] [DataNucleus.Datastore]The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
[INFO]  [2018-05-21 16:31:35] [DataNucleus.Datastore]The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
[INFO]  [2018-05-21 16:31:36] [org.apache.hadoop.hive.metastore.MetaStoreDirectSql]Using direct SQL, underlying DB is DERBY
[INFO]  [2018-05-21 16:31:36] [org.apache.hadoop.hive.metastore.ObjectStore]Initialized ObjectStore
[WARN]  [2018-05-21 16:31:37] [org.apache.hadoop.hive.metastore.ObjectStore]Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
[WARN]  [2018-05-21 16:31:38] [org.apache.hadoop.hive.metastore.ObjectStore]Failed to get database default, returning NoSuchObjectException
[WARN]  [2018-05-21 16:31:38] [hive.ql.metadata.Hive]Failed to access metastore. This class should not accessed in runtime.
org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient
	at org.apache.hadoop.hive.ql.metadata.Hive.getAllDatabases(Hive.java:1236)
	at org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:174)
	at org.apache.hadoop.hive.ql.metadata.Hive.<clinit>(Hive.java:166)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)
	at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:171)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:258)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:359)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:263)
	at org.apache.spark.sql.hive.HiveSharedState.metadataHive$lzycompute(HiveSharedState.scala:39)
	at org.apache.spark.sql.hive.HiveSharedState.metadataHive(HiveSharedState.scala:38)
	at org.apache.spark.sql.hive.HiveSharedState.externalCatalog$lzycompute(HiveSharedState.scala:46)
	at org.apache.spark.sql.hive.HiveSharedState.externalCatalog(HiveSharedState.scala:45)
	at org.apache.spark.sql.hive.HiveSessionState.catalog$lzycompute(HiveSessionState.scala:50)
	at org.apache.spark.sql.hive.HiveSessionState.catalog(HiveSessionState.scala:48)
	at org.apache.spark.sql.hive.HiveSessionState$$anon$1.<init>(HiveSessionState.scala:63)
	at org.apache.spark.sql.hive.HiveSessionState.analyzer$lzycompute(HiveSessionState.scala:63)
	at org.apache.spark.sql.hive.HiveSessionState.analyzer(HiveSessionState.scala:62)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:49)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:64)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:582)
	at com.jiang.sparkhive.SparkHive.queryData(SparkHive.java:40)
	at com.jiang.sparkhive.SparkHive.main(SparkHive.java:47)
Caused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1523)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:86)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)
	at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)
	at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)
	at org.apache.hadoop.hive.ql.metadata.Hive.getAllDatabases(Hive.java:1234)
	... 25 more
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)
	... 31 more
Caused by: java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: file:E:/git/repository/BigData/sparkdemo/spark-warehouse
	at org.apache.hadoop.fs.Path.initialize(Path.java:206)
	at org.apache.hadoop.fs.Path.<init>(Path.java:172)
	at org.apache.hadoop.hive.metastore.Warehouse.getWhRoot(Warehouse.java:159)
	at org.apache.hadoop.hive.metastore.Warehouse.getDefaultDatabasePath(Warehouse.java:177)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB_core(HiveMetaStore.java:600)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:620)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:461)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)
	at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5762)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:199)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)
	... 36 more
Caused by: java.net.URISyntaxException: Relative path in absolute URI: file:E:/git/repository/BigData/sparkdemo/spark-warehouse
	at java.net.URI.checkPath(URI.java:1823)
	at java.net.URI.<init>(URI.java:745)
	at org.apache.hadoop.fs.Path.initialize(Path.java:203)
	... 47 more
[INFO]  [2018-05-21 16:31:38] [org.apache.hadoop.hive.metastore.ObjectStore]ObjectStore, initialize called
[INFO]  [2018-05-21 16:31:38] [org.apache.hadoop.hive.metastore.MetaStoreDirectSql]Using direct SQL, underlying DB is DERBY
[INFO]  [2018-05-21 16:31:38] [org.apache.hadoop.hive.metastore.ObjectStore]Initialized ObjectStore
[WARN]  [2018-05-21 16:31:38] [org.apache.hadoop.hive.metastore.ObjectStore]Failed to get database default, returning NoSuchObjectException
[INFO]  [2018-05-21 16:31:38] [org.apache.spark.SparkContext]Invoking stop() from shutdown hook
[INFO]  [2018-05-21 16:31:38] [org.spark_project.jetty.server.ServerConnector]Stopped ServerConnector@7516e4e5{HTTP/1.1}{0.0.0.0:4040}
[INFO]  [2018-05-21 16:31:38] [org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@765f05af{/stages/stage/kill,null,UNAVAILABLE}
[INFO]  [2018-05-21 16:31:38] [org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@463b4ac8{/api,null,UNAVAILABLE}
[INFO]  [2018-05-21 16:31:38] [org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@18cebaa5{/,null,UNAVAILABLE}
[INFO]  [2018-05-21 16:31:38] [org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@31e4bb20{/static,null,UNAVAILABLE}
[INFO]  [2018-05-21 16:31:38] [org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@7c098bb3{/executors/threadDump/json,null,UNAVAILABLE}
[INFO]  [2018-05-21 16:31:38] [org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@2f6e28bc{/executors/threadDump,null,UNAVAILABLE}
[INFO]  [2018-05-21 16:31:38] [org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@1ae67cad{/executors/json,null,UNAVAILABLE}
[INFO]  [2018-05-21 16:31:38] [org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@e98770d{/executors,null,UNAVAILABLE}
[INFO]  [2018-05-21 16:31:38] [org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@4ba302e0{/environment/json,null,UNAVAILABLE}
[INFO]  [2018-05-21 16:31:38] [org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@57ac5227{/environment,null,UNAVAILABLE}
[INFO]  [2018-05-21 16:31:38] [org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@43aaf813{/storage/rdd/json,null,UNAVAILABLE}
[INFO]  [2018-05-21 16:31:38] [org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@33aeca0b{/storage/rdd,null,UNAVAILABLE}
[INFO]  [2018-05-21 16:31:38] [org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@5c18016b{/storage/json,null,UNAVAILABLE}
[INFO]  [2018-05-21 16:31:38] [org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@1951b871{/storage,null,UNAVAILABLE}
[INFO]  [2018-05-21 16:31:38] [org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@793138bd{/stages/pool/json,null,UNAVAILABLE}
[INFO]  [2018-05-21 16:31:38] [org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@5d1659ea{/stages/pool,null,UNAVAILABLE}
[INFO]  [2018-05-21 16:31:38] [org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@25bc0606{/stages/stage/json,null,UNAVAILABLE}
[INFO]  [2018-05-21 16:31:38] [org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@3d3ba765{/stages/stage,null,UNAVAILABLE}
[INFO]  [2018-05-21 16:31:38] [org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@784b990c{/stages/json,null,UNAVAILABLE}
[INFO]  [2018-05-21 16:31:38] [org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@3c989952{/stages,null,UNAVAILABLE}
[INFO]  [2018-05-21 16:31:38] [org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@3e14c16d{/jobs/job/json,null,UNAVAILABLE}
[INFO]  [2018-05-21 16:31:38] [org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@7096b474{/jobs/job,null,UNAVAILABLE}
[INFO]  [2018-05-21 16:31:38] [org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@4bd1f8dd{/jobs/json,null,UNAVAILABLE}
[INFO]  [2018-05-21 16:31:38] [org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@3f07b12c{/jobs,null,UNAVAILABLE}
[INFO]  [2018-05-21 16:31:38] [org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://172.18.42.247:4040
[INFO]  [2018-05-21 16:31:38] [org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO]  [2018-05-21 16:31:38] [org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO]  [2018-05-21 16:31:38] [org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO]  [2018-05-21 16:31:38] [org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO]  [2018-05-21 16:31:38] [org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO]  [2018-05-21 16:31:38] [org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO]  [2018-05-21 16:31:38] [org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO]  [2018-05-21 16:31:38] [org.apache.spark.util.ShutdownHookManager]Deleting directory C:\Users\Bruin\AppData\Local\Temp\spark-448dc021-3341-4644-af26-1cf225117337
[ERROR]  [2018-05-21 16:35:31] [org.apache.hadoop.util.Shell]Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:278)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:300)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:293)
	at org.apache.hadoop.hive.conf.HiveConf$ConfVars.findHadoopBinary(HiveConf.java:2327)
	at org.apache.hadoop.hive.conf.HiveConf$ConfVars.<clinit>(HiveConf.java:365)
	at org.apache.hadoop.hive.conf.HiveConf.<clinit>(HiveConf.java:105)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at org.apache.spark.util.Utils$.classForName(Utils.scala:225)
	at org.apache.spark.sql.SparkSession$.hiveClassesArePresent(SparkSession.scala:960)
	at org.apache.spark.sql.SparkSession$Builder.enableHiveSupport(SparkSession.scala:775)
	at com.jiang.sparkhive.SparkHive.<clinit>(SparkHive.java:20)
[INFO]  [2018-05-21 16:35:31] [org.apache.spark.SparkContext]Running Spark version 2.0.0
[WARN]  [2018-05-21 16:35:31] [org.apache.hadoop.util.NativeCodeLoader]Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[INFO]  [2018-05-21 16:35:32] [org.apache.spark.SecurityManager]Changing view acls to: Bruin,hadoop
[INFO]  [2018-05-21 16:35:32] [org.apache.spark.SecurityManager]Changing modify acls to: Bruin,hadoop
[INFO]  [2018-05-21 16:35:32] [org.apache.spark.SecurityManager]Changing view acls groups to: 
[INFO]  [2018-05-21 16:35:32] [org.apache.spark.SecurityManager]Changing modify acls groups to: 
[INFO]  [2018-05-21 16:35:32] [org.apache.spark.SecurityManager]SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Bruin, hadoop); groups with view permissions: Set(); users  with modify permissions: Set(Bruin, hadoop); groups with modify permissions: Set()
[INFO]  [2018-05-21 16:35:33] [org.apache.spark.util.Utils]Successfully started service 'sparkDriver' on port 52130.
[INFO]  [2018-05-21 16:35:33] [org.apache.spark.SparkEnv]Registering MapOutputTracker
[INFO]  [2018-05-21 16:35:33] [org.apache.spark.SparkEnv]Registering BlockManagerMaster
[INFO]  [2018-05-21 16:35:33] [org.apache.spark.storage.DiskBlockManager]Created local directory at C:\Users\Bruin\AppData\Local\Temp\blockmgr-6f993a70-0933-4757-a7d9-9b3ff677de36
[INFO]  [2018-05-21 16:35:33] [org.apache.spark.storage.memory.MemoryStore]MemoryStore started with capacity 901.8 MB
[INFO]  [2018-05-21 16:35:33] [org.apache.spark.SparkEnv]Registering OutputCommitCoordinator
[INFO]  [2018-05-21 16:35:33] [org.spark_project.jetty.util.log]Logging initialized @3499ms
[INFO]  [2018-05-21 16:35:33] [org.spark_project.jetty.server.Server]jetty-9.2.z-SNAPSHOT
[INFO]  [2018-05-21 16:35:33] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3f07b12c{/jobs,null,AVAILABLE}
[INFO]  [2018-05-21 16:35:33] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4bd1f8dd{/jobs/json,null,AVAILABLE}
[INFO]  [2018-05-21 16:35:33] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7096b474{/jobs/job,null,AVAILABLE}
[INFO]  [2018-05-21 16:35:33] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3e14c16d{/jobs/job/json,null,AVAILABLE}
[INFO]  [2018-05-21 16:35:33] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3c989952{/stages,null,AVAILABLE}
[INFO]  [2018-05-21 16:35:33] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@784b990c{/stages/json,null,AVAILABLE}
[INFO]  [2018-05-21 16:35:33] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@3d3ba765{/stages/stage,null,AVAILABLE}
[INFO]  [2018-05-21 16:35:33] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@25bc0606{/stages/stage/json,null,AVAILABLE}
[INFO]  [2018-05-21 16:35:33] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5d1659ea{/stages/pool,null,AVAILABLE}
[INFO]  [2018-05-21 16:35:33] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@793138bd{/stages/pool/json,null,AVAILABLE}
[INFO]  [2018-05-21 16:35:33] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1951b871{/storage,null,AVAILABLE}
[INFO]  [2018-05-21 16:35:33] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5c18016b{/storage/json,null,AVAILABLE}
[INFO]  [2018-05-21 16:35:33] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@33aeca0b{/storage/rdd,null,AVAILABLE}
[INFO]  [2018-05-21 16:35:33] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@43aaf813{/storage/rdd/json,null,AVAILABLE}
[INFO]  [2018-05-21 16:35:33] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@57ac5227{/environment,null,AVAILABLE}
[INFO]  [2018-05-21 16:35:33] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4ba302e0{/environment/json,null,AVAILABLE}
[INFO]  [2018-05-21 16:35:33] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@e98770d{/executors,null,AVAILABLE}
[INFO]  [2018-05-21 16:35:33] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@1ae67cad{/executors/json,null,AVAILABLE}
[INFO]  [2018-05-21 16:35:33] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2f6e28bc{/executors/threadDump,null,AVAILABLE}
[INFO]  [2018-05-21 16:35:33] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@7c098bb3{/executors/threadDump/json,null,AVAILABLE}
[INFO]  [2018-05-21 16:35:33] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@31e4bb20{/static,null,AVAILABLE}
[INFO]  [2018-05-21 16:35:33] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@18cebaa5{/,null,AVAILABLE}
[INFO]  [2018-05-21 16:35:33] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@463b4ac8{/api,null,AVAILABLE}
[INFO]  [2018-05-21 16:35:33] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@765f05af{/stages/stage/kill,null,AVAILABLE}
[INFO]  [2018-05-21 16:35:33] [org.spark_project.jetty.server.ServerConnector]Started ServerConnector@7516e4e5{HTTP/1.1}{0.0.0.0:4040}
[INFO]  [2018-05-21 16:35:33] [org.spark_project.jetty.server.Server]Started @3646ms
[INFO]  [2018-05-21 16:35:33] [org.apache.spark.util.Utils]Successfully started service 'SparkUI' on port 4040.
[INFO]  [2018-05-21 16:35:33] [org.apache.spark.ui.SparkUI]Bound SparkUI to 0.0.0.0, and started at http://172.18.42.247:4040
[INFO]  [2018-05-21 16:35:33] [org.apache.spark.executor.Executor]Starting executor ID driver on host localhost
[INFO]  [2018-05-21 16:35:34] [org.apache.spark.util.Utils]Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 52145.
[INFO]  [2018-05-21 16:35:34] [org.apache.spark.network.netty.NettyBlockTransferService]Server created on 172.18.42.247:52145
[INFO]  [2018-05-21 16:35:34] [org.apache.spark.storage.BlockManagerMaster]Registering BlockManager BlockManagerId(driver, 172.18.42.247, 52145)
[INFO]  [2018-05-21 16:35:34] [org.apache.spark.storage.BlockManagerMasterEndpoint]Registering block manager 172.18.42.247:52145 with 901.8 MB RAM, BlockManagerId(driver, 172.18.42.247, 52145)
[INFO]  [2018-05-21 16:35:34] [org.apache.spark.storage.BlockManagerMaster]Registered BlockManager BlockManagerId(driver, 172.18.42.247, 52145)
[INFO]  [2018-05-21 16:35:34] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@466cf502{/metrics/json,null,AVAILABLE}
[WARN]  [2018-05-21 16:35:34] [org.apache.spark.SparkContext]Use an existing SparkContext, some configuration may not take effect.
[INFO]  [2018-05-21 16:35:34] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@74f7d1d2{/SQL,null,AVAILABLE}
[INFO]  [2018-05-21 16:35:34] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@5ca17ab0{/SQL/json,null,AVAILABLE}
[INFO]  [2018-05-21 16:35:34] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@2db2dd9d{/SQL/execution,null,AVAILABLE}
[INFO]  [2018-05-21 16:35:34] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@4d411036{/SQL/execution/json,null,AVAILABLE}
[INFO]  [2018-05-21 16:35:34] [org.spark_project.jetty.server.handler.ContextHandler]Started o.s.j.s.ServletContextHandler@352c308{/static/sql,null,AVAILABLE}
[INFO]  [2018-05-21 16:35:34] [org.apache.spark.sql.hive.HiveSharedState]Warehouse path is 'hdfs://hadoop1:9000/user/hive/warehouse'.
[INFO]  [2018-05-21 16:35:34] [org.apache.spark.sql.execution.SparkSqlParser]Parsing command: SELECT * FROM user_access_sogo
[INFO]  [2018-05-21 16:35:35] [org.apache.spark.sql.hive.HiveUtils]Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
[INFO]  [2018-05-21 16:35:35] [org.apache.hadoop.conf.Configuration.deprecation]mapred.max.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.maxsize
[INFO]  [2018-05-21 16:35:35] [org.apache.hadoop.conf.Configuration.deprecation]mapred.reduce.tasks.speculative.execution is deprecated. Instead, use mapreduce.reduce.speculative
[INFO]  [2018-05-21 16:35:35] [org.apache.hadoop.conf.Configuration.deprecation]mapred.committer.job.setup.cleanup.needed is deprecated. Instead, use mapreduce.job.committer.setup.cleanup.needed
[INFO]  [2018-05-21 16:35:35] [org.apache.hadoop.conf.Configuration.deprecation]mapred.min.split.size.per.rack is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.rack
[INFO]  [2018-05-21 16:35:35] [org.apache.hadoop.conf.Configuration.deprecation]mapred.min.split.size is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize
[INFO]  [2018-05-21 16:35:35] [org.apache.hadoop.conf.Configuration.deprecation]mapred.min.split.size.per.node is deprecated. Instead, use mapreduce.input.fileinputformat.split.minsize.per.node
[INFO]  [2018-05-21 16:35:35] [org.apache.hadoop.conf.Configuration.deprecation]mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces
[INFO]  [2018-05-21 16:35:35] [org.apache.hadoop.conf.Configuration.deprecation]mapred.input.dir.recursive is deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
[INFO]  [2018-05-21 16:35:36] [org.apache.hadoop.hive.metastore.HiveMetaStore]0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
[INFO]  [2018-05-21 16:35:36] [org.apache.hadoop.hive.metastore.ObjectStore]ObjectStore, initialize called
[INFO]  [2018-05-21 16:35:36] [DataNucleus.Persistence]Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
[INFO]  [2018-05-21 16:35:36] [DataNucleus.Persistence]Property datanucleus.cache.level2 unknown - will be ignored
[INFO]  [2018-05-21 16:35:41] [org.apache.hadoop.hive.metastore.ObjectStore]Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
[INFO]  [2018-05-21 16:35:43] [DataNucleus.Datastore]The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
[INFO]  [2018-05-21 16:35:43] [DataNucleus.Datastore]The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
[INFO]  [2018-05-21 16:35:43] [DataNucleus.Datastore]The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
[INFO]  [2018-05-21 16:35:44] [DataNucleus.Datastore]The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
[INFO]  [2018-05-21 16:35:44] [org.apache.hadoop.hive.metastore.MetaStoreDirectSql]Using direct SQL, underlying DB is DERBY
[INFO]  [2018-05-21 16:35:44] [org.apache.hadoop.hive.metastore.ObjectStore]Initialized ObjectStore
[WARN]  [2018-05-21 16:35:44] [org.apache.hadoop.hive.metastore.ObjectStore]Failed to get database default, returning NoSuchObjectException
[INFO]  [2018-05-21 16:35:45] [org.apache.hadoop.hive.metastore.HiveMetaStore]Added admin role in metastore
[INFO]  [2018-05-21 16:35:45] [org.apache.hadoop.hive.metastore.HiveMetaStore]Added public role in metastore
[INFO]  [2018-05-21 16:35:45] [org.apache.hadoop.hive.metastore.HiveMetaStore]No user is added in admin role, since config is empty
[INFO]  [2018-05-21 16:35:45] [org.apache.hadoop.hive.metastore.HiveMetaStore]0: get_all_databases
[INFO]  [2018-05-21 16:35:45] [org.apache.hadoop.hive.metastore.HiveMetaStore.audit]ugi=hadoop	ip=unknown-ip-addr	cmd=get_all_databases	
[INFO]  [2018-05-21 16:35:45] [org.apache.hadoop.hive.metastore.HiveMetaStore]0: get_functions: db=default pat=*
[INFO]  [2018-05-21 16:35:45] [org.apache.hadoop.hive.metastore.HiveMetaStore.audit]ugi=hadoop	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
[INFO]  [2018-05-21 16:35:45] [DataNucleus.Datastore]The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
[INFO]  [2018-05-21 16:35:46] [org.apache.spark.SparkContext]Invoking stop() from shutdown hook
[INFO]  [2018-05-21 16:35:46] [org.spark_project.jetty.server.ServerConnector]Stopped ServerConnector@7516e4e5{HTTP/1.1}{0.0.0.0:4040}
[INFO]  [2018-05-21 16:35:46] [org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@765f05af{/stages/stage/kill,null,UNAVAILABLE}
[INFO]  [2018-05-21 16:35:46] [org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@463b4ac8{/api,null,UNAVAILABLE}
[INFO]  [2018-05-21 16:35:46] [org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@18cebaa5{/,null,UNAVAILABLE}
[INFO]  [2018-05-21 16:35:46] [org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@31e4bb20{/static,null,UNAVAILABLE}
[INFO]  [2018-05-21 16:35:46] [org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@7c098bb3{/executors/threadDump/json,null,UNAVAILABLE}
[INFO]  [2018-05-21 16:35:46] [org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@2f6e28bc{/executors/threadDump,null,UNAVAILABLE}
[INFO]  [2018-05-21 16:35:46] [org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@1ae67cad{/executors/json,null,UNAVAILABLE}
[INFO]  [2018-05-21 16:35:46] [org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@e98770d{/executors,null,UNAVAILABLE}
[INFO]  [2018-05-21 16:35:46] [org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@4ba302e0{/environment/json,null,UNAVAILABLE}
[INFO]  [2018-05-21 16:35:46] [org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@57ac5227{/environment,null,UNAVAILABLE}
[INFO]  [2018-05-21 16:35:46] [org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@43aaf813{/storage/rdd/json,null,UNAVAILABLE}
[INFO]  [2018-05-21 16:35:46] [org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@33aeca0b{/storage/rdd,null,UNAVAILABLE}
[INFO]  [2018-05-21 16:35:46] [org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@5c18016b{/storage/json,null,UNAVAILABLE}
[INFO]  [2018-05-21 16:35:46] [org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@1951b871{/storage,null,UNAVAILABLE}
[INFO]  [2018-05-21 16:35:46] [org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@793138bd{/stages/pool/json,null,UNAVAILABLE}
[INFO]  [2018-05-21 16:35:46] [org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@5d1659ea{/stages/pool,null,UNAVAILABLE}
[INFO]  [2018-05-21 16:35:46] [org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@25bc0606{/stages/stage/json,null,UNAVAILABLE}
[INFO]  [2018-05-21 16:35:46] [org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@3d3ba765{/stages/stage,null,UNAVAILABLE}
[INFO]  [2018-05-21 16:35:46] [org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@784b990c{/stages/json,null,UNAVAILABLE}
[INFO]  [2018-05-21 16:35:46] [org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@3c989952{/stages,null,UNAVAILABLE}
[INFO]  [2018-05-21 16:35:46] [org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@3e14c16d{/jobs/job/json,null,UNAVAILABLE}
[INFO]  [2018-05-21 16:35:46] [org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@7096b474{/jobs/job,null,UNAVAILABLE}
[INFO]  [2018-05-21 16:35:46] [org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@4bd1f8dd{/jobs/json,null,UNAVAILABLE}
[INFO]  [2018-05-21 16:35:46] [org.spark_project.jetty.server.handler.ContextHandler]Stopped o.s.j.s.ServletContextHandler@3f07b12c{/jobs,null,UNAVAILABLE}
[INFO]  [2018-05-21 16:35:46] [org.apache.spark.ui.SparkUI]Stopped Spark web UI at http://172.18.42.247:4040
[INFO]  [2018-05-21 16:35:46] [org.apache.spark.MapOutputTrackerMasterEndpoint]MapOutputTrackerMasterEndpoint stopped!
[INFO]  [2018-05-21 16:35:46] [org.apache.spark.storage.memory.MemoryStore]MemoryStore cleared
[INFO]  [2018-05-21 16:35:46] [org.apache.spark.storage.BlockManager]BlockManager stopped
[INFO]  [2018-05-21 16:35:46] [org.apache.spark.storage.BlockManagerMaster]BlockManagerMaster stopped
[INFO]  [2018-05-21 16:35:46] [org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint]OutputCommitCoordinator stopped!
[INFO]  [2018-05-21 16:35:46] [org.apache.spark.SparkContext]Successfully stopped SparkContext
[INFO]  [2018-05-21 16:35:46] [org.apache.spark.util.ShutdownHookManager]Shutdown hook called
[INFO]  [2018-05-21 16:35:46] [org.apache.spark.util.ShutdownHookManager]Deleting directory C:\Users\Bruin\AppData\Local\Temp\spark-e252b2c0-7aab-4dcf-b1b6-b5cd9e56fecb
